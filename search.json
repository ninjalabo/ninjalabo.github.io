[
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "SVD (a mathematical technique) helps us approximate complex weight matrices with simpler ones, to compress the model’s convolutional layers. * Goal: Reduce the model’s size and speed up computations without significantly sacrificing its accuracy.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os\n\n\n\n\nUsing the Imagenette2-320 dataset, which is a smaller version of the ImageNet dataset. It has images that belong to just 10 classes (e.g., different breeds of dogs, cats, etc.).\n\nfrom torchvision.datasets.utils import download_and_extract_archive\n\n# Download Imagenette2-320\nurl = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\nroot = \"./data\"\ndownload_and_extract_archive(url, download_root=root)\n\n# Set dataset path\ndataset_path = os.path.join(root, \"imagenette2-320\")\n\nUsing downloaded and verified file: ./data\\imagenette2-320.tgz\nExtracting ./data\\imagenette2-320.tgz to ./data\n\n\n\n\n\n\n# Data transformation: Resize, Convert to Tensor, Normalize\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224\n    transforms.ToTensor(),          # Convert images to PyTorch tensors\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalize as per ImageNet pre-training\n])\n\n# Load dataset\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=transform)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n\n\n\n\n\n# Load pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Move model to device (CPU or GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n\n\n\nThis technique reduces the size of the model by approximating its convolutional layers (layers that detect image features like edges, shapes, etc.).\n\ndef svd_compress_conv_layer(conv_layer, rank):\n    # Get the weight tensor of the convolutional layer\n    weight = conv_layer.weight.data\n    out_channels, in_channels, h, w = weight.shape\n    \n    # Reshape the weight tensor to a 2D matrix of shape (out_channels, in_channels * h * w)\n    weight_reshaped = weight.view(out_channels, -1)\n\n    # Apply SVD to the weight matrix\n    U, S, V = torch.svd(weight_reshaped)\n\n    # Keep only the top `rank` singular values/vectors\n    U_reduced = U[:, :rank]\n    S_reduced = S[:rank]\n    V_reduced = V[:, :rank]\n\n    # Construct the compressed weight matrix\n    compressed_weight = torch.mm(U_reduced, torch.diag(S_reduced))\n    compressed_weight = torch.mm(compressed_weight, V_reduced.t())\n\n    # Reshape back to the original convolutional weight shape\n    compressed_weight = compressed_weight.view(out_channels, in_channels, h, w)\n\n    # Replace the original weights with the compressed weights\n    conv_layer.weight.data = compressed_weight\n\n    return conv_layer\n\n\n\n\nWe apply SVD to every convolutional layer in the model. By keeping only the most important components (e.g., 20 components), we make each layer smaller and therefore reduce the entire model’s size.\n\ndef compress_resnet50(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            print(f\"Compressing layer: {name}\")\n            compressed_layer = svd_compress_conv_layer(module, rank=rank)\n            setattr(model, name, compressed_layer)\n\n# Compress the model with a reduced rank (e.g., keep top 20 components)\ncompress_resnet50(model, rank=20)\n\nCompressing layer: conv1\nCompressing layer: layer1.0.conv1\nCompressing layer: layer1.0.conv2\nCompressing layer: layer1.0.conv3\nCompressing layer: layer1.0.downsample.0\nCompressing layer: layer1.1.conv1\nCompressing layer: layer1.1.conv2\nCompressing layer: layer1.1.conv3\nCompressing layer: layer1.2.conv1\nCompressing layer: layer1.2.conv2\nCompressing layer: layer1.2.conv3\nCompressing layer: layer2.0.conv1\nCompressing layer: layer2.0.conv2\nCompressing layer: layer2.0.conv3\nCompressing layer: layer2.0.downsample.0\nCompressing layer: layer2.1.conv1\nCompressing layer: layer2.1.conv2\nCompressing layer: layer2.1.conv3\nCompressing layer: layer2.2.conv1\nCompressing layer: layer2.2.conv2\nCompressing layer: layer2.2.conv3\nCompressing layer: layer2.3.conv1\nCompressing layer: layer2.3.conv2\nCompressing layer: layer2.3.conv3\nCompressing layer: layer3.0.conv1\nCompressing layer: layer3.0.conv2\nCompressing layer: layer3.0.conv3\nCompressing layer: layer3.0.downsample.0\nCompressing layer: layer3.1.conv1\nCompressing layer: layer3.1.conv2\nCompressing layer: layer3.1.conv3\nCompressing layer: layer3.2.conv1\nCompressing layer: layer3.2.conv2\nCompressing layer: layer3.2.conv3\nCompressing layer: layer3.3.conv1\nCompressing layer: layer3.3.conv2\nCompressing layer: layer3.3.conv3\nCompressing layer: layer3.4.conv1\nCompressing layer: layer3.4.conv2\nCompressing layer: layer3.4.conv3\nCompressing layer: layer3.5.conv1\nCompressing layer: layer3.5.conv2\nCompressing layer: layer3.5.conv3\nCompressing layer: layer4.0.conv1\nCompressing layer: layer4.0.conv2\nCompressing layer: layer4.0.conv3\nCompressing layer: layer4.0.downsample.0\nCompressing layer: layer4.1.conv1\nCompressing layer: layer4.1.conv2\nCompressing layer: layer4.1.conv3\nCompressing layer: layer4.2.conv1\nCompressing layer: layer4.2.conv2\nCompressing layer: layer4.2.conv3\n\n\n\n\n\nAfter compression, the model needs to be retrained slightly to adjust the weights.\nCompression changes the weights significantly, which may affect the model’s accuracy. Fine-tuning helps adjust these weights and bring back the accuracy closer to its original value.\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 3\nmodel.train()\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n\nprint(\"Training complete.\")\n\nEpoch [1/3], Loss: 1.4015\nEpoch [2/3], Loss: 0.8525\nEpoch [3/3], Loss: 0.6684\nTraining complete.\n\n\n\n\n\n\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in valid_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Validation Accuracy after Compression: {100 * correct / total:.2f}%')\n\nValidation Accuracy after Compression: 72.20%"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#singular-value-decomposition-svd---resnet50",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#singular-value-decomposition-svd---resnet50",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "SVD (a mathematical technique) helps us approximate complex weight matrices with simpler ones, to compress the model’s convolutional layers. * Goal: Reduce the model’s size and speed up computations without significantly sacrificing its accuracy.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#download-and-load-the-dataset",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#download-and-load-the-dataset",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "Using the Imagenette2-320 dataset, which is a smaller version of the ImageNet dataset. It has images that belong to just 10 classes (e.g., different breeds of dogs, cats, etc.).\n\nfrom torchvision.datasets.utils import download_and_extract_archive\n\n# Download Imagenette2-320\nurl = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\nroot = \"./data\"\ndownload_and_extract_archive(url, download_root=root)\n\n# Set dataset path\ndataset_path = os.path.join(root, \"imagenette2-320\")\n\nUsing downloaded and verified file: ./data\\imagenette2-320.tgz\nExtracting ./data\\imagenette2-320.tgz to ./data"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#prepare-the-dataset",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#prepare-the-dataset",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "# Data transformation: Resize, Convert to Tensor, Normalize\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224\n    transforms.ToTensor(),          # Convert images to PyTorch tensors\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalize as per ImageNet pre-training\n])\n\n# Load dataset\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=transform)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#load-the-pre-trained-resnet-50-model",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#load-the-pre-trained-resnet-50-model",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "# Load pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Move model to device (CPU or GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#apply-svd-for-model-compression",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#apply-svd-for-model-compression",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "This technique reduces the size of the model by approximating its convolutional layers (layers that detect image features like edges, shapes, etc.).\n\ndef svd_compress_conv_layer(conv_layer, rank):\n    # Get the weight tensor of the convolutional layer\n    weight = conv_layer.weight.data\n    out_channels, in_channels, h, w = weight.shape\n    \n    # Reshape the weight tensor to a 2D matrix of shape (out_channels, in_channels * h * w)\n    weight_reshaped = weight.view(out_channels, -1)\n\n    # Apply SVD to the weight matrix\n    U, S, V = torch.svd(weight_reshaped)\n\n    # Keep only the top `rank` singular values/vectors\n    U_reduced = U[:, :rank]\n    S_reduced = S[:rank]\n    V_reduced = V[:, :rank]\n\n    # Construct the compressed weight matrix\n    compressed_weight = torch.mm(U_reduced, torch.diag(S_reduced))\n    compressed_weight = torch.mm(compressed_weight, V_reduced.t())\n\n    # Reshape back to the original convolutional weight shape\n    compressed_weight = compressed_weight.view(out_channels, in_channels, h, w)\n\n    # Replace the original weights with the compressed weights\n    conv_layer.weight.data = compressed_weight\n\n    return conv_layer"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#apply-svd-to-each-convolutional-layer-in-resnet-50",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#apply-svd-to-each-convolutional-layer-in-resnet-50",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "We apply SVD to every convolutional layer in the model. By keeping only the most important components (e.g., 20 components), we make each layer smaller and therefore reduce the entire model’s size.\n\ndef compress_resnet50(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            print(f\"Compressing layer: {name}\")\n            compressed_layer = svd_compress_conv_layer(module, rank=rank)\n            setattr(model, name, compressed_layer)\n\n# Compress the model with a reduced rank (e.g., keep top 20 components)\ncompress_resnet50(model, rank=20)\n\nCompressing layer: conv1\nCompressing layer: layer1.0.conv1\nCompressing layer: layer1.0.conv2\nCompressing layer: layer1.0.conv3\nCompressing layer: layer1.0.downsample.0\nCompressing layer: layer1.1.conv1\nCompressing layer: layer1.1.conv2\nCompressing layer: layer1.1.conv3\nCompressing layer: layer1.2.conv1\nCompressing layer: layer1.2.conv2\nCompressing layer: layer1.2.conv3\nCompressing layer: layer2.0.conv1\nCompressing layer: layer2.0.conv2\nCompressing layer: layer2.0.conv3\nCompressing layer: layer2.0.downsample.0\nCompressing layer: layer2.1.conv1\nCompressing layer: layer2.1.conv2\nCompressing layer: layer2.1.conv3\nCompressing layer: layer2.2.conv1\nCompressing layer: layer2.2.conv2\nCompressing layer: layer2.2.conv3\nCompressing layer: layer2.3.conv1\nCompressing layer: layer2.3.conv2\nCompressing layer: layer2.3.conv3\nCompressing layer: layer3.0.conv1\nCompressing layer: layer3.0.conv2\nCompressing layer: layer3.0.conv3\nCompressing layer: layer3.0.downsample.0\nCompressing layer: layer3.1.conv1\nCompressing layer: layer3.1.conv2\nCompressing layer: layer3.1.conv3\nCompressing layer: layer3.2.conv1\nCompressing layer: layer3.2.conv2\nCompressing layer: layer3.2.conv3\nCompressing layer: layer3.3.conv1\nCompressing layer: layer3.3.conv2\nCompressing layer: layer3.3.conv3\nCompressing layer: layer3.4.conv1\nCompressing layer: layer3.4.conv2\nCompressing layer: layer3.4.conv3\nCompressing layer: layer3.5.conv1\nCompressing layer: layer3.5.conv2\nCompressing layer: layer3.5.conv3\nCompressing layer: layer4.0.conv1\nCompressing layer: layer4.0.conv2\nCompressing layer: layer4.0.conv3\nCompressing layer: layer4.0.downsample.0\nCompressing layer: layer4.1.conv1\nCompressing layer: layer4.1.conv2\nCompressing layer: layer4.1.conv3\nCompressing layer: layer4.2.conv1\nCompressing layer: layer4.2.conv2\nCompressing layer: layer4.2.conv3"
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#fine-tune-the-compressed-model",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#fine-tune-the-compressed-model",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "After compression, the model needs to be retrained slightly to adjust the weights.\nCompression changes the weights significantly, which may affect the model’s accuracy. Fine-tuning helps adjust these weights and bring back the accuracy closer to its original value.\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 3\nmodel.train()\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n\nprint(\"Training complete.\")\n\nEpoch [1/3], Loss: 1.4015\nEpoch [2/3], Loss: 0.8525\nEpoch [3/3], Loss: 0.6684\nTraining complete."
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#evaluate-the-compressed-model",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V2.html#evaluate-the-compressed-model",
    "title": "Low Rank Approximation Implementation 2/4",
    "section": "",
    "text": "model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in valid_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Validation Accuracy after Compression: {100 * correct / total:.2f}%')\n\nValidation Accuracy after Compression: 72.20%"
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/lang_vs_swarm.html",
    "href": "blogs/lang_vs_swarm.html",
    "title": "LangChain + LangGraph vs. OpenAI Swarm + GPTs + Function Calling: A Comparison for Web Service Development",
    "section": "",
    "text": "At NinjaLABO, we’re always on the lookout for the most effective tools and platforms for AI-powered web service development. As we explore different architectures for building scalable AI services, two major approaches stand out: LangChain + LangGraph and OpenAI’s Swarm + GPTs + Function Calling. Both have powerful features, but they serve different needs depending on the complexity and goals of your project.\nIn this post, we’ll compare these two ecosystems, breaking down their strengths and weaknesses, and provide insights into which one might be the best fit for your next project. Let’s dive into a detailed comparison between LangChain + LangGraph and OpenAI’s Swarm + GPTs + Function Calling.\n\n\n\nWhat is LangChain + LangGraph?\nLangChain is a framework that helps developers build applications powered by language models. It’s especially useful when you need to integrate various external data sources, APIs, or tools into a language model workflow.\nLangGraph complements this by allowing users to visually design and manage complex workflows using a graph-based structure. With LangGraph, you can define nodes and edges to represent tasks and their relationships, making it easier to handle complex dependencies in workflows.\nThese two tools work together to enable sophisticated, structured workflows in AI-driven web services. While LangChain provides the API integration, LangGraph offers a powerful visual interface for managing the tasks and flows.\n\n\nWhat is OpenAI Swarm + GPTs + Function Calling?\nOn the other hand, OpenAI’s Swarm, GPTs, and Function Calling offer a highly scalable and customizable environment for AI services.\nSwarm enables orchestration of multiple AI agents that work together on complex tasks, dynamically distributing them across agents. GPTs allow for custom AI models tailored to specific tasks or industries, with a user-friendly no-code interface. Function Calling allows GPT models to interact with external APIs and data sources, executing specific functions on demand.\nThis combination of tools enables developers to quickly build scalable, real-time, and highly customizable AI services. The GPT ecosystem also offers extensive plugin support, making integration with external services much more seamless.\n\n\n\nComparison Table: LangChain + LangGraph vs. OpenAI Swarm + GPTs + Function Calling\n\n\n\nFeature\nLangChain + LangGraph\nOpenAI Swarm + GPTs + Function Calling\n\n\n\n\nPrimary Functionality\nWorkflow management with LLM integration\nLLM-based service customization and orchestration with real-time data integration\n\n\nCustomization of Models\nHighly customizable workflows with LangChain’s API integrations\nGPTs offer no-code customization for specialized AI models\n\n\nWorkflow Management\nLangGraph provides visual graph-based design of tasks and dependencies\nSwarm enables dynamic task distribution among AI agents\n\n\nFunction Calling\nNot natively integrated\nFunction Calling allows real-time API interaction\n\n\nEase of Use\nRequires knowledge of LLM workflows and visual programming\nGPTs offer no-code customization, more beginner-friendly\n\n\nReal-time Data Handling\nHandled through API integration, more setup required\nFunction Calling for instant data access\n\n\nScalability\nDependent on LangChain’s infrastructure\nHighly scalable with Swarm orchestrating multiple AI agents\n\n\nIntegration with External Data\nRequires external setup via LangChain’s API tools\nFunction Calling allows seamless data integration\n\n\nMultimodal Support\nNot emphasized\nGPT-4 multimodal support for vision and text\n\n\nDeveloper Community & Ecosystem\nGrowing community, more niche\nLarge community, extensive plugin ecosystem with GPT Store\n\n\nComplexity Management\nRequires detailed setup and manual configuration\nSwarm automates much of the complexity through agent orchestration\n\n\n\n\n\n\nWhich Approach is Best for Your Web Service Development?\nThe decision between LangChain + LangGraph and OpenAI Swarm + GPTs + Function Calling comes down to your project’s specific needs and your development team’s experience.\n\nChoose LangChain + LangGraph if your project requires:\n\nA highly structured and customizable workflow for LLM-based tasks.\nFine-grained control over task dependencies and API integrations.\nA visual interface for managing complex workflows, especially useful for teams with deep knowledge of LLM systems and infrastructure.\n\nChoose OpenAI Swarm + GPTs + Function Calling if you need:\n\nFast, scalable AI-powered web services with minimal setup.\nNo-code customization options to tailor AI models to specific needs.\nReal-time API interaction and dynamic task distribution via agent orchestration.\nAccess to a broader ecosystem, including plugins, external data integration, and GPT store for sharing or downloading pre-built models.\n\n\n\n\nFinal Thoughts\nBoth LangChain + LangGraph and OpenAI Swarm + GPTs + Function Calling are robust solutions for building LLM-powered web services, but they cater to different types of development teams and project requirements. If your goal is deep customization and complex workflow management, LangChain + LangGraph may be the way to go. However, if scalability, ease of use, and rapid development are priorities, OpenAI’s Swarm + GPTs + Function Calling is likely the better choice.\nAt NinjaLABO, we’re constantly testing and optimizing these platforms to offer cutting-edge solutions to our clients, and this analysis helps guide our decisions in choosing the right tools for the job.\nReady to build your next web service? Reach out to our team for expert advice and let us help you navigate the best path forward.\n\nExplore the Platforms:\n- LangChain\n- LangGraph\n- OpenAI Swarm\n- GPTs and Function Calling"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html",
    "href": "blogs/KD_Hint_Base_V2.html",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2015). FitNets: Hints for Thin Deep Nets. arXiv preprint arXiv:1412.6550. Retrieved from https://arxiv.org/abs/1412.6550\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport time\n\n# Define transforms for data augmentation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Load the Imagenette2-320 dataset\ndata_dir = './data/imagenette2-320/imagenette2-320'\nimage_datasets = {x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n               for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\n# Check device availability\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\n# Load pre-trained teacher (ResNet50) and student (ResNet18)\nteacher_model = models.resnet50(pretrained=True)\nstudent_model = models.resnet18(pretrained=True)\n\n# Adjust final layers to match number of classes in Imagenette (10 classes)\nnum_ftrs_teacher = teacher_model.fc.in_features\nteacher_model.fc = nn.Linear(num_ftrs_teacher, 10)\n\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move models to the appropriate device (GPU if available)\nteacher_model = teacher_model.to(device)\nstudent_model = student_model.to(device)\n\n# Set teacher model to evaluation mode (as it is not being trained)\nteacher_model.eval()\n\n# Define 1x1 convolution to match the dimensions between teacher and student feature maps\n# Assuming teacher's layer3 outputs 1024 channels and student's layer3 outputs 256 channels\n\nconv_teacher_to_student = nn.Conv2d(1024, 256, kernel_size=1).to(device)\n\n\n\n\n\nWe need to extract intermediate features from both the teacher and the student models. One way to achieve this is by using forward hooks in PyTorch to capture activations at specific layers. In this case, we’ll extract features from a chosen layer in both models, for example, the output of the third residual block in both models.\n\n# Helper function to register a hook for feature extraction\ndef extract_features(module, input, output):\n    return output\n\n# Extract features from the third residual block (layer3) for both models\nteacher_features = []\nstudent_features = []\n\ndef register_hooks(model, features_storage):\n    def hook(module, input, output):\n        features_storage.append(output)\n    return hook\n\n# Register hook to extract features from teacher model (layer3 output)\nteacher_model.layer3[5].register_forward_hook(register_hooks(teacher_model, teacher_features))\n\n# Register hook to extract features from student model (layer3 output)\nstudent_model.layer3[1].register_forward_hook(register_hooks(student_model, student_features))\n\n&lt;torch.utils.hooks.RemovableHandle at 0x21cb76104c0&gt;\n\n\n\n\n\ne now define a custom loss function that combines:\n\n\nCross-Entropy Loss on the student’s hard predictions against the ground truth labels.\n\n\nKL Divergence Loss between the teacher’s and student’s soft logits (output of final layer).\n\n\nFeature Matching Loss (e.g., L2 loss) between the intermediate feature maps of the teacher and student.\n\n\n\nclass HintBasedDistillationLoss(nn.Module):\n    def __init__(self, temperature=3.0, alpha=0.5, beta=0.5):\n        super(HintBasedDistillationLoss, self).__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.beta = beta\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.l2_loss = nn.MSELoss()\n\n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Soft targets: apply temperature scaling to teacher outputs\n        teacher_soft = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_soft = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss (KL divergence between student and teacher's softened outputs)\n        distillation_loss = self.kl_div_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n\n        # Cross entropy loss (between student predictions and true labels)\n        student_loss = self.ce_loss(student_logits, labels)\n\n        # Feature matching loss (L2 loss between teacher and student feature maps)\n        feature_loss = self.l2_loss(student_features, teacher_features)\n\n        # Combined loss\n        return self.alpha * distillation_loss + (1.0 - self.alpha) * student_loss + self.beta * feature_loss\n\n\n\n\n\ndef train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = student_model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Set student model to training mode\n        student_model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over the data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Clear gradients for student model\n            optimizer.zero_grad()\n\n            # Clear the feature lists before every forward pass\n            teacher_features.clear()  # Clear saved teacher features\n            student_features.clear()  # Clear saved student features\n\n            # Forward pass through teacher (for soft labels and features)\n            with torch.no_grad():  # Disable gradients for teacher model\n                teacher_logits = teacher_model(inputs)\n                teacher_feature = teacher_features[0]  # Extract intermediate feature from teacher\n\n            # Forward pass through student\n            student_logits = student_model(inputs)\n            student_feature = student_features[0]  # Extract intermediate feature from student\n\n            # Apply 1x1 convolution to match teacher's feature map dimensions to student's\n            teacher_feature_resized = conv_teacher_to_student(teacher_feature)\n\n            # Compute loss\n            loss = criterion(student_logits, teacher_logits, student_feature, teacher_feature_resized, labels)\n\n            # Backward pass and optimization\n            loss.backward()  # Compute gradients only for the student model\n            optimizer.step()\n\n            # Compute running statistics\n            _, preds = torch.max(student_logits, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Deep copy the model\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = student_model.state_dict()\n\n    # Training complete\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    student_model.load_state_dict(best_model_wts)\n    return student_model\n\n\n\n\n\n# Define optimizer\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define hint-based distillation loss\ncriterion = HintBasedDistillationLoss(temperature=3.0, alpha=0.5, beta=0.5)\n\n# Train the student model\ntrained_student = train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=5)\n\nEpoch 0/4\n----------\nLoss: 0.6921 Acc: 0.8747\nEpoch 1/4\n----------\nLoss: 0.6239 Acc: 0.9241\nEpoch 2/4\n----------\nLoss: 0.6072 Acc: 0.9267\nEpoch 3/4\n----------\nLoss: 0.5918 Acc: 0.9378\nEpoch 4/4\n----------\nLoss: 0.5790 Acc: 0.9413\nTraining complete in 98m 5s\nBest Acc: 0.9413\n\n\n\n\n\n\ndef evaluate_model(model, dataloaders):\n    model.eval()  # Set to evaluation mode\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad(): # No need to compute gradients during evaluation\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9735"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#define-teacher-and-student-models",
    "href": "blogs/KD_Hint_Base_V2.html#define-teacher-and-student-models",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "# Load pre-trained teacher (ResNet50) and student (ResNet18)\nteacher_model = models.resnet50(pretrained=True)\nstudent_model = models.resnet18(pretrained=True)\n\n# Adjust final layers to match number of classes in Imagenette (10 classes)\nnum_ftrs_teacher = teacher_model.fc.in_features\nteacher_model.fc = nn.Linear(num_ftrs_teacher, 10)\n\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move models to the appropriate device (GPU if available)\nteacher_model = teacher_model.to(device)\nstudent_model = student_model.to(device)\n\n# Set teacher model to evaluation mode (as it is not being trained)\nteacher_model.eval()\n\n# Define 1x1 convolution to match the dimensions between teacher and student feature maps\n# Assuming teacher's layer3 outputs 1024 channels and student's layer3 outputs 256 channels\n\nconv_teacher_to_student = nn.Conv2d(1024, 256, kernel_size=1).to(device)"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#extract-intermediate-feature-representations",
    "href": "blogs/KD_Hint_Base_V2.html#extract-intermediate-feature-representations",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "We need to extract intermediate features from both the teacher and the student models. One way to achieve this is by using forward hooks in PyTorch to capture activations at specific layers. In this case, we’ll extract features from a chosen layer in both models, for example, the output of the third residual block in both models.\n\n# Helper function to register a hook for feature extraction\ndef extract_features(module, input, output):\n    return output\n\n# Extract features from the third residual block (layer3) for both models\nteacher_features = []\nstudent_features = []\n\ndef register_hooks(model, features_storage):\n    def hook(module, input, output):\n        features_storage.append(output)\n    return hook\n\n# Register hook to extract features from teacher model (layer3 output)\nteacher_model.layer3[5].register_forward_hook(register_hooks(teacher_model, teacher_features))\n\n# Register hook to extract features from student model (layer3 output)\nstudent_model.layer3[1].register_forward_hook(register_hooks(student_model, student_features))\n\n&lt;torch.utils.hooks.RemovableHandle at 0x21cb76104c0&gt;"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#define-the-custom-distillation-loss",
    "href": "blogs/KD_Hint_Base_V2.html#define-the-custom-distillation-loss",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "e now define a custom loss function that combines:\n\n\nCross-Entropy Loss on the student’s hard predictions against the ground truth labels.\n\n\nKL Divergence Loss between the teacher’s and student’s soft logits (output of final layer).\n\n\nFeature Matching Loss (e.g., L2 loss) between the intermediate feature maps of the teacher and student.\n\n\n\nclass HintBasedDistillationLoss(nn.Module):\n    def __init__(self, temperature=3.0, alpha=0.5, beta=0.5):\n        super(HintBasedDistillationLoss, self).__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.beta = beta\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.l2_loss = nn.MSELoss()\n\n    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n        # Soft targets: apply temperature scaling to teacher outputs\n        teacher_soft = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_soft = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss (KL divergence between student and teacher's softened outputs)\n        distillation_loss = self.kl_div_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n\n        # Cross entropy loss (between student predictions and true labels)\n        student_loss = self.ce_loss(student_logits, labels)\n\n        # Feature matching loss (L2 loss between teacher and student feature maps)\n        feature_loss = self.l2_loss(student_features, teacher_features)\n\n        # Combined loss\n        return self.alpha * distillation_loss + (1.0 - self.alpha) * student_loss + self.beta * feature_loss"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#implement-the-training-loop",
    "href": "blogs/KD_Hint_Base_V2.html#implement-the-training-loop",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "def train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = student_model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Set student model to training mode\n        student_model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over the data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Clear gradients for student model\n            optimizer.zero_grad()\n\n            # Clear the feature lists before every forward pass\n            teacher_features.clear()  # Clear saved teacher features\n            student_features.clear()  # Clear saved student features\n\n            # Forward pass through teacher (for soft labels and features)\n            with torch.no_grad():  # Disable gradients for teacher model\n                teacher_logits = teacher_model(inputs)\n                teacher_feature = teacher_features[0]  # Extract intermediate feature from teacher\n\n            # Forward pass through student\n            student_logits = student_model(inputs)\n            student_feature = student_features[0]  # Extract intermediate feature from student\n\n            # Apply 1x1 convolution to match teacher's feature map dimensions to student's\n            teacher_feature_resized = conv_teacher_to_student(teacher_feature)\n\n            # Compute loss\n            loss = criterion(student_logits, teacher_logits, student_feature, teacher_feature_resized, labels)\n\n            # Backward pass and optimization\n            loss.backward()  # Compute gradients only for the student model\n            optimizer.step()\n\n            # Compute running statistics\n            _, preds = torch.max(student_logits, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Deep copy the model\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = student_model.state_dict()\n\n    # Training complete\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    student_model.load_state_dict(best_model_wts)\n    return student_model"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#training-and-evaluation",
    "href": "blogs/KD_Hint_Base_V2.html#training-and-evaluation",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "# Define optimizer\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define hint-based distillation loss\ncriterion = HintBasedDistillationLoss(temperature=3.0, alpha=0.5, beta=0.5)\n\n# Train the student model\ntrained_student = train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=5)\n\nEpoch 0/4\n----------\nLoss: 0.6921 Acc: 0.8747\nEpoch 1/4\n----------\nLoss: 0.6239 Acc: 0.9241\nEpoch 2/4\n----------\nLoss: 0.6072 Acc: 0.9267\nEpoch 3/4\n----------\nLoss: 0.5918 Acc: 0.9378\nEpoch 4/4\n----------\nLoss: 0.5790 Acc: 0.9413\nTraining complete in 98m 5s\nBest Acc: 0.9413"
  },
  {
    "objectID": "blogs/KD_Hint_Base_V2.html#evaluation",
    "href": "blogs/KD_Hint_Base_V2.html#evaluation",
    "title": "Knowledge Distillation Implementation 2/3",
    "section": "",
    "text": "def evaluate_model(model, dataloaders):\n    model.eval()  # Set to evaluation mode\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad(): # No need to compute gradients during evaluation\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9735"
  },
  {
    "objectID": "blogs/dnn_comp_techs.html",
    "href": "blogs/dnn_comp_techs.html",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Deep Neural Networks (DNNs) have achieved remarkable success across various domains, from image recognition to natural language processing. However, their deployment, especially in resource-constrained environments like mobile devices or edge computing, is often hindered by their size and computational demands. Model compression techniques are essential to address these challenges, enabling the use of DNNs in real-world applications without sacrificing too much performance. This article provides a comprehensive overview of the major DNN model compression techniques, presenting a big picture of how each approach contributes to making neural networks more efficient.\n\n\n\nQuantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage.\n\n\n\nPruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints.\n\n\n\n\nKnowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model.\n\n\n\n\nLow-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode.\n\n\n\n\n\nBatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy.\n\n\n\nWeight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity.\n\n\n\nNeural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#quantization",
    "href": "blogs/dnn_comp_techs.html#quantization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Quantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#pruning",
    "href": "blogs/dnn_comp_techs.html#pruning",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Pruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "href": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Knowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "href": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Low-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "href": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "BatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#weight-sharing",
    "href": "blogs/dnn_comp_techs.html#weight-sharing",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Weight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "href": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Neural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/tinymlaas.html",
    "href": "blogs/tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html",
    "href": "blogs/Knowledge_Distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "In the world of AI and deep learning, bigger models usually mean better performance. These models, often referred to as deep neural networks (DNNs), have taken artificial intelligence to new heights by achieving remarkable success in computer vision, natural language processing (NLP), and other domains. However, their sheer size and complexity pose significant challenges when it comes to deploying them on devices with limited resources, such as mobile phones and embedded systems. Enter knowledge distillation, an efficient solution to compress large models into smaller, faster, and more resource-friendly versions without compromising accuracy.\n\n\n\nAt its core, knowledge distillation (KD) is a technique used to transfer the knowledge from a large model (referred to as the teacher model) into a smaller model (the student model). The primary objective is to reduce the size of the deep neural network while maintaining a similar level of performance. This method has become increasingly popular in the AI community, as it offers a way to bring complex models to devices with limited computational power. The concept of KD was popularized by Hinton et al. in 2015. The idea is that instead of training a small model from scratch, it can be trained to mimic the outputs of a large model, allowing the student model to learn more efficiently and effectively. By doing so, the student model not only compresses the size of the neural network but also retains much of the functionality of the larger teacher model.\n\n\n\n\nThe process of knowledge distillation generally involves three components:\n\nThe Teacher Model: This is the pre-trained, large neural network that has high accuracy but is computationally expensive.\nThe Student Model: This smaller model aims to replicate the performance of the teacher model while being more efficient in terms of memory and speed.\nDistillation Process: The student model learns by mimicking the teacher’s outputs. The goal is to make the student model as accurate as possible, often by using a special loss function that helps it learn from the teacher’s “soft predictions” (or probability outputs) rather than the hard, ground-truth labels.\n\nIn a typical KD scenario, the teacher model is first trained on a dataset. Then, during the distillation phase, the student model is trained not only using the true labels of the dataset but also by trying to match the teacher’s outputs. These outputs often include subtle patterns and relationships in the data that the teacher has learned, which help guide the student model towards better generalization and accuracy.\n\n\n\nThe knowledge that the student model learns from the teacher can come in various forms. These include:\n\n\n\n\nResponse-Based Knowledge: This refers to the outputs or predictions made by the teacher model. The student model learns by mimicking the probability distribution of the teacher’s predictions, often referred to as “soft labels.”\nFeature-Based Knowledge: Instead of just learning from the final outputs, the student model can also learn from the intermediate representations, or feature maps, generated by the teacher. This helps the student model capture more fine-grained information about the data.\nRelation-Based Knowledge: In some cases, the relationships between different samples or features are used to guide the student model. This could involve learning the similarities and differences between pairs of data points or understanding how various features interact.\n\nEach of these methods provides unique benefits, and depending on the use case, one might be more suitable than the others.\n\n\n\n\n\n\n\n\nThere are several strategies to perform knowledge distillation, each offering different advantages:\n\nOffline Distillation: This is the most common method, where the teacher model is pre-trained and remains fixed during the distillation process. The student model learns from this static teacher, making the process simple and easy to implement.\nOnline Distillation: In this scheme, both the teacher and the student models are trained simultaneously. This is useful when a high-performance teacher model is not available from the start, allowing the two models to evolve together during training.\nSelf-Distillation: In this variant, the teacher and the student are essentially the same model. Different parts of the model (for example, deeper layers) act as the teacher to guide earlier layers (the student). This strategy has been shown to improve performance even within a single network.\n\n\n\n\n\n\n\n\nKnowledge distillation (KD) has revolutionized the process of model compression. This process enables lightweight models to perform complex tasks while retaining most of the teacher’s capabilities. Over time, numerous specialized distillation algorithms have been developed, each designed to address specific challenges or make distillation more effective. A number of prominent KD algorithms are explored in this section, along with their unique approaches and applications.\n\n\nAdversarial KD combines the principles of Generative Adversarial Networks (GANs) with distillation to refine how student models mimic teachers. Here, a generator-discriminator dynamic is introduced where the student model acts as a generator trying to fool a discriminator into thinking its outputs are from the teacher.\n\n\n\nSynthetic Data Generation: A GAN generates synthetic data that aids the student in understanding complex data distributions. This data either supplements or entirely replaces the training dataset.\nDiscriminative Supervision: The student model is trained against a discriminator that distinguishes between teacher and student outputs, ensuring the student closely mimics the teacher.\n\nThis method is particularly effective in improving KD performance in scenarios where labeled data is sparse, as the adversarial mechanism helps the student model learn complex data patterns efficiently .\n\n\n\n\n\n\n\n\nIn Multi-Teacher KD, a student model learns from multiple teachers rather than a single one, each offering distinct knowledge or expertise. This approach is useful when the teachers are trained on different datasets or specialize in different aspects of a task.\n\n\n\nAveraged Response: The simplest implementation involves averaging the outputs from multiple teachers, allowing the student to benefit from diverse viewpoints.\nFeature and Logit Transfer: Some implementations utilize both logits and intermediate feature maps from various teachers to provide the student with a comprehensive understanding of the task.\n\nThis method is beneficial in situations like object detection, where combining multiple expert models can improve the student’s performance across diverse categories.\n\n\n\n\n\n\n\n\nCross-Modal KD enables knowledge transfer between models trained on different modalities (e.g., vision, text, or audio). For example, a teacher model trained on RGB images can transfer its knowledge to a student model working with depth images.\n\n\n\nPaired Modality Learning: Often, paired data (e.g., RGB and depth images) is used, where knowledge learned by the teacher in one modality guides the student’s learning in another modality.\nHallucination Streams: In some cases, a hallucination stream is generated for a missing modality, such as generating depth features from RGB inputs to guide the student.\n\nThis technique is ideal for scenarios where data from one modality is scarce, such as human pose estimation using radio frequency signals paired with RGB video inputs.\n\n\n\n\n\n\n\n\nGraph-Based KD models knowledge as a graph, where vertices represent data instances or features, and edges capture relationships between them. This allows the student model to learn not only from individual outputs but also from the relationships between data points.\n\n\n\nIntra-Data Relations: The student learns from graphs that represent relationships between data points, such as the similarity between features or mutual relations.\nGraph Construction: Graphs can be built from logits, feature maps, or even a combination of the two, allowing for a rich transfer of relational knowledge.\n\nThis method excels in applications requiring an understanding of complex data structures, such as social network analysis or recommender systems, where the relationships between data points are as important as the points themselves.\n\n\n\n\n\n\n\n\nAttention-Based KD leverages attention mechanisms within the teacher model to highlight important features, which are then transferred to the student. Attention maps, which indicate which parts of the input are most important, guide the student in focusing on critical aspects of the data.\n\n\n\nAttention Maps: The student model is trained to replicate the attention maps of the teacher, learning to focus on the same areas of the input data.\nConfidence Assignment: Some methods also use attention mechanisms to assign different confidence levels to various parts of the data, ensuring the student model prioritizes the right features.\n\nThis approach is especially useful in image classification and object detection, where attention maps can guide the student in focusing on specific regions of an image.\n\n\n\n\n\nIn Data-Free KD, the student model is trained without access to the original training data. This is often necessary in situations where the data is sensitive, such as in healthcare or legal domains.\n\n\n\nSynthetic Data Generation: Techniques like GANs or layer activations are used to generate synthetic data that mimics the original dataset, which is then used to train the student model.\nUnlabeled Knowledge Transfer: In some cases, the student learns by replicating the teacher’s outputs without any real data.\n\nThis method is crucial when data privacy is a concern, such as in medical AI applications, where access to patient data may be restricted.\n\n\n\n\n\nQuantized KD deals with converting teacher models into low-precision student models (such as 8-bit or 4-bit representations) without losing significant accuracy. This helps in deploying models on resource-constrained devices.\n\n\n\nPrecision Reduction: The weights and activations of the student model are reduced in precision, making it more efficient for inference on edge devices.\nLoss Minimization: Techniques are employed to minimize the loss in performance due to quantization.\n\nThis method is widely used in deploying AI models on mobile devices and embedded systems, where computational and memory resources are limited.\n\n\n\n\n\n\n\n\nLifelong KD aims to continually update the student model as new tasks or data are introduced, allowing it to learn from the teacher while preserving its knowledge of previous tasks. This is a crucial aspect of lifelong learning in AI.\n\n\n\nTask Preservation: The student model is designed to retain knowledge from previous tasks while learning new ones, reducing the risk of catastrophic forgetting.\nContinuous Learning: New knowledge is distilled incrementally, making the student adaptive to evolving data.\n\nLifelong KD is important in applications like autonomous driving, where the model needs to adapt to new environments without forgetting previously learned knowledge.\n\n\n\n\n\nNeural Architecture Search (NAS)-Based KD integrates NAS into the distillation process, automating the search for the optimal student architecture under the guidance of the teacher model. This approach aims to find the best possible student model structure through data-driven optimization.\n\n\n\nAutomated Architecture Search: NAS methods search for an efficient student model architecture that best matches the teacher’s performance.\nReinforcement Learning: Some NAS-based KD methods use reinforcement learning to dynamically search for and prune unnecessary layers in the student model .\n\nThis method is highly useful in optimizing model performance across a variety of applications, including speech recognition and natural language processing.\n\n\n\n\n\n\nKnowledge distillation has found applications across various domains of artificial intelligence:\n\nVisual Recognition: Distilled models are widely used in tasks such as image classification, object detection, and human pose estimation. Smaller models are crucial for deployment on devices with limited computational power, such as smartphones or embedded cameras.\nNatural Language Processing (NLP): The success of models like BERT and GPT has revolutionized NLP, but their massive size makes them impractical for real-time applications. Distilled versions of these models have been created to retain their impressive performance while being small enough for deployment in resource-constrained environments.\nSpeech Recognition: Similar to other AI tasks, large models can achieve great accuracy in speech recognition, but distilled models enable their use in real-time applications like virtual assistants, where speed and memory are critical.\n\n\n\n\n\nWhile knowledge distillation is a promising approach to compressing deep learning models, it still faces several challenges:\n\nModel Capacity Gap: If the student model is too small compared to the teacher, it may struggle to capture all the necessary knowledge, leading to a performance drop. Researchers are exploring ways to bridge this gap, such as introducing intermediate “assistant” models to facilitate the transfer of knowledge.\nAdversarial Distillation: Incorporating adversarial learning into distillation, where a student model learns to fool a discriminator into thinking its outputs come from the teacher, is an emerging area that could improve the effectiveness of KD in complex tasks.\nCross-Modal Distillation: Transferring knowledge between different modalities (e.g., from images to text or sound) is another exciting frontier that could open up new applications, especially in multi-modal AI systems.\n\n\n\n\n\nThe evolving landscape of knowledge distillation has given rise to a diverse set of algorithms, each designed to tackle specific challenges. From adversarial techniques and multi-teacher models to cross-modal transfers and lifelong learning, these algorithms make it possible to compress complex deep learning models without compromising on accuracy. As AI continues to move towards more resource-efficient solutions, these distillation methods will be crucial in enabling powerful models to run on everyday devices.\nKnowledge distillation is a powerful tool for reducing the size of deep neural networks while maintaining their accuracy. As AI continues to advance, this technique will play a crucial role in making sophisticated models accessible on everyday devices. With ongoing research addressing current challenges, knowledge distillation is poised to become a foundational technique in the future of AI and machine learning."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#what-is-knowledge-distillation",
    "href": "blogs/Knowledge_Distillation.html#what-is-knowledge-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "At its core, knowledge distillation (KD) is a technique used to transfer the knowledge from a large model (referred to as the teacher model) into a smaller model (the student model). The primary objective is to reduce the size of the deep neural network while maintaining a similar level of performance. This method has become increasingly popular in the AI community, as it offers a way to bring complex models to devices with limited computational power. The concept of KD was popularized by Hinton et al. in 2015. The idea is that instead of training a small model from scratch, it can be trained to mimic the outputs of a large model, allowing the student model to learn more efficiently and effectively. By doing so, the student model not only compresses the size of the neural network but also retains much of the functionality of the larger teacher model."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#how-does-knowledge-distillation-work",
    "href": "blogs/Knowledge_Distillation.html#how-does-knowledge-distillation-work",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "The process of knowledge distillation generally involves three components:\n\nThe Teacher Model: This is the pre-trained, large neural network that has high accuracy but is computationally expensive.\nThe Student Model: This smaller model aims to replicate the performance of the teacher model while being more efficient in terms of memory and speed.\nDistillation Process: The student model learns by mimicking the teacher’s outputs. The goal is to make the student model as accurate as possible, often by using a special loss function that helps it learn from the teacher’s “soft predictions” (or probability outputs) rather than the hard, ground-truth labels.\n\nIn a typical KD scenario, the teacher model is first trained on a dataset. Then, during the distillation phase, the student model is trained not only using the true labels of the dataset but also by trying to match the teacher’s outputs. These outputs often include subtle patterns and relationships in the data that the teacher has learned, which help guide the student model towards better generalization and accuracy.\n\n\n\nThe knowledge that the student model learns from the teacher can come in various forms. These include:\n\n\n\n\nResponse-Based Knowledge: This refers to the outputs or predictions made by the teacher model. The student model learns by mimicking the probability distribution of the teacher’s predictions, often referred to as “soft labels.”\nFeature-Based Knowledge: Instead of just learning from the final outputs, the student model can also learn from the intermediate representations, or feature maps, generated by the teacher. This helps the student model capture more fine-grained information about the data.\nRelation-Based Knowledge: In some cases, the relationships between different samples or features are used to guide the student model. This could involve learning the similarities and differences between pairs of data points or understanding how various features interact.\n\nEach of these methods provides unique benefits, and depending on the use case, one might be more suitable than the others."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#distillation-schemes-offline-online-and-self-distillation",
    "href": "blogs/Knowledge_Distillation.html#distillation-schemes-offline-online-and-self-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "There are several strategies to perform knowledge distillation, each offering different advantages:\n\nOffline Distillation: This is the most common method, where the teacher model is pre-trained and remains fixed during the distillation process. The student model learns from this static teacher, making the process simple and easy to implement.\nOnline Distillation: In this scheme, both the teacher and the student models are trained simultaneously. This is useful when a high-performance teacher model is not available from the start, allowing the two models to evolve together during training.\nSelf-Distillation: In this variant, the teacher and the student are essentially the same model. Different parts of the model (for example, deeper layers) act as the teacher to guide earlier layers (the student). This strategy has been shown to improve performance even within a single network."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#knowledge-distillation-algorithms",
    "href": "blogs/Knowledge_Distillation.html#knowledge-distillation-algorithms",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge distillation (KD) has revolutionized the process of model compression. This process enables lightweight models to perform complex tasks while retaining most of the teacher’s capabilities. Over time, numerous specialized distillation algorithms have been developed, each designed to address specific challenges or make distillation more effective. A number of prominent KD algorithms are explored in this section, along with their unique approaches and applications.\n\n\nAdversarial KD combines the principles of Generative Adversarial Networks (GANs) with distillation to refine how student models mimic teachers. Here, a generator-discriminator dynamic is introduced where the student model acts as a generator trying to fool a discriminator into thinking its outputs are from the teacher.\n\n\n\nSynthetic Data Generation: A GAN generates synthetic data that aids the student in understanding complex data distributions. This data either supplements or entirely replaces the training dataset.\nDiscriminative Supervision: The student model is trained against a discriminator that distinguishes between teacher and student outputs, ensuring the student closely mimics the teacher.\n\nThis method is particularly effective in improving KD performance in scenarios where labeled data is sparse, as the adversarial mechanism helps the student model learn complex data patterns efficiently .\n\n\n\n\n\n\n\n\nIn Multi-Teacher KD, a student model learns from multiple teachers rather than a single one, each offering distinct knowledge or expertise. This approach is useful when the teachers are trained on different datasets or specialize in different aspects of a task.\n\n\n\nAveraged Response: The simplest implementation involves averaging the outputs from multiple teachers, allowing the student to benefit from diverse viewpoints.\nFeature and Logit Transfer: Some implementations utilize both logits and intermediate feature maps from various teachers to provide the student with a comprehensive understanding of the task.\n\nThis method is beneficial in situations like object detection, where combining multiple expert models can improve the student’s performance across diverse categories.\n\n\n\n\n\n\n\n\nCross-Modal KD enables knowledge transfer between models trained on different modalities (e.g., vision, text, or audio). For example, a teacher model trained on RGB images can transfer its knowledge to a student model working with depth images.\n\n\n\nPaired Modality Learning: Often, paired data (e.g., RGB and depth images) is used, where knowledge learned by the teacher in one modality guides the student’s learning in another modality.\nHallucination Streams: In some cases, a hallucination stream is generated for a missing modality, such as generating depth features from RGB inputs to guide the student.\n\nThis technique is ideal for scenarios where data from one modality is scarce, such as human pose estimation using radio frequency signals paired with RGB video inputs.\n\n\n\n\n\n\n\n\nGraph-Based KD models knowledge as a graph, where vertices represent data instances or features, and edges capture relationships between them. This allows the student model to learn not only from individual outputs but also from the relationships between data points.\n\n\n\nIntra-Data Relations: The student learns from graphs that represent relationships between data points, such as the similarity between features or mutual relations.\nGraph Construction: Graphs can be built from logits, feature maps, or even a combination of the two, allowing for a rich transfer of relational knowledge.\n\nThis method excels in applications requiring an understanding of complex data structures, such as social network analysis or recommender systems, where the relationships between data points are as important as the points themselves.\n\n\n\n\n\n\n\n\nAttention-Based KD leverages attention mechanisms within the teacher model to highlight important features, which are then transferred to the student. Attention maps, which indicate which parts of the input are most important, guide the student in focusing on critical aspects of the data.\n\n\n\nAttention Maps: The student model is trained to replicate the attention maps of the teacher, learning to focus on the same areas of the input data.\nConfidence Assignment: Some methods also use attention mechanisms to assign different confidence levels to various parts of the data, ensuring the student model prioritizes the right features.\n\nThis approach is especially useful in image classification and object detection, where attention maps can guide the student in focusing on specific regions of an image.\n\n\n\n\n\nIn Data-Free KD, the student model is trained without access to the original training data. This is often necessary in situations where the data is sensitive, such as in healthcare or legal domains.\n\n\n\nSynthetic Data Generation: Techniques like GANs or layer activations are used to generate synthetic data that mimics the original dataset, which is then used to train the student model.\nUnlabeled Knowledge Transfer: In some cases, the student learns by replicating the teacher’s outputs without any real data.\n\nThis method is crucial when data privacy is a concern, such as in medical AI applications, where access to patient data may be restricted.\n\n\n\n\n\nQuantized KD deals with converting teacher models into low-precision student models (such as 8-bit or 4-bit representations) without losing significant accuracy. This helps in deploying models on resource-constrained devices.\n\n\n\nPrecision Reduction: The weights and activations of the student model are reduced in precision, making it more efficient for inference on edge devices.\nLoss Minimization: Techniques are employed to minimize the loss in performance due to quantization.\n\nThis method is widely used in deploying AI models on mobile devices and embedded systems, where computational and memory resources are limited.\n\n\n\n\n\n\n\n\nLifelong KD aims to continually update the student model as new tasks or data are introduced, allowing it to learn from the teacher while preserving its knowledge of previous tasks. This is a crucial aspect of lifelong learning in AI.\n\n\n\nTask Preservation: The student model is designed to retain knowledge from previous tasks while learning new ones, reducing the risk of catastrophic forgetting.\nContinuous Learning: New knowledge is distilled incrementally, making the student adaptive to evolving data.\n\nLifelong KD is important in applications like autonomous driving, where the model needs to adapt to new environments without forgetting previously learned knowledge.\n\n\n\n\n\nNeural Architecture Search (NAS)-Based KD integrates NAS into the distillation process, automating the search for the optimal student architecture under the guidance of the teacher model. This approach aims to find the best possible student model structure through data-driven optimization.\n\n\n\nAutomated Architecture Search: NAS methods search for an efficient student model architecture that best matches the teacher’s performance.\nReinforcement Learning: Some NAS-based KD methods use reinforcement learning to dynamically search for and prune unnecessary layers in the student model .\n\nThis method is highly useful in optimizing model performance across a variety of applications, including speech recognition and natural language processing."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#applications-of-knowledge-distillation",
    "href": "blogs/Knowledge_Distillation.html#applications-of-knowledge-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge distillation has found applications across various domains of artificial intelligence:\n\nVisual Recognition: Distilled models are widely used in tasks such as image classification, object detection, and human pose estimation. Smaller models are crucial for deployment on devices with limited computational power, such as smartphones or embedded cameras.\nNatural Language Processing (NLP): The success of models like BERT and GPT has revolutionized NLP, but their massive size makes them impractical for real-time applications. Distilled versions of these models have been created to retain their impressive performance while being small enough for deployment in resource-constrained environments.\nSpeech Recognition: Similar to other AI tasks, large models can achieve great accuracy in speech recognition, but distilled models enable their use in real-time applications like virtual assistants, where speed and memory are critical."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#challenges-and-future-directions",
    "href": "blogs/Knowledge_Distillation.html#challenges-and-future-directions",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "While knowledge distillation is a promising approach to compressing deep learning models, it still faces several challenges:\n\nModel Capacity Gap: If the student model is too small compared to the teacher, it may struggle to capture all the necessary knowledge, leading to a performance drop. Researchers are exploring ways to bridge this gap, such as introducing intermediate “assistant” models to facilitate the transfer of knowledge.\nAdversarial Distillation: Incorporating adversarial learning into distillation, where a student model learns to fool a discriminator into thinking its outputs come from the teacher, is an emerging area that could improve the effectiveness of KD in complex tasks.\nCross-Modal Distillation: Transferring knowledge between different modalities (e.g., from images to text or sound) is another exciting frontier that could open up new applications, especially in multi-modal AI systems."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#conclusion",
    "href": "blogs/Knowledge_Distillation.html#conclusion",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "The evolving landscape of knowledge distillation has given rise to a diverse set of algorithms, each designed to tackle specific challenges. From adversarial techniques and multi-teacher models to cross-modal transfers and lifelong learning, these algorithms make it possible to compress complex deep learning models without compromising on accuracy. As AI continues to move towards more resource-efficient solutions, these distillation methods will be crucial in enabling powerful models to run on everyday devices.\nKnowledge distillation is a powerful tool for reducing the size of deep neural networks while maintaining their accuracy. As AI continues to advance, this technique will play a crucial role in making sophisticated models accessible on everyday devices. With ongoing research addressing current challenges, knowledge distillation is poised to become a foundational technique in the future of AI and machine learning."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#the-need-for-faster-ai",
    "href": "blogs/Knowledge_Distillation.html#the-need-for-faster-ai",
    "title": "Knowledge Distillation",
    "section": "The Need for Faster AI",
    "text": "The Need for Faster AI\nAI models, especially deep learning models, are known for their ability to handle large-scale data and solve complex tasks. However, this power comes at a cost—many AI models are large, computationally intensive, and require significant memory and energy. As AI becomes increasingly integrated into real-time applications like autonomous driving, healthcare, mobile applications, and edge computing, deploying massive models on devices with limited resources becomes a significant challenge.\nFor example: - Mobile applications need AI models that can run quickly and efficiently on smartphones with limited memory and processing power. - Edge computing devices, like IoT sensors and cameras, require lightweight AI models that deliver real-time results without relying on powerful cloud servers. - Autonomous vehicles depend on fast, real-time decision-making AI models that must run efficiently without draining the battery or overloading the hardware.\nTo meet these growing demands, FasterAI offers a comprehensive solution that makes AI models more accessible, efficient, and ready for deployment across various platforms."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#how-fasterai-works",
    "href": "blogs/Knowledge_Distillation.html#how-fasterai-works",
    "title": "Knowledge Distillation",
    "section": "How FasterAI Works",
    "text": "How FasterAI Works\nFasterAI streamlines the development and deployment of AI models using several advanced techniques. Here’s how it works:\n\n1. Model Compression\nModel compression is a critical technique for reducing the size of deep learning models while retaining their accuracy. FasterAI uses compression techniques such as pruning, quantization, and weight sharing to shrink the model’s size and make it faster.\n\nPruning: This process removes unnecessary weights and neurons from a neural network, reducing the model’s complexity without affecting performance.\nQuantization: By converting high-precision models (e.g., 32-bit floating point) into lower precision (e.g., 8-bit or 4-bit models), FasterAI drastically reduces the memory footprint while keeping the model’s accuracy intact.\nWeight Sharing: FasterAI identifies and merges similar weights in the model, allowing it to run more efficiently.\n\nWith these techniques, FasterAI reduces both the storage and computational costs of deploying large models on smaller devices.\n\n\n2. Knowledge Distillation\nKnowledge distillation is a process where a large, complex AI model (the teacher) transfers its knowledge to a smaller model (the student). The student model is trained to mimic the behavior of the teacher, delivering similar performance while being much smaller and faster.\nFasterAI uses knowledge distillation to: - Shrink models: Student models are much smaller in size compared to teacher models but retain nearly the same level of performance. - Improve efficiency: Distilled models are optimized for faster inference, making them ideal for real-time applications. - Ensure accuracy: Despite the reduction in size, FasterAI ensures that the smaller models maintain high accuracy, allowing them to be used in mission-critical applications.\n\n\n3. Neural Architecture Search (NAS)\nNeural Architecture Search (NAS) is a technique that automates the process of designing AI models. Instead of manually choosing the architecture of a neural network, FasterAI uses NAS to explore different architectures and find the most efficient model for a specific task.\n\nAutomated Search: NAS automatically searches for the optimal model structure, balancing speed and accuracy.\nCustomized Models: FasterAI tailors models to specific hardware requirements, ensuring that AI applications can run smoothly across different platforms—whether it’s a cloud server or an embedded device.\nTask-Specific Optimization: With NAS, FasterAI generates models that are highly optimized for particular tasks, ensuring maximum performance and minimal resource usage.\n\nThis approach enables FasterAI to create models that are both high-performing and efficient, with minimal human intervention."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#why-choose-fasterai",
    "href": "blogs/Knowledge_Distillation.html#why-choose-fasterai",
    "title": "Knowledge Distillation",
    "section": "Why Choose FasterAI?",
    "text": "Why Choose FasterAI?\nFasterAI is not just another AI framework—it’s a comprehensive solution designed to tackle the growing demand for faster, more efficient AI models. Here’s why FasterAI stands out:\n\n1. Real-Time AI\nFasterAI is built to support real-time applications that require immediate processing, such as autonomous vehicles, facial recognition, and augmented reality. By optimizing models for speed, FasterAI ensures low-latency performance, allowing AI models to run in real time without lag.\n\n\n2. Cross-Platform Deployment\nWith FasterAI, AI models can be easily deployed across various platforms—from cloud servers and desktops to mobile devices and edge hardware. Whether you’re working on a mobile app or an IoT project, FasterAI ensures that your models run smoothly, regardless of the hardware limitations.\n\n\n3. Scalability\nAs businesses scale, so do their AI needs. FasterAI provides a scalable solution by making it easy to deploy models on a wide range of devices. From small-scale projects to large enterprise applications, FasterAI enables seamless scaling without the need for complex retraining or re-engineering.\n\n\n4. Energy Efficiency\nIn scenarios where power consumption is a major concern (e.g., edge computing, battery-operated devices), FasterAI optimizes models to use less energy while maintaining high performance. This is especially important for applications like wearable technology or smart home devices.\n\n\n5. Enhanced User Experience\nFasterAI enables AI models to deliver results faster, enhancing user experiences in applications that rely on AI for real-time feedback. Whether it’s a voice assistant responding instantly or an app that processes images in a fraction of a second, FasterAI ensures your AI enhances, not hinders, user experience."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#applications-of-fasterai",
    "href": "blogs/Knowledge_Distillation.html#applications-of-fasterai",
    "title": "Knowledge Distillation",
    "section": "Applications of FasterAI",
    "text": "Applications of FasterAI\nFasterAI can be applied across various industries, making AI faster and more accessible in real-world scenarios:\n\nHealthcare: FasterAI compresses complex diagnostic models, enabling them to run on portable devices in hospitals, improving patient care without needing bulky hardware.\nAutonomous Driving: In autonomous vehicles, FasterAI ensures that AI models can make quick, real-time decisions while conserving battery life and reducing hardware demands.\nRetail: FasterAI powers AI-driven recommendation systems and customer service bots in real-time, enhancing the shopping experience for customers and driving more sales.\nSmart Devices: From smartwatches to smart speakers, FasterAI makes AI possible on devices with limited computing power, allowing real-time AI-driven features like voice recognition and activity tracking.\nFinance: In banking and financial services, FasterAI optimizes AI models used for fraud detection and algorithmic trading, ensuring they run efficiently with minimal delay."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#conclusion-1",
    "href": "blogs/Knowledge_Distillation.html#conclusion-1",
    "title": "Knowledge Distillation",
    "section": "Conclusion",
    "text": "Conclusion\nIn an era where AI is becoming indispensable in our daily lives, FasterAI is the solution that bridges the gap between cutting-edge AI capabilities and the practical need for speed, efficiency, and scalability. By leveraging advanced techniques like model compression, knowledge distillation, and neural architecture search, FasterAI delivers fast, efficient AI models that are easy to deploy across a wide range of devices.\nWhether you’re developing mobile apps, working on IoT solutions, or deploying large-scale AI systems, FasterAI offers the tools to make AI work smarter, faster, and more efficiently—paving the way for the future of AI-powered innovation."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#references",
    "href": "blogs/Knowledge_Distillation.html#references",
    "title": "Knowledge Distillation",
    "section": "References",
    "text": "References\nhttps://github.com/Jingnan-Jia/Awesome-Knowledge-Distillation\nhttps://arxiv.org/pdf/2006.05525"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html",
    "href": "blogs/KD_Soft_Target_V1.html",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "Soft Targets (Logits Distillation): The student model is trained not only on the hard labels (i.e., the ground truth labels), but also on the soft targets provided by the teacher model. The soft targets are the probabilities output by the teacher model after applying a temperature scaling to the logits (pre-softmax outputs).\nAn intuitive example of hard and soft targets for knowledge distillation:\nThe output of the network with no activation function is Logit.\n\n\n\n\n\n\n\n\nThink of the temperature as a dial that controls how “sharp” or “smooth” the teacher model’s predictions are. Normally, when a model makes predictions, it assigns high confidence to one class (the correct one) and very low confidence to others. This results in a “spiky” output, where one class has a high probability, and all others are close to zero.\n\n\n\n\n“Softening” refers to this process of making the output probabilities smoother (less sharp) when the temperature is increased. It’s called softening because the output becomes “softer” and less certain, giving some probability to even incorrect classes. This softened output is called soft targets.\n\n\n\n\nHinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531. Retrieved from https://arxiv.org/abs/1503.02531\nhttps://www.youtube.com/watch?v=zhrfBCNSO1Q\nhttps://www.youtube.com/watch?v=G0_lB1Ce65c&list=PLRhS6hiNUzERWXK6KE150aS8MJg-sVUCB&index=2\nhttps://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n\n\n# !pip install torch torchvision\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport time\n\n\n\n\n\nPreprocess the Imagenette2-320 dataset. Apply transforms to the images (resizing, normalization, etc.).\nThe input images are RGB, so they have 3 channels and are 32x32 pixels. Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging from 0 to 255. A common practice in neural networks is to normalize the input, which is done for multiple reasons, including avoiding saturation in commonly used activation functions and increasing numerical stability. Our normalization process consists of subtracting the mean and dividing by the standard deviation along each channel. The tensors “mean=[0.485, 0.456, 0.406]” and “std=[0.229, 0.224, 0.225]” were already computed, and they represent the mean and standard deviation of each channel in the predefined subset of dataset intended to be the training set. (https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)\n\n# Image Preprocessing and Augmentation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Load Dataset\ndata_dir =  './data/imagenette2-320/imagenette2-320'\nimage_datasets = {x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n               for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\n# Load Teacher (ResNet50) and Student (ResNet18)\nteacher_model = models.resnet50(pretrained=True)\nstudent_model = models.resnet18(pretrained=True)\n\n# Modify the last layer to match the number of classes (Imagenette has 10 classes)\nnum_ftrs_teacher = teacher_model.fc.in_features\nteacher_model.fc = nn.Linear(num_ftrs_teacher, 10)\n\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move models to the appropriate device (GPU if available)\nteacher_model = teacher_model.to(device)\nstudent_model = student_model.to(device)\n\n# Set teacher to evaluation mode, as its weights are frozen\nteacher_model.eval()\n\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\layal/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n\n\n\n\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\layal/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n\n\n\n\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=10, bias=True)\n)\n\n\n\n\n\nDistillation Loss: The loss function used combines:\n\nCross-Entropy Loss (Hard Labels): This is the standard supervised learning loss between the student model’s predictions and the true labels.\nKullback-Leibler (KL) Divergence Loss (Soft Labels): This measures how closely the student model’s softened output distribution aligns with the teacher’s softened output distribution.\n\n\n\nThe distillation loss function is expressed as:\n\\[\nL_{\\text{KD}} = \\alpha \\cdot T^2 \\cdot \\text{KL}(p_{\\text{teacher}}, p_{\\text{student}}) + (1 - \\alpha) \\cdot L_{\\text{CE}}(y_{\\text{true}}, p_{\\text{student}})\n\\]\nWhere:\n\n\\(p_{\\text{teacher}}\\) is the soft probability distribution from the teacher model.\n\\(p_{\\text{student}}\\) is the soft probability distribution from the student model.\n\\(L_{\\text{CE}}\\) is the cross-entropy loss.\n\\(\\alpha\\) is a hyperparameter to control the contribution of the soft targets.\n\\(T\\) is the temperature scaling factor applied to both teacher and student logits.\n\n\nclass DistillationLoss(nn.Module):\n    def __init__(self, temperature=3.0, alpha=0.5):\n        super(DistillationLoss, self).__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def forward(self, student_logits, teacher_logits, labels):\n        # Soft targets: apply temperature scaling to teacher outputs\n        teacher_soft = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_soft = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss (KL divergence between student and teacher's softened outputs)\n        distillation_loss = self.kl_div_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n\n        # Cross entropy loss (between student predictions and true labels)\n        student_loss = self.ce_loss(student_logits, labels)\n\n        # Combined loss\n        return self.alpha * distillation_loss + (1.0 - self.alpha) * student_loss\n\n\n\n\n\n\ndef train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = student_model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Training phase\n        student_model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass through both teacher and student\n            with torch.no_grad():\n                teacher_outputs = teacher_model(inputs)\n\n            student_outputs = student_model(inputs)\n\n            # Compute loss\n            loss = criterion(student_outputs, teacher_outputs, labels)\n\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Statistics\n            _, preds = torch.max(student_outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Copy the best model\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = student_model.state_dict()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:4f}')\n\n    # Load best model weights\n    student_model.load_state_dict(best_model_wts)\n    return student_model\n\n\n\n\n\nSGD is a popular optimizer for training neural networks and has been widely used in training large architectures like ResNet. It’s particularly good at providing a regularized, stable learning process when the learning rate is tuned properly. It avoids overfitting and promotes generalization when combined with momentum.\nMomentum (0.9) helps in accelerating gradients and converging faster by averaging out noise in gradient updates, preventing the optimizer from oscillating as much.\nLearning Rate 0.01 is a common learning rate for large-scale training tasks with SGD. It is not too large to cause unstable updates, nor too small to slow down learning excessively.\nTemperature scaling is used to control the softness of the teacher model’s output probability distribution. Higher temperatures produce softer probability distributions, revealing more about the relationships between different classes. A value of 3.0 is commonly used in knowledge distillation to smooth out the logits from the teacher model and make the student focus more on the relative probabilities assigned by the teacher model, rather than just the highest-probability class.\nAlpha controls the balance between the soft labels (from the teacher) and the hard labels (from the ground truth). A value of 0.5 equally weights the contribution from both the distillation loss (soft labels) and the cross-entropy loss (hard labels). This balance is crucial when training on datasets like Imagenette, where both the ground truth labels and the teacher’s soft predictions carry valuable information. You may adjust this value depending on how much you want the student model to prioritize learning from the teacher versus learning from the original ground truth. For example: If the teacher’s knowledge is highly reliable, you might increase alpha (&gt; 0.5) to emphasize learning from the teacher’s soft targets. If the ground truth labels are of high quality, you might decrease alpha (&lt; 0.5) to rely more on the hard labels.\n\n\n# Define optimizer for the student model\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define the distillation loss\ncriterion = DistillationLoss(temperature=3.0, alpha=0.5)\n\n# Train the student model\ntrained_student = train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25)\n\nEpoch 0/24\n----------\nLoss: 0.6731 Acc: 0.8762\nEpoch 1/24\n----------\nLoss: 0.6200 Acc: 0.9182\nEpoch 2/24\n----------\nLoss: 0.5981 Acc: 0.9310\nEpoch 3/24\n----------\nLoss: 0.5907 Acc: 0.9364\nEpoch 4/24\n----------\nLoss: 0.5797 Acc: 0.9427\nEpoch 5/24\n----------\nLoss: 0.5728 Acc: 0.9468\nEpoch 6/24\n----------\nLoss: 0.5677 Acc: 0.9465\nEpoch 7/24\n----------\nLoss: 0.5639 Acc: 0.9492\nEpoch 8/24\n----------\nLoss: 0.5581 Acc: 0.9494\nEpoch 9/24\n----------\nLoss: 0.5515 Acc: 0.9538\nEpoch 10/24\n----------\nLoss: 0.5536 Acc: 0.9537\nEpoch 11/24\n----------\nLoss: 0.5467 Acc: 0.9586\nEpoch 12/24\n----------\nLoss: 0.5463 Acc: 0.9554\nEpoch 13/24\n----------\nLoss: 0.5459 Acc: 0.9559\nEpoch 14/24\n----------\nLoss: 0.5440 Acc: 0.9566\nEpoch 15/24\n----------\nLoss: 0.5399 Acc: 0.9611\nEpoch 16/24\n----------\nLoss: 0.5358 Acc: 0.9637\nEpoch 17/24\n----------\nLoss: 0.5318 Acc: 0.9639\nEpoch 18/24\n----------\nLoss: 0.5336 Acc: 0.9612\nEpoch 19/24\n----------\nLoss: 0.5262 Acc: 0.9666\nEpoch 20/24\n----------\nLoss: 0.5318 Acc: 0.9631\nEpoch 21/24\n----------\nLoss: 0.5313 Acc: 0.9622\nEpoch 22/24\n----------\nLoss: 0.5245 Acc: 0.9678\nEpoch 23/24\n----------\nLoss: 0.5258 Acc: 0.9657\nEpoch 24/24\n----------\nLoss: 0.5243 Acc: 0.9640\nTraining complete in 517m 41s\nBest Acc: 0.967790\n\n\n\n\n\n\ndef evaluate_model(model, dataloaders):\n    model.eval()  # Set to evaluation mode\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad(): # No need to compute gradients during evaluation\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9809"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#knowledge-distillation-technique-using-soft-targets-also-known-as-soft-label-distillation-or-logits-distillation.",
    "href": "blogs/KD_Soft_Target_V1.html#knowledge-distillation-technique-using-soft-targets-also-known-as-soft-label-distillation-or-logits-distillation.",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "Soft Targets (Logits Distillation): The student model is trained not only on the hard labels (i.e., the ground truth labels), but also on the soft targets provided by the teacher model. The soft targets are the probabilities output by the teacher model after applying a temperature scaling to the logits (pre-softmax outputs).\nAn intuitive example of hard and soft targets for knowledge distillation:\nThe output of the network with no activation function is Logit.\n\n\n\n\n\n\n\n\nThink of the temperature as a dial that controls how “sharp” or “smooth” the teacher model’s predictions are. Normally, when a model makes predictions, it assigns high confidence to one class (the correct one) and very low confidence to others. This results in a “spiky” output, where one class has a high probability, and all others are close to zero.\n\n\n\n\n“Softening” refers to this process of making the output probabilities smoother (less sharp) when the temperature is increased. It’s called softening because the output becomes “softer” and less certain, giving some probability to even incorrect classes. This softened output is called soft targets.\n\n\n\n\nHinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531. Retrieved from https://arxiv.org/abs/1503.02531\nhttps://www.youtube.com/watch?v=zhrfBCNSO1Q\nhttps://www.youtube.com/watch?v=G0_lB1Ce65c&list=PLRhS6hiNUzERWXK6KE150aS8MJg-sVUCB&index=2\nhttps://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n\n\n# !pip install torch torchvision\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport time"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#load-the-dataset",
    "href": "blogs/KD_Soft_Target_V1.html#load-the-dataset",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "Preprocess the Imagenette2-320 dataset. Apply transforms to the images (resizing, normalization, etc.).\nThe input images are RGB, so they have 3 channels and are 32x32 pixels. Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging from 0 to 255. A common practice in neural networks is to normalize the input, which is done for multiple reasons, including avoiding saturation in commonly used activation functions and increasing numerical stability. Our normalization process consists of subtracting the mean and dividing by the standard deviation along each channel. The tensors “mean=[0.485, 0.456, 0.406]” and “std=[0.229, 0.224, 0.225]” were already computed, and they represent the mean and standard deviation of each channel in the predefined subset of dataset intended to be the training set. (https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)\n\n# Image Preprocessing and Augmentation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Load Dataset\ndata_dir =  './data/imagenette2-320/imagenette2-320'\nimage_datasets = {x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n               for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#define-the-teacher-and-student-models-teacher-resnet50-and-student-resnet18",
    "href": "blogs/KD_Soft_Target_V1.html#define-the-teacher-and-student-models-teacher-resnet50-and-student-resnet18",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "# Load Teacher (ResNet50) and Student (ResNet18)\nteacher_model = models.resnet50(pretrained=True)\nstudent_model = models.resnet18(pretrained=True)\n\n# Modify the last layer to match the number of classes (Imagenette has 10 classes)\nnum_ftrs_teacher = teacher_model.fc.in_features\nteacher_model.fc = nn.Linear(num_ftrs_teacher, 10)\n\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move models to the appropriate device (GPU if available)\nteacher_model = teacher_model.to(device)\nstudent_model = student_model.to(device)\n\n# Set teacher to evaluation mode, as its weights are frozen\nteacher_model.eval()\n\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\layal/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n\n\n\n\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\layal/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n\n\n\n\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#knowledge-distillation-loss",
    "href": "blogs/KD_Soft_Target_V1.html#knowledge-distillation-loss",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "Distillation Loss: The loss function used combines:\n\nCross-Entropy Loss (Hard Labels): This is the standard supervised learning loss between the student model’s predictions and the true labels.\nKullback-Leibler (KL) Divergence Loss (Soft Labels): This measures how closely the student model’s softened output distribution aligns with the teacher’s softened output distribution.\n\n\n\nThe distillation loss function is expressed as:\n\\[\nL_{\\text{KD}} = \\alpha \\cdot T^2 \\cdot \\text{KL}(p_{\\text{teacher}}, p_{\\text{student}}) + (1 - \\alpha) \\cdot L_{\\text{CE}}(y_{\\text{true}}, p_{\\text{student}})\n\\]\nWhere:\n\n\\(p_{\\text{teacher}}\\) is the soft probability distribution from the teacher model.\n\\(p_{\\text{student}}\\) is the soft probability distribution from the student model.\n\\(L_{\\text{CE}}\\) is the cross-entropy loss.\n\\(\\alpha\\) is a hyperparameter to control the contribution of the soft targets.\n\\(T\\) is the temperature scaling factor applied to both teacher and student logits.\n\n\nclass DistillationLoss(nn.Module):\n    def __init__(self, temperature=3.0, alpha=0.5):\n        super(DistillationLoss, self).__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def forward(self, student_logits, teacher_logits, labels):\n        # Soft targets: apply temperature scaling to teacher outputs\n        teacher_soft = torch.softmax(teacher_logits / self.temperature, dim=1)\n        student_soft = torch.log_softmax(student_logits / self.temperature, dim=1)\n\n        # Distillation loss (KL divergence between student and teacher's softened outputs)\n        distillation_loss = self.kl_div_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n\n        # Cross entropy loss (between student predictions and true labels)\n        student_loss = self.ce_loss(student_logits, labels)\n\n        # Combined loss\n        return self.alpha * distillation_loss + (1.0 - self.alpha) * student_loss"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#training-loop",
    "href": "blogs/KD_Soft_Target_V1.html#training-loop",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "def train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = student_model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Training phase\n        student_model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass through both teacher and student\n            with torch.no_grad():\n                teacher_outputs = teacher_model(inputs)\n\n            student_outputs = student_model(inputs)\n\n            # Compute loss\n            loss = criterion(student_outputs, teacher_outputs, labels)\n\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Statistics\n            _, preds = torch.max(student_outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Copy the best model\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = student_model.state_dict()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:4f}')\n\n    # Load best model weights\n    student_model.load_state_dict(best_model_wts)\n    return student_model"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#optimizer-and-hyperparameters",
    "href": "blogs/KD_Soft_Target_V1.html#optimizer-and-hyperparameters",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "SGD is a popular optimizer for training neural networks and has been widely used in training large architectures like ResNet. It’s particularly good at providing a regularized, stable learning process when the learning rate is tuned properly. It avoids overfitting and promotes generalization when combined with momentum.\nMomentum (0.9) helps in accelerating gradients and converging faster by averaging out noise in gradient updates, preventing the optimizer from oscillating as much.\nLearning Rate 0.01 is a common learning rate for large-scale training tasks with SGD. It is not too large to cause unstable updates, nor too small to slow down learning excessively.\nTemperature scaling is used to control the softness of the teacher model’s output probability distribution. Higher temperatures produce softer probability distributions, revealing more about the relationships between different classes. A value of 3.0 is commonly used in knowledge distillation to smooth out the logits from the teacher model and make the student focus more on the relative probabilities assigned by the teacher model, rather than just the highest-probability class.\nAlpha controls the balance between the soft labels (from the teacher) and the hard labels (from the ground truth). A value of 0.5 equally weights the contribution from both the distillation loss (soft labels) and the cross-entropy loss (hard labels). This balance is crucial when training on datasets like Imagenette, where both the ground truth labels and the teacher’s soft predictions carry valuable information. You may adjust this value depending on how much you want the student model to prioritize learning from the teacher versus learning from the original ground truth. For example: If the teacher’s knowledge is highly reliable, you might increase alpha (&gt; 0.5) to emphasize learning from the teacher’s soft targets. If the ground truth labels are of high quality, you might decrease alpha (&lt; 0.5) to rely more on the hard labels.\n\n\n# Define optimizer for the student model\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define the distillation loss\ncriterion = DistillationLoss(temperature=3.0, alpha=0.5)\n\n# Train the student model\ntrained_student = train_student(teacher_model, student_model, dataloaders, criterion, optimizer, num_epochs=25)\n\nEpoch 0/24\n----------\nLoss: 0.6731 Acc: 0.8762\nEpoch 1/24\n----------\nLoss: 0.6200 Acc: 0.9182\nEpoch 2/24\n----------\nLoss: 0.5981 Acc: 0.9310\nEpoch 3/24\n----------\nLoss: 0.5907 Acc: 0.9364\nEpoch 4/24\n----------\nLoss: 0.5797 Acc: 0.9427\nEpoch 5/24\n----------\nLoss: 0.5728 Acc: 0.9468\nEpoch 6/24\n----------\nLoss: 0.5677 Acc: 0.9465\nEpoch 7/24\n----------\nLoss: 0.5639 Acc: 0.9492\nEpoch 8/24\n----------\nLoss: 0.5581 Acc: 0.9494\nEpoch 9/24\n----------\nLoss: 0.5515 Acc: 0.9538\nEpoch 10/24\n----------\nLoss: 0.5536 Acc: 0.9537\nEpoch 11/24\n----------\nLoss: 0.5467 Acc: 0.9586\nEpoch 12/24\n----------\nLoss: 0.5463 Acc: 0.9554\nEpoch 13/24\n----------\nLoss: 0.5459 Acc: 0.9559\nEpoch 14/24\n----------\nLoss: 0.5440 Acc: 0.9566\nEpoch 15/24\n----------\nLoss: 0.5399 Acc: 0.9611\nEpoch 16/24\n----------\nLoss: 0.5358 Acc: 0.9637\nEpoch 17/24\n----------\nLoss: 0.5318 Acc: 0.9639\nEpoch 18/24\n----------\nLoss: 0.5336 Acc: 0.9612\nEpoch 19/24\n----------\nLoss: 0.5262 Acc: 0.9666\nEpoch 20/24\n----------\nLoss: 0.5318 Acc: 0.9631\nEpoch 21/24\n----------\nLoss: 0.5313 Acc: 0.9622\nEpoch 22/24\n----------\nLoss: 0.5245 Acc: 0.9678\nEpoch 23/24\n----------\nLoss: 0.5258 Acc: 0.9657\nEpoch 24/24\n----------\nLoss: 0.5243 Acc: 0.9640\nTraining complete in 517m 41s\nBest Acc: 0.967790"
  },
  {
    "objectID": "blogs/KD_Soft_Target_V1.html#evaluation",
    "href": "blogs/KD_Soft_Target_V1.html#evaluation",
    "title": "Knowledge Distillation Implementation 1/3",
    "section": "",
    "text": "def evaluate_model(model, dataloaders):\n    model.eval()  # Set to evaluation mode\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad(): # No need to compute gradients during evaluation\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9809"
  },
  {
    "objectID": "blogs/edgeimplusestudio.html",
    "href": "blogs/edgeimplusestudio.html",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse has rapidly become a prominent player in the field of machine learning for edge devices. Tailored to cater to a wide range of users, from hobbyists to professional embedded engineers, Edge Impulse Studio offers a comprehensive platform that supports the development, training, and deployment of machine learning models directly onto edge devices.\n\n\nEdge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools\n\n\n\n\n\n\n\nEdge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments.\n\n\n\n\n\nEdge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#platform-overview",
    "href": "blogs/edgeimplusestudio.html#platform-overview",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools"
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#key-features",
    "href": "blogs/edgeimplusestudio.html#key-features",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#conclusion",
    "href": "blogs/edgeimplusestudio.html#conclusion",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/programmable_gpt.html",
    "href": "blogs/programmable_gpt.html",
    "title": "Programmable Platform with ChatGPT API",
    "section": "",
    "text": "The goal is to develop a platform that enables users to dynamically create and manage services through natural language commands. This platform will leverage ChatGPT alongside OpenAI’s function calling capabilities to interact programmatically with various REST APIs. The system will automatically parse API definitions provided via OpenAPI specifications, enabling real-time monitoring, automation, and notifications based on user-specified conditions. For example, users could issue a command like, “Notify me when traffic is clear for 10 minutes,” and ChatGPT would generate the necessary logic to monitor and respond accordingly."
  },
  {
    "objectID": "blogs/programmable_gpt.html#translating-natural-language-into-programmatic-execution",
    "href": "blogs/programmable_gpt.html#translating-natural-language-into-programmatic-execution",
    "title": "Programmable Platform with ChatGPT API",
    "section": "1. Translating Natural Language into Programmatic Execution",
    "text": "1. Translating Natural Language into Programmatic Execution\n\nUsers input high-level commands such as: “Notify me when there is no traffic for 10 minutes.”\nChatGPT processes the input, parses the intent, and generates the necessary API call sequences to continuously monitor traffic conditions.\nThe system sets up polling or event-driven API calls and implements notification mechanisms based on defined thresholds or conditions (e.g., 10-minute traffic window).\n\nThe API interaction layer is handled through OpenAI’s function calling, allowing for structured input and output between ChatGPT and external APIs. The platform supports asynchronous monitoring through recurring API calls or webhooks for real-time notifications."
  },
  {
    "objectID": "blogs/programmable_gpt.html#parsing-openapi-specifications-for-api-discovery",
    "href": "blogs/programmable_gpt.html#parsing-openapi-specifications-for-api-discovery",
    "title": "Programmable Platform with ChatGPT API",
    "section": "2. Parsing OpenAPI Specifications for API Discovery",
    "text": "2. Parsing OpenAPI Specifications for API Discovery\n\nThe platform ingests OpenAPI specifications (YAML or JSON), which describe available endpoints, methods (GET, POST, etc.), parameters, authentication mechanisms, and expected responses.\nChatGPT uses these specs to generate dynamic prompts and actions, enabling users to explore API capabilities by issuing natural language queries such as, “What does this API offer?”\n\nThe OpenAI API extracts metadata from the spec and converts it into a structured format, making the platform capable of offering service discovery or suggesting actions based on user intent."
  },
  {
    "objectID": "blogs/programmable_gpt.html#api-integration-and-mashups",
    "href": "blogs/programmable_gpt.html#api-integration-and-mashups",
    "title": "Programmable Platform with ChatGPT API",
    "section": "3. API Integration and Mashups",
    "text": "3. API Integration and Mashups\n\nChatGPT can combine multiple APIs into a cohesive workflow based on user input. For instance, a user might request, “Show me both the weather and parking availability in Tokyo.”\nChatGPT identifies relevant API endpoints, orchestrates concurrent API calls (using asyncio or other parallel processing techniques), and aggregates the data into a unified response.\n\nBy leveraging OpenAI’s function calling, the platform dynamically maps user commands to corresponding API actions and handles the logic required for multi-API integration. This includes performing API authentication, pagination, and error handling in the background."
  },
  {
    "objectID": "blogs/programmable_gpt.html#auto-generating-fastapi-backends",
    "href": "blogs/programmable_gpt.html#auto-generating-fastapi-backends",
    "title": "Programmable Platform with ChatGPT API",
    "section": "4. Auto-Generating FastAPI Backends",
    "text": "4. Auto-Generating FastAPI Backends\n\nThe platform can automatically generate a FastAPI backend for user-defined services. For example, if a user requests a service to monitor parking spots and traffic, ChatGPT can:\n\nDefine RESTful endpoints in FastAPI.\nImplement periodic polling of external APIs or event-based triggers (using WebSockets or other event-driven mechanisms).\nAuto-generate input validation schemas using Pydantic, ensuring robustness against malformed inputs.\n\n\nThe backend logic is dynamically generated based on the user’s intent and can be containerized for deployment."
  },
  {
    "objectID": "blogs/programmable_gpt.html#continuous-monitoring-and-condition-based-automation",
    "href": "blogs/programmable_gpt.html#continuous-monitoring-and-condition-based-automation",
    "title": "Programmable Platform with ChatGPT API",
    "section": "5. Continuous Monitoring and Condition-Based Automation",
    "text": "5. Continuous Monitoring and Condition-Based Automation\n\nUsers can define conditions for continuous monitoring through ChatGPT. For example, “Alert me when parking availability in Tokyo exceeds 10 spots for more than 10 minutes.”\nChatGPT sets up a scheduled task (using APScheduler or similar libraries) to periodically call the API and evaluate the conditions.\nWhen conditions are met, the system triggers actions such as sending a notification through external services (email, SMS, Slack, etc.).\n\nThis logic is executed as part of the backend infrastructure, allowing for real-time, event-driven automation. The scheduling and monitoring infrastructure can scale horizontally using task queues like Celery or Redis."
  },
  {
    "objectID": "blogs/programmable_gpt.html#dynamic-api-invocation-via-function-calling",
    "href": "blogs/programmable_gpt.html#dynamic-api-invocation-via-function-calling",
    "title": "Programmable Platform with ChatGPT API",
    "section": "6. Dynamic API Invocation via Function Calling",
    "text": "6. Dynamic API Invocation via Function Calling\n\nOpenAI’s function calling feature allows the platform to dynamically invoke APIs by extracting required parameters from user input and mapping them to structured requests.\nFor example, a user request like, “Get available parking spots in Tokyo,” will:\n\nTrigger ChatGPT to parse the query, map it to an endpoint described in the OpenAPI spec, and dynamically generate the API call with the appropriate parameters.\nHandle API authentication tokens, error handling, retries, and rate limits automatically, ensuring robust API interactions.\n\n\nThis mechanism ensures that user commands can be mapped to API invocations in a highly automated and scalable way."
  },
  {
    "objectID": "blogs/programmable_gpt.html#natural-language-api-mapping",
    "href": "blogs/programmable_gpt.html#natural-language-api-mapping",
    "title": "Programmable Platform with ChatGPT API",
    "section": "7. Natural Language API Mapping",
    "text": "7. Natural Language API Mapping\n\nChatGPT automatically translates natural language queries into appropriate API calls by extracting intent, identifying available endpoints, and populating required parameters.\nExample flow:\n\nUser: “Monitor parking availability in Tokyo every 30 minutes and alert me when more than 10 spots are available.”\nChatGPT processes the request, sets up periodic API polling using a scheduler, and defines the logic for alerting based on the response.\n\n\nThrough natural language processing, ChatGPT simplifies the complexity of working with raw API specifications, enabling seamless API integration for users with no coding experience."
  },
  {
    "objectID": "blogs/programmable_gpt.html#auto-generating-docker-containers",
    "href": "blogs/programmable_gpt.html#auto-generating-docker-containers",
    "title": "Programmable Platform with ChatGPT API",
    "section": "8. Auto-Generating Docker Containers",
    "text": "8. Auto-Generating Docker Containers\n\nOnce a FastAPI service or custom workflow is defined, ChatGPT can auto-generate a Dockerfile to containerize the service. This ensures that all dependencies, environment variables, and configurations are encapsulated, making the service portable across different deployment environments.\nExample Dockerfile generated by ChatGPT:\nFROM python:3.9-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nThe system can generate fully containerized services, making deployment in cloud environments like AWS, Azure, or GCP straightforward via CI/CD pipelines."
  },
  {
    "objectID": "blogs/programmable_gpt.html#auto-generating-openapi-specifications-for-new-services",
    "href": "blogs/programmable_gpt.html#auto-generating-openapi-specifications-for-new-services",
    "title": "Programmable Platform with ChatGPT API",
    "section": "9. Auto-Generating OpenAPI Specifications for New Services",
    "text": "9. Auto-Generating OpenAPI Specifications for New Services\n\nFor every new service or API integration created by ChatGPT, the system can also generate a corresponding OpenAPI specification. This allows third-party developers to consume the newly generated APIs and integrate them into their own applications.\nFor example:\n\nA user requests an API that combines both weather and parking data. ChatGPT generates a FastAPI service for this, and then creates the relevant OpenAPI spec for the combined API, making it accessible for external integration."
  },
  {
    "objectID": "blogs/programmable_gpt.html#natural-language-understanding",
    "href": "blogs/programmable_gpt.html#natural-language-understanding",
    "title": "Programmable Platform with ChatGPT API",
    "section": "1. Natural Language Understanding",
    "text": "1. Natural Language Understanding\nChatGPT has advanced language understanding capabilities, allowing it to interpret user input in natural language and map it to programmatic actions. It can dynamically generate requests to external APIs, define workflows, and automate services based on real-time user queries and commands."
  },
  {
    "objectID": "blogs/programmable_gpt.html#function-calling",
    "href": "blogs/programmable_gpt.html#function-calling",
    "title": "Programmable Platform with ChatGPT API",
    "section": "2. Function Calling",
    "text": "2. Function Calling\nOpenAI’s function calling allows ChatGPT to execute structured operations, such as invoking REST APIs or performing specific tasks that require formatted input and output. This capability enables ChatGPT to handle complex workflows, parameter validation, and API invocation dynamically without hardcoding API logic into the application."
  },
  {
    "objectID": "blogs/programmable_gpt.html#code-generation",
    "href": "blogs/programmable_gpt.html#code-generation",
    "title": "Programmable Platform with ChatGPT API",
    "section": "3. Code Generation",
    "text": "3. Code Generation\nChatGPT can generate executable Python code, FastAPI applications, Dockerfiles, and OpenAPI specs dynamically. This enables users to set up new services, containerize them, and deploy them with minimal manual intervention."
  },
  {
    "objectID": "blogs/programmable_gpt.html#openapi-spec-parsing",
    "href": "blogs/programmable_gpt.html#openapi-spec-parsing",
    "title": "Programmable Platform with ChatGPT API",
    "section": "4. OpenAPI Spec Parsing",
    "text": "4. OpenAPI Spec Parsing\nChatGPT can parse and understand OpenAPI specifications, allowing it to discover API capabilities and automatically generate function call schemas. This means the system can identify available endpoints, handle API authentication, and dynamically execute API calls based on the user’s instructions."
  },
  {
    "objectID": "blogs/programmable_gpt.html#real-time-monitoring-and-automation",
    "href": "blogs/programmable_gpt.html#real-time-monitoring-and-automation",
    "title": "Programmable Platform with ChatGPT API",
    "section": "5. Real-Time Monitoring and Automation",
    "text": "5. Real-Time Monitoring and Automation\nBy using OpenAI’s API, ChatGPT can continuously monitor APIs, execute scheduled tasks, and trigger alerts based on user-defined conditions, enabling dynamic, real-time automation of services like traffic monitoring or parking availability."
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V1.html",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V1.html",
    "title": "Low Rank Approximation Implementation 1/4",
    "section": "",
    "text": "Singular Value Decomposition (SVD) is a mathematical technique used to decompose a matrix into three other matrices. In other words, SVD is a classic low-rank approximation technique, where the weight matrices of CNN layers (particularly fully connected or convolutional layers) are factorized into three matrices: \\(W = U \\Sigma V^T\\). Here, \\(U\\) and \\(V\\) are orthogonal matrices, and \\(\\Sigma\\) is a diagonal matrix with singular values. By truncating the smaller singular values, a low-rank approximation of the weight matrix can be obtained, reducing the number of parameters.\nSVD , which makes it extremely useful for low-rank approximation. Specifically, given an \\(m \\times n\\) matrix \\(A\\), SVD decomposes it as: \\[A = U \\Sigma V^T\\]\n\n\nWhere:\n\n\\(U\\): An \\(m \\times m\\) orthogonal matrix, whose columns are called left singular vectors. The matrix is considered an orthogonal matrix if the product of a matrix and its transpose gives an identity value.\n\\(\\Sigma\\): An \\(m \\times n\\) diagonal matrix with non-negative real numbers on the diagonal (these are called singular values). A matrix is diagonal if it has non-zero elements only in the diagonal, running from the upper left to the lower right corner of the matrix.\n\\(V^T\\): The transpose of an \\(n \\times n\\) orthogonal matrix, whose columns are called right singular vectors.\n\nRank :The rank of a matrix is a fundamental concept in linear algebra that indicates the number of linearly independent rows or columns in a matrix.\nThe goal of Low Rank Factorization is to approximate \\(A\\) with a matrix of lower rank \\(k\\), where \\(k &lt; min(m, n)\\), to reduce the amount of data and computation while retaining the most important information.\nTransformation express by one rotation, horizontal or vertical stretching, and another rotation. \nReferences * https://www.youtube.com/watch?v=DG7YTlGnCEo\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import models\n\n\n# Step 1: Load Imagenette Dataset\ntransform = transforms.Compose([\n    transforms.Resize((320, 320)),  # Resize all images to the same size\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = torchvision.datasets.ImageFolder(root='./imagenette/imagenette2-320/train', transform=transform)\ntrainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n\ntestset = torchvision.datasets.ImageFolder(root='./imagenette/imagenette2-320/val', transform=transform)\ntestloader = DataLoader(testset, batch_size=32, shuffle=False)\n\n\n# Step 2: Define ResNet-18 Model\nmodel = models.resnet18(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 10)  # Adjust the final layer for 10 classes\n\n\n# Step 3: Apply Low Rank Factorization on Convolutional Layers\ndef low_rank_approximation(conv_layer, rank):\n    with torch.no_grad():\n        # Reshape the weight of convolutional layer to 2D tensor for SVD\n        W = conv_layer.weight.data\n        out_channels, in_channels, kh, kw = W.size()\n        W_2d = W.view(out_channels, -1)\n\n        # Apply SVD\n        U, S, V = torch.svd(W_2d)\n        U_r, S_r, V_r = U[:, :rank], S[:rank], V[:, :rank]\n        \n        # Create low-rank approximation\n        W_low_rank = torch.mm(U_r, torch.diag(S_r)).mm(V_r.t())\n        W_low_rank = W_low_rank.view(out_channels, in_channels, kh, kw)\n        \n        # Update the conv layer weight\n        conv_layer.weight.data = W_low_rank\n\n# Apply Low Rank Approximation to Conv1 and Conv2 layers of ResNet-18\nrank = 8  # Set rank to be half of the original rank to reduce parameters\nlow_rank_approximation(model.layer1[0].conv1, rank)\nlow_rank_approximation(model.layer1[0].conv2, rank)\n\n\n# Step 4: Retrain the Model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\ndef train(model, trainloader, criterion, optimizer, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(trainloader, 0):\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if i % 100 == 99:    # Print every 100 mini-batches\n                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n                running_loss = 0.0\n\ntrain(model, trainloader, criterion, optimizer, epochs=5)\n\nEpoch 1, Batch 100, Loss: 2.092\nEpoch 1, Batch 200, Loss: 1.813\nEpoch 2, Batch 100, Loss: 1.549\nEpoch 2, Batch 200, Loss: 1.418\nEpoch 3, Batch 100, Loss: 1.292\nEpoch 3, Batch 200, Loss: 1.268\nEpoch 4, Batch 100, Loss: 1.103\nEpoch 4, Batch 200, Loss: 1.096\nEpoch 5, Batch 100, Loss: 0.997\nEpoch 5, Batch 200, Loss: 0.988\n\n\n\n# Step 5: Evaluate the Final Model\ndef evaluate(model, testloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Accuracy: {100 * correct / total:.2f}%')\n\nevaluate(model, testloader)\n\nAccuracy: 64.79%\n\n\n\nReferences:\nhttps://pytorch.org/docs/stable/generated/torch.svd.html\nSainath, T. N., Kingsbury, B., Sindhwani, V., Arisoy, E., & Ramabhadran, B. (2013). Low-rank matrix factorization for Deep Neural Network training. In ICASSP."
  },
  {
    "objectID": "blogs/LRA_SVD_Compress_Convolutional_Layers_V1.html#singular-value-decomposition-svd---resnet18",
    "href": "blogs/LRA_SVD_Compress_Convolutional_Layers_V1.html#singular-value-decomposition-svd---resnet18",
    "title": "Low Rank Approximation Implementation 1/4",
    "section": "",
    "text": "Singular Value Decomposition (SVD) is a mathematical technique used to decompose a matrix into three other matrices. In other words, SVD is a classic low-rank approximation technique, where the weight matrices of CNN layers (particularly fully connected or convolutional layers) are factorized into three matrices: \\(W = U \\Sigma V^T\\). Here, \\(U\\) and \\(V\\) are orthogonal matrices, and \\(\\Sigma\\) is a diagonal matrix with singular values. By truncating the smaller singular values, a low-rank approximation of the weight matrix can be obtained, reducing the number of parameters.\nSVD , which makes it extremely useful for low-rank approximation. Specifically, given an \\(m \\times n\\) matrix \\(A\\), SVD decomposes it as: \\[A = U \\Sigma V^T\\]\n\n\nWhere:\n\n\\(U\\): An \\(m \\times m\\) orthogonal matrix, whose columns are called left singular vectors. The matrix is considered an orthogonal matrix if the product of a matrix and its transpose gives an identity value.\n\\(\\Sigma\\): An \\(m \\times n\\) diagonal matrix with non-negative real numbers on the diagonal (these are called singular values). A matrix is diagonal if it has non-zero elements only in the diagonal, running from the upper left to the lower right corner of the matrix.\n\\(V^T\\): The transpose of an \\(n \\times n\\) orthogonal matrix, whose columns are called right singular vectors.\n\nRank :The rank of a matrix is a fundamental concept in linear algebra that indicates the number of linearly independent rows or columns in a matrix.\nThe goal of Low Rank Factorization is to approximate \\(A\\) with a matrix of lower rank \\(k\\), where \\(k &lt; min(m, n)\\), to reduce the amount of data and computation while retaining the most important information.\nTransformation express by one rotation, horizontal or vertical stretching, and another rotation. \nReferences * https://www.youtube.com/watch?v=DG7YTlGnCEo\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import models\n\n\n# Step 1: Load Imagenette Dataset\ntransform = transforms.Compose([\n    transforms.Resize((320, 320)),  # Resize all images to the same size\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = torchvision.datasets.ImageFolder(root='./imagenette/imagenette2-320/train', transform=transform)\ntrainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n\ntestset = torchvision.datasets.ImageFolder(root='./imagenette/imagenette2-320/val', transform=transform)\ntestloader = DataLoader(testset, batch_size=32, shuffle=False)\n\n\n# Step 2: Define ResNet-18 Model\nmodel = models.resnet18(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 10)  # Adjust the final layer for 10 classes\n\n\n# Step 3: Apply Low Rank Factorization on Convolutional Layers\ndef low_rank_approximation(conv_layer, rank):\n    with torch.no_grad():\n        # Reshape the weight of convolutional layer to 2D tensor for SVD\n        W = conv_layer.weight.data\n        out_channels, in_channels, kh, kw = W.size()\n        W_2d = W.view(out_channels, -1)\n\n        # Apply SVD\n        U, S, V = torch.svd(W_2d)\n        U_r, S_r, V_r = U[:, :rank], S[:rank], V[:, :rank]\n        \n        # Create low-rank approximation\n        W_low_rank = torch.mm(U_r, torch.diag(S_r)).mm(V_r.t())\n        W_low_rank = W_low_rank.view(out_channels, in_channels, kh, kw)\n        \n        # Update the conv layer weight\n        conv_layer.weight.data = W_low_rank\n\n# Apply Low Rank Approximation to Conv1 and Conv2 layers of ResNet-18\nrank = 8  # Set rank to be half of the original rank to reduce parameters\nlow_rank_approximation(model.layer1[0].conv1, rank)\nlow_rank_approximation(model.layer1[0].conv2, rank)\n\n\n# Step 4: Retrain the Model\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\ndef train(model, trainloader, criterion, optimizer, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(trainloader, 0):\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if i % 100 == 99:    # Print every 100 mini-batches\n                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n                running_loss = 0.0\n\ntrain(model, trainloader, criterion, optimizer, epochs=5)\n\nEpoch 1, Batch 100, Loss: 2.092\nEpoch 1, Batch 200, Loss: 1.813\nEpoch 2, Batch 100, Loss: 1.549\nEpoch 2, Batch 200, Loss: 1.418\nEpoch 3, Batch 100, Loss: 1.292\nEpoch 3, Batch 200, Loss: 1.268\nEpoch 4, Batch 100, Loss: 1.103\nEpoch 4, Batch 200, Loss: 1.096\nEpoch 5, Batch 100, Loss: 0.997\nEpoch 5, Batch 200, Loss: 0.988\n\n\n\n# Step 5: Evaluate the Final Model\ndef evaluate(model, testloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Accuracy: {100 * correct / total:.2f}%')\n\nevaluate(model, testloader)\n\nAccuracy: 64.79%\n\n\n\nReferences:\nhttps://pytorch.org/docs/stable/generated/torch.svd.html\nSainath, T. N., Kingsbury, B., Sindhwani, V., Arisoy, E., & Ramabhadran, B. (2013). Low-rank matrix factorization for Deep Neural Network training. In ICASSP."
  },
  {
    "objectID": "blogs/about.html",
    "href": "blogs/about.html",
    "title": "About",
    "section": "",
    "text": "What’s NinjaLABO is and how it was started!\n\n\n\nNinjaLABO’s origin",
    "crumbs": [
      "Get Started",
      "About"
    ]
  },
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "Compiling and optimizing ML models for multiple hardware platforms can be complex and time-consuming. One promising solution on our radar is the Intermediate Representation Execution Environment (IREE), which can significantly simplify this process. Here’s an in-depth look at what I’ve learned about IREE.\n\n\nIREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms.\n\n\n\n\nImport Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware.\n\n\n\n\n\nIntermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments.\n\n\n\n\nHere was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/iree.html#what-is-iree",
    "href": "blogs/iree.html#what-is-iree",
    "title": "IREE review",
    "section": "",
    "text": "IREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms."
  },
  {
    "objectID": "blogs/iree.html#workflow-with-iree",
    "href": "blogs/iree.html#workflow-with-iree",
    "title": "IREE review",
    "section": "",
    "text": "Import Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware."
  },
  {
    "objectID": "blogs/iree.html#key-features-of-iree",
    "href": "blogs/iree.html#key-features-of-iree",
    "title": "IREE review",
    "section": "",
    "text": "Intermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments."
  },
  {
    "objectID": "blogs/iree.html#conclusion",
    "href": "blogs/iree.html#conclusion",
    "title": "IREE review",
    "section": "",
    "text": "Here was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html",
    "href": "blogs/todo_2024_autumn.html",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "This document provides an overview of NinjaLABO’s tasks and objectives for the autumn of 2024. The following sections outline key areas of focus, ranging from way of working (WoW) to model development and hardware deployment strategies.\n\n\n\nToDo 2024 Autumn Overview\n\n\n\n\nOur Way of Working (WoW) aims to ensure efficient and collaborative work among team members. This includes SCRUM practices, GitHub workflows, and workshops for knowledge sharing.\n\n\nSCRUM is our core agile framework, promoting daily check-ins, iterative sprint cycles, and continuous improvement.\n\n\n\n\n\nHeld on Discord in the Voice Channel.\nA strict 15-minute standup where everyone declares the following:\n\nWhat they completed (Done).\nWhat they will work on next (ToDo).\nAny blockers or issues (Issue).\n\n\n\n\n\nIf a topic takes longer than 5 minutes, continue the discussion in backlog item comments or schedule another meeting.\n\n\n\n\n\n\nEach sprint follows a clear structure, from planning to review, ensuring that we meet our project goals while continuously improving.\n\n\n\nBacklog grooming and sprint planning usually take place on Monday morning at the beginning of the sprint.\nThe team selects high-priority tasks from the backlog.\n\n\n\n\n\nThe review occurs on Friday afternoon at the end of the sprint.\nWe assess completed backlog items and discuss progress with stakeholders.\n\n\n\n\n\nAfter the review, we hold a retrospective to discuss what worked well and what could be improved in our WoW.\nThe retro typically happens right after the review on Friday afternoon.\n\n\n\n\n\n\n\nGitHub is our primary tool for project management, code reviews, and CI/CD.\n\n\n\n\n\nWe use the KANBAN board to visualize and track our backlog items, ensuring we stay on top of tasks and priorities.\n\n\n\n\n\n\n\n\nWe follow the Acceptance Test-Driven Development (ATDD) approach, which ensures that tests are written based on the system’s behavior before the implementation. \n\n\n\n\n\n\nUsing Nbdev, we streamline writing, testing, documenting, and distributing software packages directly from Jupyter Notebooks.\n\n\n\n\n\nTo facilitate knowledge sharing, we conduct workshops, encouraging developers to present and follow along using Jupyter notebooks.\n\n\n\n\nThe following techniques will be our focus for optimizing model performance:\n\n\nIn-depth profiling is necessary to guide our compression strategies.\n\n\n\nReducing the number of bits needed to represent weights and activations with Post-training Quantization.\n\n\n\n\n\nPruning involves removing less critical parameters from the model.\n\n\n\n\n\nTraining a smaller model (student) to mimic a larger model (teacher) through Knowledge Distillation.\n\n\n\n\n\nReducing model complexity by approximating weight matrices with Low-Rank Factorization.\n\n\n\n\n\n\nExploring hardware platforms and their capabilities.\n\n\n\nInvestigate the potential of using IREE for machine learning workloads.\n\n\n\n\n\nContinue optimizing and leveraging CUDA for GPU-accelerated tasks.\n\n\n\n\n\n\n\nBenchmark and compare model performance with NVIDIA’s TensorRT on Jetson devices.\n\n\n\n\n\n\n\n\nRunning Large Language Models (LLM) on the Orange Pi 5 Plus, evaluating its performance and scalability.\n\n\n\n\n\n\nOur tinyMLaaS focuses on providing scalable machine learning services, particularly for resource-constrained devices.\n\n\n\nA pipeline for flexible model transformations, such as compression, dockerization, and packaging models for various environments.\n\n\n\n\nLeveraging FastHTML for quicker model transformations and deployment.\n\n\n\n\n\n\nDistributed model execution across nodes, utilizing technologies like FaaS (Function as a Service) and Docker Swarm.\n\n\n\n\n\nOur custom runtime for executing deep learning models on various hardware platforms.\n\n\n\nFocus on improving GPU acceleration within our runtime.\n\n\n\n\n\nCPU-based inference and training optimizations.\n\n\n\n\n\n\nEvaluating inference performance with the ResNet50 model.\n\n\n\n\nUsing Convolutional Neural Networks (CNN) to process image data.\n\n\n\n\n\n\nBenchmarking with the Imagenette dataset.\n\n\n\n\n\nSupporting inference of Transformers, including Large Language Models (LLMs) and Vision Transformers.\n\n\n\n\n\n\nOptimizing training pipelines on both CPU and CUDA-based platforms.\n\n\n\n\n\n\nModel development and collaboration projects for 2024.\n\n\n\n\n\nDeveloping models for hyperspectral imaging using 3D Convolutional Neural Networks, particularly for CubeSat applications.\n\n\n\n\n\n\n\n\nFocusing on compression techniques for LLMs, making them more efficient for deployment.\n\n\n\n\n\nExploring physics simulation models as part of the ESA tender."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#wow",
    "href": "blogs/todo_2024_autumn.html#wow",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our Way of Working (WoW) aims to ensure efficient and collaborative work among team members. This includes SCRUM practices, GitHub workflows, and workshops for knowledge sharing.\n\n\nSCRUM is our core agile framework, promoting daily check-ins, iterative sprint cycles, and continuous improvement.\n\n\n\n\n\nHeld on Discord in the Voice Channel.\nA strict 15-minute standup where everyone declares the following:\n\nWhat they completed (Done).\nWhat they will work on next (ToDo).\nAny blockers or issues (Issue).\n\n\n\n\n\nIf a topic takes longer than 5 minutes, continue the discussion in backlog item comments or schedule another meeting.\n\n\n\n\n\n\nEach sprint follows a clear structure, from planning to review, ensuring that we meet our project goals while continuously improving.\n\n\n\nBacklog grooming and sprint planning usually take place on Monday morning at the beginning of the sprint.\nThe team selects high-priority tasks from the backlog.\n\n\n\n\n\nThe review occurs on Friday afternoon at the end of the sprint.\nWe assess completed backlog items and discuss progress with stakeholders.\n\n\n\n\n\nAfter the review, we hold a retrospective to discuss what worked well and what could be improved in our WoW.\nThe retro typically happens right after the review on Friday afternoon.\n\n\n\n\n\n\n\nGitHub is our primary tool for project management, code reviews, and CI/CD.\n\n\n\n\n\nWe use the KANBAN board to visualize and track our backlog items, ensuring we stay on top of tasks and priorities.\n\n\n\n\n\n\n\n\nWe follow the Acceptance Test-Driven Development (ATDD) approach, which ensures that tests are written based on the system’s behavior before the implementation. \n\n\n\n\n\n\nUsing Nbdev, we streamline writing, testing, documenting, and distributing software packages directly from Jupyter Notebooks.\n\n\n\n\n\nTo facilitate knowledge sharing, we conduct workshops, encouraging developers to present and follow along using Jupyter notebooks."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#compression",
    "href": "blogs/todo_2024_autumn.html#compression",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "The following techniques will be our focus for optimizing model performance:\n\n\nIn-depth profiling is necessary to guide our compression strategies.\n\n\n\nReducing the number of bits needed to represent weights and activations with Post-training Quantization.\n\n\n\n\n\nPruning involves removing less critical parameters from the model.\n\n\n\n\n\nTraining a smaller model (student) to mimic a larger model (teacher) through Knowledge Distillation.\n\n\n\n\n\nReducing model complexity by approximating weight matrices with Low-Rank Factorization."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#hw",
    "href": "blogs/todo_2024_autumn.html#hw",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Exploring hardware platforms and their capabilities.\n\n\n\nInvestigate the potential of using IREE for machine learning workloads.\n\n\n\n\n\nContinue optimizing and leveraging CUDA for GPU-accelerated tasks.\n\n\n\n\n\n\n\nBenchmark and compare model performance with NVIDIA’s TensorRT on Jetson devices.\n\n\n\n\n\n\n\n\nRunning Large Language Models (LLM) on the Orange Pi 5 Plus, evaluating its performance and scalability."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#tinymlaas",
    "href": "blogs/todo_2024_autumn.html#tinymlaas",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our tinyMLaaS focuses on providing scalable machine learning services, particularly for resource-constrained devices.\n\n\n\nA pipeline for flexible model transformations, such as compression, dockerization, and packaging models for various environments.\n\n\n\n\nLeveraging FastHTML for quicker model transformations and deployment.\n\n\n\n\n\n\nDistributed model execution across nodes, utilizing technologies like FaaS (Function as a Service) and Docker Swarm."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#tinyruntime",
    "href": "blogs/todo_2024_autumn.html#tinyruntime",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our custom runtime for executing deep learning models on various hardware platforms.\n\n\n\nFocus on improving GPU acceleration within our runtime.\n\n\n\n\n\nCPU-based inference and training optimizations.\n\n\n\n\n\n\nEvaluating inference performance with the ResNet50 model.\n\n\n\n\nUsing Convolutional Neural Networks (CNN) to process image data.\n\n\n\n\n\n\nBenchmarking with the Imagenette dataset.\n\n\n\n\n\nSupporting inference of Transformers, including Large Language Models (LLMs) and Vision Transformers.\n\n\n\n\n\n\nOptimizing training pipelines on both CPU and CUDA-based platforms."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#model",
    "href": "blogs/todo_2024_autumn.html#model",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Model development and collaboration projects for 2024.\n\n\n\n\n\nDeveloping models for hyperspectral imaging using 3D Convolutional Neural Networks, particularly for CubeSat applications.\n\n\n\n\n\n\n\n\nFocusing on compression techniques for LLMs, making them more efficient for deployment.\n\n\n\n\n\nExploring physics simulation models as part of the ESA tender."
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "It has been nearly five years since I first wrote about TinyML and TinyML as-a-Service (TinyMLaaS) on the Ericsson blog.\n\nIn the rapidly evolving landscape of artificial intelligence (AI), efficiency and resource optimization are paramount. AI Compression as-a-Service (ACaaS) and TinyML as-a-Service (TinyMLaaS) emerge as transformative solutions that address the growing demand for deploying AI on resource-constrained devices. These services offer scalable, cost-effective, and high-performance AI capabilities, enabling a new wave of innovation in edge computing.\n\n\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance.\n\n\n\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security.\n\n\n\n\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications.\n\n\n\n\n\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring.\n\n\n\n\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Resource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "",
    "text": "TinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Smart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/ericsson_blog2.html",
    "href": "blogs/ericsson_blog2.html",
    "title": "How can we democratize machine learning on IoT devices?",
    "section": "",
    "text": "ghttps://www.ericsson.com/en/blog/2020/2/how-can-we-democratize-machine-learning-iot-devices\n\n\n\nHow can we democratize machine learning on IoT devices?"
  },
  {
    "objectID": "blogs/gpts_function_call_web.html",
    "href": "blogs/gpts_function_call_web.html",
    "title": "The Impact of GPTs + Function-Calling on Web Development",
    "section": "",
    "text": "It’s the weekend, and I spent some time researching how to develop our LLM-based services. What I discovered is how GPTs + function calling are transforming traditional web development. I realize now that I was a bit late to fully grasp this shift, but the impact is undeniable.\n\n\nHow GPT’s Function Calling is Revolutionizing Web Development\nIn recent years, web development has undergone rapid transformation, primarily driven by the rise of APIs, microservices, and cloud architectures. Traditionally, developers had to manually integrate services, convert data formats, and handle various API-specific nuances. With the advent of Large Language Models (LLMs) and, more specifically, the function calling capability in GPT-based models, this landscape is evolving rapidly.\nLet’s explore how these advancements are reshaping web development into a more intuitive and automated process, and why we should be eager to embrace this change at NinjaLABO.\n\n\n\nThe Old Paradigm: Web Development as API Mashups\nWeb development during the era of microservices could often be seen as a series of API mashups. Developers spent significant time stitching together APIs from various services to create cohesive systems. This required extensive effort to ensure data was correctly formatted, sent over the wire, and processed by another system.\nFor example, integrating a payment service with an e-commerce platform meant developers had to handle authentication, format requests in JSON or XML, manage error handling, and ensure that the responses aligned with the rest of the system. These repetitive tasks consumed a large portion of a developer’s time.\nThis approach introduced friction. Developers were often bogged down in ensuring that APIs “spoke the same language,” especially when translating between services with slightly different data formats (e.g. JSON).\n\n\n\nThe Game Changer: GPT’s Function Calling Capability\nWith the recent advancements in LLMs, particularly the introduction of GPT models’ function calling capability, this tedious workflow is becoming a thing of the past. These new-generation tools don’t just act as text-based conversational systems; they can now handle function calls, process structured data, and automate much of the repetitive work developers used to do manually.\nFor instance, instead of writing boilerplate code to connect services, you can now instruct GPT to perform a function call and execute API integrations seamlessly. The LLM can handle the format conversion behind the scenes—whether it’s JSON or XML—allowing you to focus on higher-level tasks. This ability to perform complex operations from natural language queries opens up new possibilities for how web services are built and integrated. And it’s not only for requests but also responces to be processed and given as a format of natural language, no visibility of intermediate data format at all in this route.\nFor developers, this means a shift from worrying about low-level implementation details to focusing on higher-level logic. You simply describe what you want the system to do in natural language, and the underlying LLM takes care of the rest. A task that once required understanding API documentation, writing glue code, and managing data flows can now be handled efficiently by AI.\n\n\n\nAPI + GPT: A New Development Paradigm\nIn the past, developers had to publish API specifications (such as OpenAPI) to describe how different services could interact. This required both developers and consumers to understand the specification, write code to communicate with it, and handle any necessary modifications.\nWith GPT and function-calling, this interaction is simplified into a natural language interface. You no longer need an OpenAPI spec or an in-depth understanding of each service’s API. Instead, you simply ask GPT to interact with a service, and it takes care of the rest.\nFor example, let’s say you want to add sophisticated features to our LLM camera project—where the camera can detect events, generate insights, and send notifications in real-time. Instead of manually integrating external APIs for each feature, you can prompt GPT to handle those interactions. The LLM will take care of the API calls, processing the data, and returning the necessary results—all without the need for deep API expertise.\nThis paradigm opens up a new world where developing complex systems doesn’t have to be constrained by detailed API knowledge. Instead, the focus shifts to how we articulate the behavior and logic of these systems in natural language, letting the model handle the underlying mechanics.\n\n\n\nWhat This Means for NinjaLABO Developers\nThe implications are clear: as web development becomes more about problem-solving and user experience design, we can focus less on the technical minutiae of how services communicate with each other. At NinjaLABO, this opens up incredible potential for our ongoing projects, especially the LLM camera project we’ve been developing for AI mobility applications.\nWith LLMs and function-calling, we can bridge the gap between various external data sources (e.g., parking availability, traffic patterns, or public safety events) and integrate them into the camera’s insights seamlessly. You could easily imagine scenarios where:\n\nThe camera detects a free parking spot and notifies nearby users in real-time.\nIt monitors traffic flow and sends alerts when congestion builds up.\nIt autonomously alerts authorities when an incident occurs, based on real-time data.\n\nEach of these actions would have required detailed integration work, error handling, and data processing in the past. Now, with GPT’s function-calling capabilities, these workflows can be simplified, enabling our team to focus on building better, faster services without the overhead of manual API management.\nThis shift means that our developers have a powerful new toolset at their disposal to streamline backend logic and empower the camera’s capabilities. We’re moving into a phase where we can achieve more by focusing on describing what we want the system to do, rather than how to do it.\n\n\n\nWhat’s Left? The Front-End, and a New Way of Thinking\nWith backend integration becoming increasingly automated through GPTs and function-calling, one might wonder—what about the front-end? Are we approaching a point where even front-end development could be handled just as easily?\nTools like Anthropic Artifacts suggest that even UI generation may eventually be abstracted, allowing developers to describe front-end behavior and design in natural language, with the system generating code and interfaces automatically.\nWhile this may seem overly optimistic, the rapid advancements in AI make it worth considering. The gap between high-level design thinking and low-level implementation is closing. As LLMs evolve, the line between coding and simply “describing” what you want is becoming increasingly blurred. This new way of thinking can empower our team to explore even more innovative solutions in the near future.\n\n\n\n\nLooking Ahead: Embracing the Future of Web Development\nWeb development is on the brink of a major transformation. With the function-calling feature and LLM-powered automation, the focus will shift toward designing user stories and experiences, rather than connecting disparate services manually. This shift will lead to faster development cycles, smoother integrations, and make technology more accessible to a broader range of users.\nAt NinjaLABO, we’re already exploring how to leverage these capabilities to drive the LLM camera project forward. This isn’t just a front-end or backend improvement—it’s an entirely new approach to how we build, iterate, and innovate.\nBy embracing this shift, our developers have an opportunity to experiment with these cutting-edge tools and push the boundaries of what’s possible, creating smarter, faster, and more intuitive services.\nThe web development world is evolving—are you ready to explore what’s next?"
  },
  {
    "objectID": "blogs/summer_insights_2024.html",
    "href": "blogs/summer_insights_2024.html",
    "title": "Summer Insights: Evolving tinyMLaaS with Flexible Pipelines and Distributed Execution",
    "section": "",
    "text": "This summer, our team embarked on an ambitious journey, iterating through eight MVP releases of our tinyMLaaS platform, which focuses on deep neural network (DNN) model compression within a SaaS environment. The feedback we received from a diverse group of reviewers was invaluable—your insights and suggestions were instrumental in shaping the future direction of our platform. Through this collaborative effort, we identified two significant architectural enhancements that are crucial for improving the flexibility and scalability of tinyMLaaS. These enhancements focus on two key areas: abstracting the model transformation process through flexible pipelining and enabling distributed execution of these transformations across various virtual machines (VMs).\n\nFlexible Pipelining for DNN Model Transformations\nAt the heart of DNN model compression lies a series of transformations applied to pre-trained models. These transformations are essential for optimizing models, reducing their size, and minimizing computational demands while maintaining accuracy. The term “DNN model” covers a broad spectrum, from a pure saved representation of computationa grapgh (e.g. a seris of feedforward networks, convolutional and recurrent networks) to an installable OS / docker container images, where a model comes with its runtime enviroments).\nIn the early stages of tinyMLaaS, our platform supported two primary types of transformations: the compiler and the installer:\n\nCompiler: This component applies a variety of compression techniques, both hardware-independent and hardware-aware. These include methods such as quantization (which reduces the precision of the model’s weights), layer fusion (which merges layers to streamline computation), and pruning (which removes redundant or less critical neurons). The goal is to shrink the model’s footprint and speed up inference without significantly sacrificing accuracy.\nInstaller: After compression, the installer packages the model with a runtime execution environment, preparing it for deployment in various formats. This could involve creating an installable operating system image or a Docker container that encapsulates everything needed to run the model.\n\nWhile these transformations proved effective, we recognized a limitation in treating them as distinct processes. By abstracting these processes into a unified concept we now refer to as a transformer, we introduce the ability to create more flexible and modular pipelines. With this new approach, any number of transformers, of any type, can be sequentially applied to a DNN model. This flexibility allows us to customize the compression and packaging workflow to suit the specific needs of each project or use case, enabling the creation of more complex and tailored solutions.\n\nFor instance, a user could now chain multiple transformations, such as applying quantization, followed by pruning, and then wrapping the final model in a Docker container, all within a single streamlined pipeline. This modularity also opens the door to integrating new and emerging techniques into the pipeline, ensuring that tinyMLaaS remains at the cutting edge of model compression technology.\n\n\nDistributed Execution of Model Transformations\nIn our initial design, tinyMLaaS operated on a single, large GPU machine. This centralized approach was chosen for simplicity, but as we explored real-world use cases, we encountered several challenges that highlighted the need for a more distributed architecture.\nOne significant issue arose with Quantization Aware Training (QAT), a technique where the model is retrained with quantization in mind to maintain accuracy post-compression. QAT requires the use of large datasets, which, in our original setup, had to be uploaded to our cloud infrastructure. This process not only consumed considerable time but also raised concerns among customers who needed to keep their proprietary data on-premises for security and compliance reasons.\nTo overcome these challenges, we are evolving tinyMLaaS to support distributed execution of model transformations. In this enhanced architecture, tinyMLaaS would continue to function as a control panel hosted in our data center. However, the actual processing of the models—whether it’s compression, packaging, or retraining—would take place on VMs located within the customer’s environment. This setup allows customers to maintain full control over their data, using their own computational resources to perform the necessary transformations. The benefits are twofold: it significantly reduces data transfer times and enhances security by keeping sensitive data within the customer’s infrastructure.\n\nOur platform’s current components, the compiler and installer, are already encapsulated as Docker containers. This containerization is a crucial advantage as it allows us to seamlessly integrate distributed execution. The missing piece was the ability to specify where each transformer should be executed. We are addressing this by leveraging Docker Swarm, a powerful tool for orchestrating distributed systems. Docker Swarm enables us to manage a cluster of VMs, dynamically assigning tasks based on the specific characteristics and capabilities of each VM. This orchestration allows us to distribute the transformation workload intelligently, whether on-premises or in the cloud, optimizing for performance, security, and efficiency.\n\n\nConclusion\nThe two architectural changes we’re implementing—flexible pipelining of DNN model transformations and distributed execution—are not just incremental improvements; they are transformative steps forward for the tinyMLaaS platform. These enhancements are driven by the real-world challenges and feedback we’ve encountered during our MVP iterations, ensuring that tinyMLaaS is equipped to meet the evolving needs of edge AI and model compression.\nAs we look to the future, we are eager to bring these innovations to life and continue pushing the boundaries of what tinyMLaaS can achieve. We believe these changes will empower our users to tackle more complex problems, deploy models more efficiently, and maintain the highest standards of security and scalability.\nThank you again to everyone who contributed feedback and ideas—your input is shaping the future of tinyMLaaS, and we’re excited to continue this journey together.\n\n\n\nReferences\n\nQuantization Aware Training: A method that simulates the effect of quantization during training, allowing for more accurate compressed models. For more details, see Quantization Aware Training.\nDocker Swarm: A native clustering and orchestration tool for Docker, enabling the management of a cluster of Docker nodes as a single virtual system. Learn more about Docker Swarm here.\nModel Compression Techniques: An overview of various DNN model compression techniques can be found in this comprehensive review."
  },
  {
    "objectID": "blogs/dnn_hsi_opt.html",
    "href": "blogs/dnn_hsi_opt.html",
    "title": "Optimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data",
    "section": "",
    "text": "Introduction\nHyperspectral Imaging (HSI) is a powerful technique that captures a wide spectrum of light across dozens to hundreds of narrow wavelength bands. Unlike traditional RGB imaging, which only captures three broad bands of red, green, and blue, HSI provides rich spectral information for each pixel, enabling detailed analysis of the material properties, composition, and other characteristics of the observed objects.\nHowever, this richness comes at a cost: HSI data is massive and complex, requiring sophisticated processing techniques to extract useful information. Deep Neural Networks (DNNs) are a natural choice for processing such data due to their ability to model complex patterns and relationships. But given the high dimensionality and large data volumes inherent to HSI, optimizing DNNs for this task is crucial to ensure efficiency and effectiveness. This article explores the key strategies for optimizing DNN models tailored for HSI data.\n\n\n\n1. Dimensionality Reduction\nHSI datasets often contain hundreds of spectral bands, leading to high-dimensional data that can be computationally expensive to process and prone to overfitting. Dimensionality reduction techniques help mitigate these issues by reducing the number of input features while preserving the essential information.\n\nPrincipal Component Analysis (PCA): PCA is widely used to reduce the dimensionality of HSI data by transforming it into a set of orthogonal components that capture the most variance in the data. This reduces the input size for DNNs, making the models more manageable and less computationally intensive.\nLinear Discriminant Analysis (LDA): LDA is particularly useful in supervised learning tasks, as it maximizes class separability by finding the linear combinations of features that best separate the classes. This helps in focusing the DNN on the most discriminative features.\nNon-negative Matrix Factorization (NMF): NMF decomposes the data into non-negative components, making it particularly suitable for HSI data, which often consists of non-negative values. This method not only reduces dimensionality but also facilitates interpretability, as the resulting components often correspond to physically meaningful spectra.\n\n\n\n2. Custom Network Architectures\nGiven the unique characteristics of HSI data, custom DNN architectures can be designed to exploit both the spectral and spatial dimensions effectively.\n\n3D Convolutional Neural Networks (3D-CNNs): Unlike traditional 2D CNNs used for RGB images, 3D-CNNs process data across three dimensions: height, width, and spectral depth. This allows the model to simultaneously capture spatial patterns and spectral features, making it ideal for HSI data.\nSpectral-Spatial Networks: These networks separately handle spectral and spatial features before combining them. For example, 1D CNNs can be used to process spectral data, while 2D CNNs handle spatial data. This approach ensures that the model effectively captures relevant features from both domains.\nCapsule Networks: Capsule networks preserve the hierarchical relationships between features, which can be particularly useful when dealing with the complex and multidimensional nature of HSI data. They can enhance the model’s ability to understand spatial hierarchies and the orientation of spectral features.\n\n\n\n3. Lightweight Models and Compression Techniques\nDue to the large size and complexity of HSI data, DNN models can become computationally expensive to train and deploy. Techniques to reduce the model size and complexity are essential for making them practical, especially in resource-constrained environments.\n\nKnowledge Distillation: This technique involves training a smaller “student” model to mimic the predictions of a larger “teacher” model. The student model, while being less complex, learns to approximate the teacher’s performance, making it more efficient for processing HSI data.\nModel Compression: Compression techniques such as quantization, pruning, and low-rank approximation can significantly reduce the size of a DNN without drastically affecting its performance. For instance, pruning removes less important neurons or filters, reducing computational cost and memory usage.\nLightweight Architectures: Adapting lightweight models like MobileNet or EfficientNet for HSI data processing can significantly improve efficiency. These architectures are designed to be computationally efficient and can be customized to handle the higher dimensionality of HSI data.\n\n\n\n4. Data Augmentation and Regularization\nHSI datasets are often smaller compared to RGB datasets, which increases the risk of overfitting. Data augmentation and regularization are crucial for improving the generalization capability of DNN models.\n\nData Augmentation: Techniques such as spectral variation, spatial transformations, and noise injection can increase the diversity of the training data, helping the model to generalize better to unseen data. This is particularly important in HSI, where the data can be highly variable depending on the environmental conditions.\nRegularization Techniques: Methods like dropout, L2 regularization, and spectral regularization are important for preventing overfitting. Regularization helps the model to remain robust and generalize well, even when trained on high-dimensional HSI data.\n\n\n\n5. Multi-Scale Learning\nHSI data often contains features at multiple spatial or spectral scales, making multi-scale learning an effective approach.\n\nMulti-Scale CNNs: These networks use filters of different sizes to capture features at various scales, allowing the model to understand both fine-grained details and broader patterns in the HSI data. This is crucial for accurately capturing the complex relationships inherent in HSI data.\nAttention Mechanisms: Attention mechanisms enable the model to focus on the most relevant spectral bands or spatial regions, improving both efficiency and accuracy. By selectively focusing on important features, attention mechanisms help the model to prioritize the most critical information, reducing computational overhead.\n\n\n\nConclusion\nOptimizing DNN models for processing HSI data is essential due to the high dimensionality and complexity of the data. Through dimensionality reduction, custom architectures, model compression, data augmentation, regularization, and multi-scale learning, it is possible to develop efficient and effective models for HSI applications. These optimizations not only make HSI processing more practical but also open up new possibilities for deploying such models in real-world scenarios, including mobile and edge computing environments. As technology continues to advance, the integration of HSI with optimized DNNs will likely play a crucial role in a wide range of fields, from agriculture and environmental monitoring to healthcare and beyond."
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html",
    "href": "tutorials/runwalkthrough.html",
    "title": "Run Walkthrough",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#summary",
    "href": "tutorials/runwalkthrough.html#summary",
    "title": "Run Walkthrough",
    "section": "Summary",
    "text": "Summary\nThe video tutorial is directly obtained from our internal review process of MVP6. It covers the process of creating and reviewing two default runs via project dashboard. Starting with user registration, the current MVP creates a default project and redirects the user to the project dashboard, in which page the main operation takes place. With the project dashboard, the user can specify a model and a dataset to run inference on a selected device. By default, two versions of this inference task are executed (forming two separate runs): a version that uses the original model for inference via Pytorch runtime, and a version that uses a quantized model (8-bit quantization) via tinyruntime (our custom runtime). After both runs are finished, the frontend provides visualizations to compare the results (accuracy), speed, and size of the model for each inference task.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#features-breakdown",
    "href": "tutorials/runwalkthrough.html#features-breakdown",
    "title": "Run Walkthrough",
    "section": "Features Breakdown",
    "text": "Features Breakdown\n\nRegistration & Project Setup\nThe registration process in MVP6 is simplified, only requiring the user’s email address and a secure password. Email verification is yet to be implemented at this stage. After a successful registration, the user is redirected to the project dashboard with a default name and description, both of which can be modified later on. New projects can be created via sidebar button, which requires a unique name and possibly some description. Different projects can be selected also via the sidebar, which navigates to the project dashboard page of the corresponding selected project.\n\n\nModel & Dataset Selection\nWe are currently using models and datasets uploaded to Hugging Face Hub. The user can choose any publicly accessible models or datasets from Hugging Face. However, the currently recommended ones are our own uploaded models (ninjalabo/resnet18, ninjalabo/resnet34, and ninjalabo/resnet50) and dataset (ninjalabo/imagenette2-320) for consistency in directory structure and accepted file formats.\nThe user can also upload models and datasets to our own UI, which accepts a model file model.pkl and a dataset file in .zip or .tar.gz format. For convenience, the user can download our example model and dataset to easily test this feature without compatability issue.\n\n\nDevice Selection\nThe user can setup and select an edge device in which the inference task(s) shall be executed. The user can register a device with information including its architecture, connectivity details, memory and storage limits, and installation option. Currently, we are supporting devices with arm64 or amd64 architectures, in which the model will be installed in the form of a Docker image in our VM. The model will be optimized to the selected device, and its performance on it is measured and reported (accuracy, execution time, and model size).",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "href": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "title": "Project starting point",
    "section": "",
    "text": "This was a successor of the previous software project in Winter. Here are the info:\n\nVideo Sprint3 demo, Mid\nVideo Sprint7 demo, Final\nSource code in GH Repo\nProject documentation, generated automatically\n\n\n\n\nimage.png\n\n\n     \n\nGoal for summer 2023 project\nRight now there’s no clear boundary between UI and its backend. We want to make them separted. The current Streamlit UI should be a pure frontend. The backend logic should be a REST backend server (e.g. fast API) Finall we want a CLI tool to control in additon to the current UI. A CLI tool should do the exact same things as the UI does right now.\n$ tmlaas device list\n&lt;list device name&gt;\n$ tmlaas model list\n&lt;list device name&gt;\n$ tmlass device=&lt;device id&gt; install model=&lt;model id&gt;\nYou may want to refer to this project as CLI example, https://ghapi.fast.ai/\n\n\nDevelopment environment\n\nSCRUM, User story mapping to set common goals with all stakeholders.\nNbdev, Jupyter notebook framework for code, (unit)tests & doc at once, Nbdev tutorial video.\nDocker compose to run the whole system at once, turorial video.\nAcceptance Test Driven Development (ATDD) to sync up with a client.\nStreamlit for UI framework used in this project.\nGH Project as Kanban\nGH Workflow as CI/CD\n\n\n\nCommunication\n\nDiscord, Click to join.\n\n\n\nNext\n\nWho’s SCRUM master for Sprint1?\nSprint1 (Week20)\n\nInitial research of this project by Students\n\nSprint1 review & planning at 10:00AM 22nd May\n\nReview WoW proposal from students\nQ&A for a client\nPrioritize user story?\n\nWhich Kanban board to share with customer?",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Project starting point"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html",
    "href": "docs/tinymlaas/Demonstration.html",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "",
    "text": "This document will demonstrate the steps in TinyMLaaS app. Some pages have dependencies on other pages, so go through pages from top to bottom is recommended at the start.\nIn order to run TinyMLaaS end-to-end following components need to be run: - The Frontend - The Backend - The Relay\nThese can all easily be started with the help of docker using the docker-compose-with-bridge.yml file in the Main repository. The application can be started with",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#device",
    "href": "docs/tinymlaas/Demonstration.html#device",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Device",
    "text": "Device\nIn order to install a model to a embedded device, the briging device of the wanted device and the device need to be selected.\nThe first thing that should be done on the Device page is to either add a new bridge or selecting an existing bridge. Let’s add the bridge that was started by docker compose. To do that, add the name of the bridge docker container with the port 8080 as the bridges address. The bridge will also not use a HTTPS connection in this case.\n\n\n\nAdd new bridge\n\n\nAfter adding the bridge, select the wanted bridge by clicking the Select bridge button next to the wanted bridge\n\n\n\nSelect the bridge\n\n\nSelecting a device to which to install the trained machine learning model later on is required. If the wanted device has not been registered already, register it either manually or by selecting it from the list of devices connected to the bridge. Lets add a device connected to the bridge by pressing the Register this device button next to that device\n\n\n\nRegister the new device\n\n\nAdd the missing information on the form and click add\n\n\n\nDevice form\n\n\nThe added device will automatically be selected as the active device.\n\n\n\nSelected device",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#data",
    "href": "docs/tinymlaas/Demonstration.html#data",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Data",
    "text": "Data\nIn order to train a model, a dataset with which to train the model needs to be selected.\n\n\n\nDataset selection complete\n\n\nUser can add images from local storage to selected dataset.\nIf the existing datasets are not enough, a new dataset can be added to the software.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#model",
    "href": "docs/tinymlaas/Demonstration.html#model",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Model",
    "text": "Model\nThis page shows already trained models as well as allows training of a new model.\nLet’s train a new model. For this, we first need to decide the parameters with which to train the model with. This time we chose to train the model with 27 epochs and with a batch size of 56. The image size is 96x96, as this model is trained for an Arduino, which takes pictures of this size.\n\n\n\nTrain a new model\n\n\nAfter the training is done, the software will show an image of the statistics of the training process as well as a test image with a prediction that the newly trained model gave for that picture.\n\n\n\nAfter training",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#compiling",
    "href": "docs/tinymlaas/Demonstration.html#compiling",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Compiling",
    "text": "Compiling\nThe page is responsible for ML compilation. It will turn the selected ML model and turn it into a tflite model as well as generate a C-array of it. The C array is the tflite model turned into bytes stored in a C array, which is required for embedded devices, which do not have a filesystem.\nAfter the compiling is done, the newly compiled model will be selected as the active model.\n\n\n\nCompilation done",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#installing",
    "href": "docs/tinymlaas/Demonstration.html#installing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Installing",
    "text": "Installing\nNow that a model has been compiled, it can be installed on the device that was selected on the Device page. The page shows a single button, install. When this is pressed, the software will install the selected compiled model to the selected device on the selected bridge.\nBe sure that the software has access to the device. If you are not sure, the next command will give all users permissions to read, write and execute to the machine\nchmod 777 /path/to/port\nThis time, the device is connected to /dev/ttyACM0, so it was given permissions.\nNow, install the model to the device.\n\n\n\nInstall successfully complete",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#observing",
    "href": "docs/tinymlaas/Demonstration.html#observing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Observing",
    "text": "Observing\nOn the observing page, user can see real-time predictions from device when the start button has been activated.\n\n\n\nReal-time predictions as device output",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Use_cases.html",
    "href": "docs/tinymlaas/Use_cases.html",
    "title": "Use case scenarios",
    "section": "",
    "text": "As a security-conscious homeowner, I want to use TinyML(*) for human-detection to monitor my home and receive alerts when unexpected activity is detected, so that I can take action to ensure my home is safe.\n\nData Collection: User collects video footage from their smart camera, which will be used to train the TinyML model for human-detection.\nModel Training: User trains a TinyML human-detection model using the collected video footage and publicly available datasets.\nModel Squeezing: User optimizes the model size to ensure it can be deployed on TinyML-enabled devices.\nModel Deployment: User deploys the optimized TinyML model on their smart camera.\nInference: The smart camera uses the deployed TinyML model to perform real-time human-detection, identifying humans and activities within its field of view.\nAlerts: If the camera detects unusual activity, such as a person entering the home when no one is expected to be there, it sends alerts to the user’s smartphone.\nModel Update: User periodically updates the TinyML model with new data to ensure its accuracy and improve its performance over time.\nUser Access: User can access real-time video footage and receive notifications to confirm the alert and take appropriate action, such as contacting the police or checking in on the home from a remote location.\n\n(*) The alternative to TinyML would be running human detection in the cloud, with all the possible network latency issue we may face. TinyML-based human-detection can trigger an alarm installed “in place” where the camera is. By relying on cloud, we would need to transfer the video footage to the cloud and then running inference at there.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Use case scenarios"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "title": "Arduino sketch",
    "section": "",
    "text": "This is a arduino sktech for image recognition with the help of machine learning. It has been designed to be used on the Arduino Nano 33 BLE with the OV7675 camera module.\nAt the moment, it can be used for binary detection, for example weather there is a person in the picture.\n\n\nThis sketch depends on Tensorflow lite for microcontrollers.\n\n\n\n\n\nFirst, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list\n\n\n\n\nIn order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "title": "Arduino sketch",
    "section": "",
    "text": "This sketch depends on Tensorflow lite for microcontrollers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "title": "Arduino sketch",
    "section": "",
    "text": "First, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "title": "Arduino sketch",
    "section": "",
    "text": "In order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html",
    "href": "docs/tinymlaas/Technologies.html",
    "title": "Technological choices",
    "section": "",
    "text": "This document is meant to answer why certain technological choices have been made and why certain frameworks have been chosen.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#main",
    "href": "docs/tinymlaas/Technologies.html#main",
    "title": "Technological choices",
    "section": "Main",
    "text": "Main\nThe TinyMLaaS-main -repository is meant for mostly building the whole software with docker and documentation. However, some development is also done in this repository. All tensorflow modules are coded here in Jupyter notebooks.\n\nJupyter notebooks and NBDEV\nAs jupyter notebooks can not be used as modules in python, they need to be exported into python modules. This is done with NBDEV. Nbdev also automatically creates documentation from the jupyter notebooks and deploys them to Github pages\n\n\nDocker\nRunning the software is meant to be done with docker. Docker allows running the software on different computers, without the software being platform spesific. Also, all the dependencies required for the software do not need to be installed on the host, rather, they will all be installed in the seperate docker container. If you are not familiar with docker, check out University of Helsinkis course Devops with Docker materials to get a basic understanding of docker.\n\nSysbox\nThere are parts of the software that require starting their own docker containers. For example, the relay will start containers to compile arduino sketches and to install them to the devices. When running the relay itself inside a docker container, there are a few ways of starting a new docker container from this docker container. First, is by using so called sibling containers. This gives the docker container access to the host machines docker socket, which allows it to control other containers on the host machine. However, this has a big security flaw, as if someone gets access to this docker container, they will be able to control all other docker containers on the host machine and start their own containers. The other way is by using docker inside docker, which allows docker containers to be started inside the docker container recursively. This approach requires privileged mode to be set for the docker container, meaning that it will have root privileges on the host machine. In order to run docker machines without privileged mode, Sysbox runtime is used. This allows starting docker containers inside the docker containers without having the docker container in privileged mode, which is a lot more secure and allows for better isolation of the docker containers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#frontend",
    "href": "docs/tinymlaas/Technologies.html#frontend",
    "title": "Technological choices",
    "section": "Frontend",
    "text": "Frontend\n\nStreamlit\nThe frontend of the software is build with Streamlit. This is done to make the development process faster, as this frontend is mainly meant for demo purposes. Streamlit makes it easy to create good looking websites, however, there isn’t much room for cutomization and some features can be quite difficult to create.\nThere is still a dependecy on usbutils. This will be talked more about in the Bridge-section",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#backend",
    "href": "docs/tinymlaas/Technologies.html#backend",
    "title": "Technological choices",
    "section": "Backend",
    "text": "Backend\nThe backend is the heart of the software. It does all the communication between all the other modules and does a lot of the heavy lifting of the software. It is created as a API.\n\nFastAPI\nThe backend is created with FastAPI. FastAPI is very powerful for creating API:s, as it has great data validation with the help of Pydantic, it automatically creates good documentation about the different API requests and is simple to understand. To checkout more, read the Starting documentation.\nWhen deploying the API to production, the api will most likely be behind a proxy with some URL that has prefixes. For example, it might be deployed to example.uri.com/api/. For the API to function correctly, the root-path of the API needs to be declared for FastAPI, in this case, /api/.\n\n\nSQLAlchemy\nThe backend talks with the database with sqlalchemy. This means that it is able to talk with any SQL-database without any changes to the backends software.\nAs of now, the database in use is sqlite. However, this is meant to be more of a temporary solution to make development easier. For more information, checkout the suggestions in Suggestions for further development",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#bridge",
    "href": "docs/tinymlaas/Technologies.html#bridge",
    "title": "Technological choices",
    "section": "Bridge",
    "text": "Bridge\nThe relay is the part of the software to which the microcontrollers are connected to. It is also done in API style.\n\nFlask\nUnlike the backend, the bridge is created with Flask. Flask is lightweight and easy to understand, which makes sence for the bridge, as the hardware, on which the bridge runs, might not be that powerfull.\n\n\nUsbutils\nTo find USB-devices, the software does not use pythons libraries, such as PyUSB. This is because these softwares also have OS dependencies, that need to be installed and do not work that well in docker containers. USButils works great in contianers and is easy to install, which is why it has been chosen over pythons own libraries.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#command-line-interface",
    "href": "docs/tinymlaas/Technologies.html#command-line-interface",
    "title": "Technological choices",
    "section": "Command-line interface",
    "text": "Command-line interface\nAn API client is automatically generated from the OpenAPI definition provided by FastAPI. The generation is done with OpenAPI Generator. A command line tool interfacing the autogenerated client is built with Typer.\n\nOpen API Generator\nOpenAPI Generator enables building extensive Python clients with documentation. Generating a client makes it easy to design customized workflows around the API. This project uses the client as the main component of the command-line interface. The autogeneration also builds templates for tests. More generally using a generator was a way to test automatic code generation.\n\n\nTyper\nTyper is an easy to use Python library for building command-line interfaces. Typer can be used to build light weight CLI’s so it’s a good fit for the autogenerated client. Typer also uses Python type hints and provides automatic help functions that include required arguments and a command’s description from docstrings.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html",
    "href": "docs/tinymlaas/Architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "This page contains general information about the architechture and how each component is related to each other.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#overview",
    "href": "docs/tinymlaas/Architecture.html#overview",
    "title": "Architecture",
    "section": "Overview",
    "text": "Overview\nTinyMLaaS consist of several components: - backend - streamlit frontend - cli - MCU components\nThe different components are stored in their own repositories which can be found here.\nThe backend is the core component which contains all the API endpoints. By calling them you can execute all the tasks necessary for the workflow. The backend is responsible with communicating with the machine learing components, storing data in the database and installing & managing MCU devices.\nTensorflow machine learning components live in the main repository and need to be fetched for the backend seperately.\nMCU repository contains the bridge for communicating with the devices and the code needed for devices.\nWe have implemented two different interfaces for the TinyMLaaS: CLI and website GUI using streamlit. Since you can make API calls directly to backend it’s extremely simple to build your own frontends in the future.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#block-diagram",
    "href": "docs/tinymlaas/Architecture.html#block-diagram",
    "title": "Architecture",
    "section": "Block Diagram",
    "text": "Block Diagram\n\n\n\nBlock Diagram\n\n\nThe backend is the main component that deals with calling the tensorflow functions and communicating with the MCU devices. Tensorflow is currently the supported UI but you can also make API calls directly or use the CLI. In the future the tensorflow components can be containarized as their own service.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#key-components",
    "href": "docs/tinymlaas/Architecture.html#key-components",
    "title": "Architecture",
    "section": "Key Components",
    "text": "Key Components\n\nML model training\nData Storage and loading (database)\nML model quantization and optimization\nML model compilation for MCUs",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#database-diagram",
    "href": "docs/tinymlaas/Architecture.html#database-diagram",
    "title": "Architecture",
    "section": "Database diagram",
    "text": "Database diagram\n\n\n\nDatabase Diagram",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.247210\n473.507812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n9.644408\n98.714844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n7.335336\n62.062500\n11.197475\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#amd64",
    "href": "performance.html#amd64",
    "title": "Performance",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.247210\n473.507812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n9.644408\n98.714844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n7.335336\n62.062500\n11.197475\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#arm64",
    "href": "performance.html#arm64",
    "title": "Performance",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html",
    "href": "docs/tinymlaas/Next_steps.html",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "href": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#database",
    "href": "docs/tinymlaas/Next_steps.html#database",
    "title": "Suggestions for further development",
    "section": "Database",
    "text": "Database\n\nProduction ready database\nThe backend uses SQLite. However, as SQlite is so lightweight, there are drawbacks, that affect the usage of the software. First of all, SQLite is not meant for storing big files. Because of this, all datasets, models and compiled models are stored outside the database in directories, and the database contains the path to the files. This is not ideal, as the backend can get messy with all the directories and if permanent storage is required outside the docker container, all of these volumes need to be mounted to the docker container.\nA SQLite database is also a single file, meaning that accidentally deleting the database or misplacing it is more common.\nBecause of these drawbacks, we would suggest changing the used database from SQLite to something more robust and production ready, such as MariaDB or PostgreSQL. Larger files can be stored in these databases and it is easy to mount one database rather than multiple different locations for all different saving locations.\n\n\nMove saving of models and datasets to database\nAs mentioned in the previous section, all datasets and models are stored outside the database. With the change of the database, it would be better to save all these files in the database.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#datasets",
    "href": "docs/tinymlaas/Next_steps.html#datasets",
    "title": "Suggestions for further development",
    "section": "Datasets",
    "text": "Datasets\n\nDownloading/uploading datasets (as zip)\nCurrently, you can only upload and append to datasets. It’s not possible to view or download it anyway. The application should have way to download the entire dataset to your machine (propably as zip so it works on both Windows and Linux).\nAt the moment you can only send bulk of pictures of to the app to create a dataset. It’s propably good idea to allow sending zips and creating datasets from them. You propably need to make sure that - it’s a valid dataset with working images - the saved dataset keeps same folder structure as the send zip - the user doesn’t send malicious data\n\n\nEditing, deleting and managing datasets\nCurrently, only adding and appending datasets is supported. The user should propably be able to manage the datasets better. First you should be able to remove unwanted datasets. Second, the user should be able to make folder structure for the datasets and edit them for labeling purposes see this tutorial for example how the data is structured. Being able to simply download the datasets,editing it locally, removing it from the app and readding it allows for the user to edit it. After that, if you want to enhance the user experience one idea is to add better editing options to the app itself: Being able remove photos, being able to add photos, creating new folders, removing folders.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#cli",
    "href": "docs/tinymlaas/Next_steps.html#cli",
    "title": "Suggestions for further development",
    "section": "CLI",
    "text": "CLI\nThe CLI is mostly autogenerated with this OpenAPI Generator. The current generator works as far as the existing API endpoints stay the same regarding input and output. Whenever new HTTP methods are added or modified a new generation has to be done. This can be done by using the json-file provided by the FastAPI backend. See detailed instructions here.\n\nAdd missing functionality + fixes to existing\nOnly some of the functionality is available with the CLI. See examples from the already implemented functionality from services and tiny_mlaas.py. To help with implementing more functionality, see examples for using the client from the docs. For example for bridges here.\nAdding and listing existing devices isn’t working properly because newly added devices don’t have a designated bridge. It’s worth considering if adding a device should be possible without a bridge. This fix would be implemented in backend. Alternative fix is to modify the validation done by the generated client.\nTraining via the CLI trains a model, but the output contains a picture that can’t be displayed in CLI. A possible fix is to modify the training function so that displaying an example of a prediction is optional.\nMany of the functions have hardcoded parameters. This helps with development and testing, but should be fixed in the future. See example of adding parameters: https://github.com/TinyMLaas/TinyML-CLI/blob/main/services/models.py . Here dataset_id and description are required parameters. Epoch and the rest of the hard coded variables can be handled in a similar fashion.\n\n\nAutogenerate end-to-end CLI from OpenAPI YAML\nThe current version of the CLI is autogenerated with the exception of services and tiny_mlaas.py.\nEnd-to-end autogeneration could be done after publishing services and tiny-mlaas.py as a Python package and having it as a dependency. See instructions: https://typer.tiangolo.com/tutorial/package/.\nA preliminary idea for implementing the end-to-end generation is following:\n\nPublish a package that contains the forementioned files\nOpenAPI yaml-file is needed for generating the CLI tool. Get it by browsing to backend_url/openapi.json. Convert the json to yaml with for example: https://editor.swagger.io/\n\nThe repository for autogeneration would consist of the yaml -file from step 2 and requirements.txt -file with the package published in step 1 (in addition to current requirements). With these prerequisites the steps for end-to-end generation could be:\n\nClone repo\nInstall the generator tool with: npm install @openapitools/openapi-generator-cli -g\nGenerate the client with npx @openapitools/openapi-generator-cli generate -i file.yaml -g python -o output_path\n\nfile.yaml is the yaml-file in the repo\noutput path should probably be the root directory of the cloned CLI-repository\n\nInstall the requirements: pip install -r requirements.txt\nFinish the installation with: python3 setup.py install\n\nFor usage instructions see: https://github.com/TinyMLaas/TinyML-CLI#usage\nMisc: if requirements.txt is overwritten by the generator use a different name, interface_requirements.txt etc (remember to install these in step 4). Using the CLI package might differ from using it locally, see Typer documentation: https://typer.tiangolo.com/tutorial/package/",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#bridge",
    "href": "docs/tinymlaas/Next_steps.html#bridge",
    "title": "Suggestions for further development",
    "section": "Bridge",
    "text": "Bridge\n\nSupport for Raspberry PI\nInstallation and observation for Arduino nano 33 BLE has been imported fully to this version of the version. However, the Raspberry PI support has not been tested at all and most likely will not work out of the box. Adding support/making sure installation to Raspberries work is a required feature, that unfortunately does not exist yet.\n\n\nBridge port\nA bridge can be saved to the backend as an IP address and as an URL. There is right now no validation to make sure that the given IP address or URL is a valid address. Also, if IP address is used, it automatically asumes that the bridge is hosted on port 5000. This should be changed so that the bridge can be hosted on any port.\n\n\nError handling\nThere is little to none error handling on the bridge. This means that, even if operations fail, it will still say the operation was successfull. The next ones are known errors that do not have error handling:\n\nWhen the compiled arduino sketch is uploaded successfully, but it can not be started for some reason, most likely because there isn’t enough memory for model on the device.\nObservation doesn’t have permission to the device.\n\nThe installation process has a chance to fail when running inside a docker container and the reason is unknown.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html",
    "href": "docs/tinymlaas/TinyML-backend_README.html",
    "title": "TinyML-backend",
    "section": "",
    "text": "GitHub Actions\n\n\nThis is the backend for the TinyMLaaS project.\n\n\nStart by cloning this repository. Then setup and run the app with the following steps.\n\n\n\nSetup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate\n\n\n\nInstall project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt\n\n\n\nIn a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"\n\n\n\nThe projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.\n\n\n\nWith the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload\n\n\n\nUse the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.\n\n\n\nRun unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "href": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "title": "TinyML-backend",
    "section": "",
    "text": "Start by cloning this repository. Then setup and run the app with the following steps.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "href": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "title": "TinyML-backend",
    "section": "",
    "text": "Setup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "title": "TinyML-backend",
    "section": "",
    "text": "Install project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "In a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "The projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "href": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "title": "TinyML-backend",
    "section": "",
    "text": "With the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "href": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "title": "TinyML-backend",
    "section": "",
    "text": "Use the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "title": "TinyML-backend",
    "section": "",
    "text": "Run unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html",
    "href": "docs/tinymlaas/TinyMLaaS_README.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\nGitHub Actions\n\n\n\n\nThe GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.\n\n\n\n\nBackend\nFrontend\nCLI\nMCU components\n\n\n\n\n\nWay of Working\nProduct backlog\nWorking hours\nDatabase schema\n\n\n\n\nUse Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "The GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Backend\nFrontend\nCLI\nMCU components",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Way of Working\nProduct backlog\nWorking hours\nDatabase schema",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Use Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html",
    "href": "docs/tinymlaas/TinyML-frontend_README.html",
    "title": "TinyML-frontend",
    "section": "",
    "text": "GitHub Actions\n\n\nfrontend for TinyMLaaS.\n\n\nDepends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils\n\n\n\nThe frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.\n\n\n\nRun backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py\n\n\n\nThis project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Depends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "title": "TinyML-frontend",
    "section": "",
    "text": "The frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Run backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "title": "TinyML-frontend",
    "section": "",
    "text": "This project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_README.html",
    "title": "TinyML-MCU",
    "section": "",
    "text": "Code for TinyMLaaS MCU devices.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "title": "TinyML-MCU",
    "section": "Dependencies",
    "text": "Dependencies\nThe bridge has a dependency on usbutils. On Debian-based systems, it can be installed with\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "title": "TinyML-MCU",
    "section": "Running",
    "text": "Running\n\nLocally\nInstall the python dependecies\npip install -r requirements.txt\nThere are two different ways to run the bridge locally.\nFirst, to run a development server, run\nflask --app main.py --debug run\nFor production, this is not ideal. To use waitress as production server\nwaitress-serve main:app\n\n\nDocker\nThe docker version of the bridge requires nestybox/sysbox.\nThe provided docker-compose file looks like this\nversion: '3'\n\nservices:\n  bridge:\n    build:\n      context: .\n    # If you have devices connected to the relay, add them here\n    # devices:\n      # - \"/dev/ttyACM0:/dev/ttyACM0\"\n    volumes:\n      - \"/dev/bus/usb:/dev/bus/usb\"\n      - \"/dev/serial:/dev/serial\"\n    runtime: sysbox-runc\n    ports:\n      - 5000:8080\nAny microcontrollers you want to control with the bridge need to be added to the devices at this point. They also need to be connected to the computer.\nIf you want to add more microcontrollers to the bridge later, you can add them manually by adding them to the docker-compose file and restarting the container.\nThere is also a provided script to check for added Arduinos. The script docker-container-restarted automatically detects, when new arduinos are added and restart the container with the new devices. It can be run by making it executable and running it\nchmod u+x docker-container-restarter.py\n./docker-container-restarter.py\nThere might be problems with serial port permissions. As a solution, give permissions to the port for all users with\nchmod 777 /path/to/port\nNote that this will most likely require root priviledges.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "title": "TinyML-MCU",
    "section": "Serving the application to internet",
    "text": "Serving the application to internet\n\nWebserver\nIf you already have a webserver setup, such as Apache2 or Nginx, with which you can serve the application to the internet, add a new proxy for the bridge according to the style of the webserver.\n\n\nPort forwarding\nAnother way is to add a new port forwarding setting to the host machine to port 5000 in the router.\n\n\nNgrok\nHowever, these might not be usable by everyone. With Ngrok, you can serve the bridge to the internet without having access to the local router.\n\nInstall ngrok\nServe the application with\n\nngrok http 5000",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "title": "TinyML-MCU",
    "section": "Microcontrollers",
    "text": "Microcontrollers\nThere are seperate instructions for microcontrollers.\nArduino",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html",
    "href": "docs/tinymlaas/Background.html",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "href": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#edge-impulse",
    "href": "docs/tinymlaas/Background.html#edge-impulse",
    "title": "Background information and some literature sources",
    "section": "Edge Impulse",
    "text": "Edge Impulse\nEdge Impulse is the leading development platform for ML on edge devices. It is free for developers. With Edge Impulse one can: - build advanced embedded ML apps - build custom datasets rapidly - collect sensor, audio or camera data from devices, files or cloud integrations - use automatic labeling tools such as object detection and audio segmentation - use Edge Impulse cloud infrastructure to set up and run reusable scripted operations that transform the input data on large sets of data in parallel - integrate deployment pipelines, CI/CD tools and custom data sources with open APIs - develop models and algortihms - with prebuilt DSP (Digital Signal Processors) and ML blocks - hardware decisions can be made on device performance and Flash/RAM on every step - DSP feature extraction algorithms can be customized - custom machine learning models with Keras APIs - use visualized insights on datasets, model performance and memory to fine-tune the production model - optimize models and algorithms - EON TUNER for finding balance between DSP configurations and model architecture, budgeted against memory and latency constraints - EON Compiler for lighter and faster neural networks with equal accuracy - have full visibility across the whole ML pipeline - complete access to data attributes, DSP algorithms, model hyperparameters throughout whole development lifecycle - test model performance accurately - virtual cloud hardware simulation framework to get performance and accuracy metrics before deploying on any physical device - model performance can be evaluated with live classification - devices, automated ML pipeline testing, integration with the testing framework - deploy easily on any edge target - optimize source code by generating optimized embedded libraries and applications for any edge device - build ready-to-go binaries with selected development boards supported by Edge Impulse with special firmware - without OS or hardware dependencies and compile to nearly anything - make digital twin by re-deploying cloud-hosted ML projects to any hardware target on the fly - benefit from access and integrations to the leading hardware partner ecosystem from MCUs to MPUs and GPUs including acceleration - Arduino - Himax - OpenMV - Nvidia - Nordic semiconductor - Raspberry Pi - Silicon Labs - Sony - ST - Syntiant - Texas Instruments",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "href": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "title": "Background information and some literature sources",
    "section": "Arduino and Wiring",
    "text": "Arduino and Wiring\nArduino is an open-source electronics platform intended for anyone to make interactive projects. It is based on easy-to-use hardware and software. Arduino boards can - read inputs: light on a sensor, finger on a button, twitter message - turn inputs into outputs: activate motor, turn on a LED, publish something online - be instructed what to do by sending a set of instructions to the microcontroller on the board - using Arduino programming language (based on Wiring) and the Arduino Software (IDE) (based on Processing, an open source integrated development environment (IDE) like the Arduino IDE)\nBecause of simple and accessible user experience Arduino has been used in thousands of projects and applications by a worldwide community of makers, students, hobbyists, artists, programmers and professionals to produce a vast amount of accessible knowledge. The product range includes products for IoT applications, wearable, 3D printing and embedded environments.\nThe Arduino software - is easy-to-use for beginners but flexible enough for advanced users - runs on Mac, Windows and Linux - is used by teachers and students to build low cost scientific instruments to prove chemistry and physics principles and to get started with programming and robotics - is used by designers and architects to build interactive prototypes - is used by musicians and artists to build installations ansd to experiment with new musical instruments - is used by makers to build projects - can be used by anyone by following detailed instructions of a kit or sharing ideas online\nArduino offers some pros over other microcontrollers and microcontroller platforms available: - inexpensive compared to other microcontrolle platforms; the least expensive version of the Arduino module can be assembled by hand, pre-assembled Arduino modules are also affordable - cross-platform, the Arduino software (IDE) runs on Windows, Macintosh OSX, Linux (most microcontroller systems are limited to Windows) - simple, clear programming environment - easy-to-use for beginners but flexible enough for advanced users - because it’s based on Processing programming environment, students learning to program in that environment will be familiar with how the Arduino IDE works - open source and extensible software, published as open source tools - the language is based on AVR-C programming and can be expanded through C++ libraries - AVR-C code can be added directly into the Arduino programs - open source and extensible hardware, with the plans of the Arduino boards published under a Creative Commons licence, so own versions of the module with extensions and improvements can be built by anyone\nCode can be developed in the Arduino Cloud to build smart IoT projects. - smart devices can be connected within minutes - wide range of compatible devices, the Arduino Cloud provides the necessary code - nice dashbords can be created with mix and match customizable widgets to visualize real time or historical data or to control the device - projects can be controlled from anywhere in the world from any device, for example Alexa. - projects and libraries are always synced and up to date - all projects are cloud based and accessible from any device - data is always ecrypted and always belongs to the user (you) - is open and customizable, has flexible APIs to integrate and customize Cloud - all connected Arduino boards have a built-in crypto chip that makes them incredibly secure - sketches and project data are stored in AES 256-bit encrypted datastores - account security is protected with single use authentication codes - open and transparent data privacy terms and your data always belongs to you - has a wide range of resources - tutorials - APIs - documentation\nArduino’s history is interesting. The Arduino project was based on the developing platform Wiring created by Hernando Barragán as a Master’s thesis project at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy in 2003.\nHernando has been developing Wiring ever since and now Wiring is an open source electronics prototyping platform composed of programming language, an integrated development environment (IDE) , and a single-board microcontroller. More about Wiring can be found here.\nWiring offers new boards to customers in their webshop, but Wiring supports other boards directly, so Wiring IDE can be used on other boards as well (and more will be added). Thus Arduino board can be used with the Wiring IDE. Wiring can be downloaded for Linux, MacOS X and Windows.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#mbed",
    "href": "docs/tinymlaas/Background.html#mbed",
    "title": "Background information and some literature sources",
    "section": "Mbed",
    "text": "Mbed\nMbed is an open-source platform and operating system for internet-connected devices that are based on 32-bit ARM Cortex-M microcontrollers. These devices are known as IoT devices. Mbed is collaboratively developed by Arm and its tech partners.\nMbed is free and open source IoT operating system with connectivity, security, storage, device management and ML. Mbed offers free development tools, thousands of code examples and support for hundreds of microcontrollers development boards, as described here. - Mbed has its own Mbed OS with - well-defined API to develop C++ applications - free tools and thousands of code examples, libraries and drivers for common components - built-in security stack - core components such as storage and connectivity options - Mbed Enabled hardware has many options - Compiler and IDE - Keil Studio Cloud - modern, zero-installation Mbed development environment in the browser - code high-lighting, WebUSB flash and debug and version control - Mbed Studio - an IDE for application and library development - single environment with everything to create, compile and debug Mbed programs - Mbed CLI - command line interface allows to integrate Mbed functionality into preferred editor or enhance automation setup - Security - Arm Mbed TLS provides comprehensive SSL/TLS solution - easy to include cryptographic and SSL/TLS capabilities in the software and embedded products - as an SSL library Arm Mbed TLS provides an intuitive API, readable source code and a minimal and highly configurable code footprint",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#ifttt",
    "href": "docs/tinymlaas/Background.html#ifttt",
    "title": "Background information and some literature sources",
    "section": "IFTTT",
    "text": "IFTTT\nIFTTT is a private company that runs online digital automation platforms and offers them as a service. IFTTT is short for If This Then That. IFTTT integrates apps, devices and services quick and easy. IFTTT makes tech incompatibility easy to tackle. Automating process is simple, the user chooses - trigger - action(s) - name for the applet and finish\nIFTTT has over 700 services ready (more added weekly) to be automated. Price range of the services is from 0€/forever to 5€/month. IFTTT provides a simple way to create for example a smart home: - make a user account and log in (can be done with Google or Facebook) - trigger: give Google Assistant a voice command “Hey Google, I need coffee” - action(s): coffee machine is turned on and when the coffee is ready, the coffee machine turns off - name the applet: “make coffee” and finish the applet",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sony-mesh",
    "href": "docs/tinymlaas/Background.html#sony-mesh",
    "title": "Background information and some literature sources",
    "section": "Sony MESH",
    "text": "Sony MESH\nSony MESH is a Sony corporate startup that sells a range of colored blocks with different sensors and wireless connection to the IoT. It’s digital DIY platform to connect everyday objects into IoT and create your own projects. - MESH blocks are wireless - Visual Coding App called Canvas simplifies programming and wiring with drag-and-drop functions - project can be connected to web services and popular smart gadgets like WeMo and Google Assistant voice activation - hardware projects can be expanded without expertise - IoT block in the project allows additions of smart features such as motion-sensitivity, remote control, orientation monitoring, voice commands, notifications, text messaging and more - projects can be connected to the internet instantly - project can be transformed into an IoT device, such as - Twitter alarm system - a voice-activated, data-logging, remote-controlled car - allows customization of smart gadgets - MESH is compatible with over 350 smart gadgets, home automation devices and web services on IFTTT - each IoT block has built-in IFTTT integration, so that it’s simple to add custom features on a smart gadget - MESH Motion and MESH Temperature & Humidity used together allow addition of motion-activated, multi-room temperature monitoring to a smart device like Nest thermostat - allows to build own smart gadget - MESH GPIO is a simple interface for development boards like Arduino and Raspberry Pi or actuators like a DC motor - MESH GPIO integrates any smart devices or web services on IFTTT, incl. - Amazon Alexa for Echo - Google Assistant - Google Sheets - LIFX - Nest - Phillips Hue - Twitter - WeMo - over 350 more - MESH blocks use Bluetooth - MESH blocks are rechargeable, durable and compact",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sources",
    "href": "docs/tinymlaas/Background.html#sources",
    "title": "Background information and some literature sources",
    "section": "Sources:",
    "text": "Sources:\nOn-Device Training of Machine Learning Models on Microcontrollers with Federated Learning\nTinyFedTL: Federated Transfer Learning on Ubiquitous Tiny IoT Devices\nEdge Impulse\nBeginner’s Guide to DSP\nKeras APIs\nArduino\nArduino Getting started guide\nArduino Tutorials on Arduino Project Hub\nMbed\nMbed (product)\nIFTTT\nMESH blocks\nWiring\nWiring webshop",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Low Rank Approximation Implementation 1/4\n\n\n\n\n\n\nTech\n\n\n\nSingular Value Decomposition (SVD) - Model Compression : Resnet18\n\n\n\n\n\nOct 24, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow Rank Approximation Implementation 2/4\n\n\n\n\n\n\nTech\n\n\n\nSingular Value Decomposition (SVD) - Model Compression : Resnet50\n\n\n\n\n\nOct 24, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow Rank Approximation Implementation 4/4\n\n\n\n\n\n\nTech\n\n\n\nSingular Value Decomposition (SVD) - Dimensionality Reduction\n\n\n\n\n\nOct 24, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow Rank Approximation Implementation 3/4\n\n\n\n\n\n\nTech\n\n\n\nSingular Value Decomposition (SVD) - Dimensionality Reduction\n\n\n\n\n\nOct 24, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nProgrammable Platform with ChatGPT API\n\n\n\n\n\n\nTech\n\n\nWeb\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAnthropic Projects vs. OpenAI GPTs: A Comparison for LLM-Based Web Service Development\n\n\n\n\n\n\nTech\n\n\nWeb\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nThe Impact of GPTs + Function-Calling on Web Development\n\n\n\n\n\n\nTech\n\n\nWeb\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain + LangGraph vs. OpenAI Swarm + GPTs + Function Calling: A Comparison for Web Service Development\n\n\n\n\n\n\nTech\n\n\nWeb\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts\n\n\n\n\n\n\nTech\n\n\nWeb\n\n\nLLM\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHack Your City: Empowering Smart Solutions with No Code Vision LLM\n\n\nEnabling citizens and businesses to create smart city solutions effortlessly\n\n\n\nUseCase\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation Implementation 2/3\n\n\n\n\n\n\nTech\n\n\n\n2. Hint-Based Distillation\n\n\n\n\n\nOct 10, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation Implementation 3/3\n\n\n\n\n\n\nTech\n\n\n\n3. Self-Distillation\n\n\n\n\n\nOct 10, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation Implementation 1/3\n\n\n\n\n\n\nTech\n\n\n\n1. Soft Label Distillation\n\n\n\n\n\nOct 10, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow-rank approximation 1/4\n\n\n\n\n\n\nTech\n\n\n\nOverview of Low-rank approximation - Part1\n\n\n\n\n\nOct 7, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow-rank approximation 2/4\n\n\n\n\n\n\nTech\n\n\n\nOverview of Low-rank approximation – Part2\n\n\n\n\n\nOct 7, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow-rank approximation 3/4\n\n\n\n\n\n\nTech\n\n\n\nOverview of Low-rank approximation – Part3\n\n\n\n\n\nOct 7, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nLow-rank approximation 4/4\n\n\n\n\n\n\nTech\n\n\n\nOverview of Low-rank approximation – Part4\n\n\n\n\n\nOct 7, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation\n\n\n\n\n\n\nTech\n\n\n\nOverview of Knowledge Distillation: Simplifying Deep Learning Models\n\n\n\n\n\nOct 1, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nDemistifying TinyMLaaS\n\n\n\n\n\n\nTech\n\n\nTinyMLaaS\n\n\n\nBrief overview of the technology behind NinjaLABO’s TinyML as-a-Service\n\n\n\n\n\nSep 27, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nLLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC\n\n\nBeyond 1984: Smart Cities Powered by Ethical AI and Citizen Empowerment\n\n\n\nUseCase\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\ntinyRuntime review\n\n\n\n\n\n\nTech\n\n\n\nReviewing the current status of our lightweight runtime\n\n\n\n\n\nSep 26, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nToDo list in Autumn 2024\n\n\n\n\n\n\nWoW\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Insights: Evolving tinyMLaaS with Flexible Pipelines and Distributed Execution\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for Software Engineer - AI Model Compression & DNN Runtime\n\n\n\n\n\n\nHiring\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Overview of DNN Model Compression Techniques\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Real-Time Face Identification System: A Comprehensive Guide\n\n\n\n\n\n\nTech\n\n\nUseCase\n\n\n\nExplore how to build a real-time face identification system using Ultralytics YOLOv8, a state-of-the-art object detection model known for its speed and accuracy. We’ll compare various tools, including managed services like AWS Rekognition and Azure Face API, to help you choose the best solution for your needs. Whether you’re a developer seeking full control over your models or looking for an easy-to-integrate, scalable solution, this guide will provide you with the knowledge and tools to create a powerful face identification system.\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying PyTorch Static Quantization\n\n\n\n\n\n\nTech\n\n\n\nA deep dive into how PyTorch performs inference with quantized models.\n\n\n\n\n\nAug 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service with Helsinki University summer course 2023\n\n\n\n\n\n\nHU\n\n\nTinyMLaaS\n\n\n\nThis is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\nAbout\n\n\nTeam\n\n\n\nThe team of experts in software development, research, and design is working to enable businesses to integrate AI & Machine Learning applications on a wide range of devices without relying on expensive networking & cloud computing services thanks to their solution, TinyML as a Service.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAI Compression as-a-Service MVP6 review\n\n\n\n\n\n\nNews\n\n\nMVP\n\n\n\nOur internal review of MVP6 usage\n\n\n\n\n\nAug 6, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Impulse Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nEdge Impulse - A Comprehensive Review of the Leading Edge AI Platform\n\n\n\n\n\nAug 5, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service Overview\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of NinjaLABO’s TinyML as-a-Service / AI compression as-a-Service \n\n\n\n\n\nJul 17, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nIREE review\n\n\n\n\n\n\nTech\n\n\nCompiler\n\n\n\nBrief overview of IREE\n\n\n\n\n\nJul 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nImagimob Studio Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nBrief overview of Imagimob Studio - A solution for Edge AI applications\n\n\n\n\n\nJul 8, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nUseCase\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we democratize machine learning on IoT devices?\n\n\n\n\n\n\nEricsson\n\n\n\nMaking tiny machine learning widely available on edge IoT devices could prove to be a major leap in smart sensing across industries. Below, we plot the technical milestones to making that happen as part of our ongoing research into TinyML as-a-service.\n\n\n\n\n\nAug 6, 2020\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge\n\n\n\n\n\n\nEricsson\n\n\n\nThe TinyML as-a-Service project at Ericsson Research sets out to address the challenges that today limit the potential of machine learning (ML) paradigms at the edge of the embedded IoT world. In this post, the second post in our TinyML series, we take a closer look at the technical and non-technical challenges on our journey to making that happen. Learn more below.\n\n\n\n\n\nAug 6, 2019\n\n\nHiroshi Doyu\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Blog"
    ]
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html",
    "href": "blogs/demistify_tinymlaas.html",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "TinyMLaaS is an end-to-end service that supports the entire lifecycle of a machine learning (ML) model deployment - from dataset selection to running inference on real devices. This system streamlines the process by combining both frontend and backend services into a unified platform known as NinjaLABO Studio. This blog post will guide you through how the system functions, covering essential components like pipelines, artifacts, GitHub automation, Docker integration, and more.\n\n\nOur research results are published via a Landing Page that features documentation, research blogs, and performance reports. The page helps users stay updated with the latest performance benchmarks and model optimizations.\n\nResearch Blogs: Published in formats such as .ipynb, .md, or .qmd using Quarto architecture.\nPerformance Updates: Automated updates on model performance are triggered through GitHub Actions.\nHTML Page Generation: GitHub Actions are also employed to publish performance metrics and blog posts as .html pages for easy access through the landing page.\n\n\n\n\nThe functionalities of TinyMLaaS are delivered through its Studio interface, where users interact with the system. The Studio is constructed from coordinating a backend API developed with FastAPI and frontend interface built with Streamlit.\nThe Studio allows users to manage projects and monitor their deployment pipelines. Some notable features of the Studio include:\n\nProject dashboard: The configurations of deployment pipelines are simplified into a singular dashboard. The user can easily spawns multiple runs with different models, datasets, and compression techniques under a project.\n\nResult comparison and visualization: After multiple pipelines (runs) are complete, the user can compare their inference accuracy, execution time, and model size. The user can also visualize such differences and trade-off between these metrics to select the most suitable compression techniques.\n\nUpload models & datasets: The user can upload their own models and datasets following our accepted format.\n\nExport inference image: The user can export artifacts from their pipeline including the compressed model file and inference image to run on their local device.\n\nEmail notifications: The user is notified about the pipeline status (success / error) via emails.\n\n\n\n\nProject Dashboard UI\n\n\nFor more information about the user experience, please check out the walkthrough tutorial!\n\n\nTinyMLaaS utilizes a system with high-degree integration with Hugging Face and Docker, with the main components being:\n\nUser: Entity representing the user’s requests through Studio.\n\nEndpoint: The API layer that handles requests and manages interactions with backend services.\n\nDatabase: Stores records related to models, datasets, runs, and their statuses.\n\nVolume: Dedicated storage for artifacts generated by Docker containers.\n\nHugging Face: Provides access to pre-trained models and datasets.\n\nDocker Client: Executes tasks inside containers, including compiling models, installing models to a specified device, and running inference.\n\n\n\n\nThe Studio operates via an API layer that coordinates user requests, Docker containers, Hugging Face datasets, and device installations to construct the deployment pipeline of a machine learning model on edge device. The pipeline steps are:\n\nCreate a Run: A new run is created for a project, starting the entire process.\nDownload or Extract Models and Datasets: The model is either fetched from Hugging Face or extracted from a user-provided file.\nCompile Model: The model is compiled inside a Docker container.\nInstall Model on Device: The compiled model is installed on the specified device.\nRun Inference: Inference is executed using the test dataset on the device.\n\n\n\n\n\n\n\n---\ntitle: Pipeline interactions\n---\nsequenceDiagram\n  actor User\n  participant Endpoint\n  participant Database\n  participant Volume\n  participant HuggingFace\n  participant DockerClient\n\n  note over User,Volume: New run\n  User-&gt;&gt;Endpoint: Create a new run\n  Endpoint-&gt;&gt;+Database: add record\n  Endpoint-&gt;&gt;+Volume: create new directory as run volume\n  Endpoint--&gt;&gt;User: 201 CREATED\n  \n  note over User,HuggingFace: Download Model & Dataset from Hugging Face\n  User-&gt;&gt;+Endpoint: Download model & dataset from HF\n  Endpoint--&gt;&gt;+HuggingFace: verify access to HF ID\n  HuggingFace--&gt;&gt;-Endpoint: confirm accessibility\n  Endpoint--&gt;&gt;HuggingFace: request downloading\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  HuggingFace-&gt;&gt;Volume: download\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,Volume: Upload customized Model & Dataset\n  User-&gt;&gt;+Endpoint: Upload model & dataset\n  Endpoint--&gt;&gt;Volume: upload temporary file & request extracting / copying to run directory\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  Volume-&gt;&gt;Volume: extracting .zip / copying model.pkl\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Compile model\n  User-&gt;&gt;+Endpoint: Compile model\n  Endpoint-&gt;&gt;Volume: update compression configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Install model on device\n  User-&gt;&gt;+Endpoint: Register device\n  Endpoint-&gt;&gt;Database: add records of device information\n  Endpoint--&gt;&gt;-User: 201 CREATED\n\n  User-&gt;&gt;+Endpoint: Install model on device\n  Endpoint-&gt;&gt;Volume: update runtime & device configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Run inference\n  User-&gt;&gt;+Endpoint: Run inference on device\n  Endpoint--&gt;&gt;+Volume: extract inference image name\n  Volume--&gt;&gt;-Endpoint: return image name (result of installation)\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n\n\n\n\n\n\n\n\n\n\nTinyMLaaS simplifies the entire machine learning model lifecycle, from research to deployment on edge devices. With seamless integration between Docker and Hugging Face, users can manage models, datasets, and pipelines through an intuitive Studio interface. By automating key tasks like model compilation, deployment, and performance monitoring, TinyMLaaS enables fast, efficient edge AI development, making it a powerful solution for professionals and researchers alike."
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#research-and-performance",
    "href": "blogs/demistify_tinymlaas.html#research-and-performance",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "Our research results are published via a Landing Page that features documentation, research blogs, and performance reports. The page helps users stay updated with the latest performance benchmarks and model optimizations.\n\nResearch Blogs: Published in formats such as .ipynb, .md, or .qmd using Quarto architecture.\nPerformance Updates: Automated updates on model performance are triggered through GitHub Actions.\nHTML Page Generation: GitHub Actions are also employed to publish performance metrics and blog posts as .html pages for easy access through the landing page."
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#user-experience-with-ninjalabo-studio",
    "href": "blogs/demistify_tinymlaas.html#user-experience-with-ninjalabo-studio",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "The functionalities of TinyMLaaS are delivered through its Studio interface, where users interact with the system. The Studio is constructed from coordinating a backend API developed with FastAPI and frontend interface built with Streamlit.\nThe Studio allows users to manage projects and monitor their deployment pipelines. Some notable features of the Studio include:\n\nProject dashboard: The configurations of deployment pipelines are simplified into a singular dashboard. The user can easily spawns multiple runs with different models, datasets, and compression techniques under a project.\n\nResult comparison and visualization: After multiple pipelines (runs) are complete, the user can compare their inference accuracy, execution time, and model size. The user can also visualize such differences and trade-off between these metrics to select the most suitable compression techniques.\n\nUpload models & datasets: The user can upload their own models and datasets following our accepted format.\n\nExport inference image: The user can export artifacts from their pipeline including the compressed model file and inference image to run on their local device.\n\nEmail notifications: The user is notified about the pipeline status (success / error) via emails.\n\n\n\n\nProject Dashboard UI\n\n\nFor more information about the user experience, please check out the walkthrough tutorial!\n\n\nTinyMLaaS utilizes a system with high-degree integration with Hugging Face and Docker, with the main components being:\n\nUser: Entity representing the user’s requests through Studio.\n\nEndpoint: The API layer that handles requests and manages interactions with backend services.\n\nDatabase: Stores records related to models, datasets, runs, and their statuses.\n\nVolume: Dedicated storage for artifacts generated by Docker containers.\n\nHugging Face: Provides access to pre-trained models and datasets.\n\nDocker Client: Executes tasks inside containers, including compiling models, installing models to a specified device, and running inference.\n\n\n\n\nThe Studio operates via an API layer that coordinates user requests, Docker containers, Hugging Face datasets, and device installations to construct the deployment pipeline of a machine learning model on edge device. The pipeline steps are:\n\nCreate a Run: A new run is created for a project, starting the entire process.\nDownload or Extract Models and Datasets: The model is either fetched from Hugging Face or extracted from a user-provided file.\nCompile Model: The model is compiled inside a Docker container.\nInstall Model on Device: The compiled model is installed on the specified device.\nRun Inference: Inference is executed using the test dataset on the device.\n\n\n\n\n\n\n\n---\ntitle: Pipeline interactions\n---\nsequenceDiagram\n  actor User\n  participant Endpoint\n  participant Database\n  participant Volume\n  participant HuggingFace\n  participant DockerClient\n\n  note over User,Volume: New run\n  User-&gt;&gt;Endpoint: Create a new run\n  Endpoint-&gt;&gt;+Database: add record\n  Endpoint-&gt;&gt;+Volume: create new directory as run volume\n  Endpoint--&gt;&gt;User: 201 CREATED\n  \n  note over User,HuggingFace: Download Model & Dataset from Hugging Face\n  User-&gt;&gt;+Endpoint: Download model & dataset from HF\n  Endpoint--&gt;&gt;+HuggingFace: verify access to HF ID\n  HuggingFace--&gt;&gt;-Endpoint: confirm accessibility\n  Endpoint--&gt;&gt;HuggingFace: request downloading\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  HuggingFace-&gt;&gt;Volume: download\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,Volume: Upload customized Model & Dataset\n  User-&gt;&gt;+Endpoint: Upload model & dataset\n  Endpoint--&gt;&gt;Volume: upload temporary file & request extracting / copying to run directory\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  Volume-&gt;&gt;Volume: extracting .zip / copying model.pkl\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Compile model\n  User-&gt;&gt;+Endpoint: Compile model\n  Endpoint-&gt;&gt;Volume: update compression configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Install model on device\n  User-&gt;&gt;+Endpoint: Register device\n  Endpoint-&gt;&gt;Database: add records of device information\n  Endpoint--&gt;&gt;-User: 201 CREATED\n\n  User-&gt;&gt;+Endpoint: Install model on device\n  Endpoint-&gt;&gt;Volume: update runtime & device configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Run inference\n  User-&gt;&gt;+Endpoint: Run inference on device\n  Endpoint--&gt;&gt;+Volume: extract inference image name\n  Volume--&gt;&gt;-Endpoint: return image name (result of installation)\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK"
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#conclusion",
    "href": "blogs/demistify_tinymlaas.html#conclusion",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "TinyMLaaS simplifies the entire machine learning model lifecycle, from research to deployment on edge devices. With seamless integration between Docker and Hugging Face, users can manage models, datasets, and pipelines through an intuitive Studio interface. By automating key tasks like model compilation, deployment, and performance monitoring, TinyMLaaS enables fast, efficient edge AI development, making it a powerful solution for professionals and researchers alike."
  },
  {
    "objectID": "blogs/howto_enable_jupyter_remotely.html",
    "href": "blogs/howto_enable_jupyter_remotely.html",
    "title": "How to set up Jupyter notebook on a remote VM",
    "section": "",
    "text": "ssh root@gpu-instance-ip\njupyter notebook --no-browser --port=8888 --allow-root\nssh -N -L localhost:7777:localhost:8888 root@gpu-instance-ip\nhttps://www.scaleway.com/en/docs/tutorials/setup-jupyter-notebook/"
  },
  {
    "objectID": "blogs/swarm.html",
    "href": "blogs/swarm.html",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "",
    "text": "Building an LLM-based web service can be a complex process, requiring you to balance scalability, flexibility, real-time data handling, and user interaction. But by using the right combination of tools like OpenAI’s Swarm, GPTs, Function Calling, and Anthropic’s Artifacts, you can make this process much more efficient and manageable.\nIn this post, we’ll walk you through how to create a scalable and dynamic LLM web service by integrating these technologies and show you how to build a powerful solution with minimal effort."
  },
  {
    "objectID": "blogs/swarm.html#example",
    "href": "blogs/swarm.html#example",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "Example:",
    "text": "Example:\n\nA customer support GPT can answer user queries, retrieve FAQs, or handle common troubleshooting requests.\nA content generation GPT can be set up to create blog posts, marketing materials, or other forms of content based on user inputs.\n\nWhy GPTs? - No-code setup: You can easily configure GPTs to handle specific tasks without writing extensive code. - Customizable: Each GPT can be tailored to a different business need or function.\nBy using GPTs, you can quickly develop AI models that are perfectly suited to the needs of your web service."
  },
  {
    "objectID": "blogs/swarm.html#example-1",
    "href": "blogs/swarm.html#example-1",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "Example:",
    "text": "Example:\n\nA user asks for the latest stock price or current weather. The GPT can call an external API, fetch the data, and return the response to the user in real-time.\nA user needs to schedule a meeting or retrieve information from an internal database. Function Calling allows the GPT to integrate with your calendar API or database.\n\nWhy Function Calling? - Real-time capabilities: GPTs can retrieve and process live data, making your web service more dynamic and responsive. - External integrations: Easily connect your GPTs to third-party APIs for deeper functionality."
  },
  {
    "objectID": "blogs/swarm.html#example-2",
    "href": "blogs/swarm.html#example-2",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "Example:",
    "text": "Example:\n\nWhen your web service has multiple users submitting requests (e.g., data analysis, customer support queries, real-time data fetches), Swarm ensures that tasks are distributed among different GPTs.\nSwarm automatically manages task flow, making sure that no single GPT is overwhelmed, and ensures the service remains fast and efficient under heavy load.\n\nWhy Swarm? - Efficient orchestration: Swarm dynamically assigns tasks to GPT agents based on current workloads, ensuring that tasks are processed in parallel for greater efficiency. - Scalability: As your web service grows, Swarm allows you to add more GPT agents, effortlessly scaling the service without performance bottlenecks."
  },
  {
    "objectID": "blogs/swarm.html#example-3",
    "href": "blogs/swarm.html#example-3",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "Example:",
    "text": "Example:\n\nUsers can upload a PDF document or image that needs to be summarized, analyzed, or processed. Artifacts manage these files, and GPTs process the data and return insights or summaries directly to the user.\nArtifacts also act as a bridge for managing structured and unstructured data, enabling GPTs to retrieve or store user-uploaded data for ongoing workflows.\n\nWhy Artifacts? - Frontend data management: Artifacts provide a seamless way for users to interact with your web service by uploading and managing files or documents. - File processing: GPTs can handle uploaded data, analyze it, and provide insights, adding a powerful interactive element to your web service."
  },
  {
    "objectID": "blogs/swarm.html#example-4",
    "href": "blogs/swarm.html#example-4",
    "title": "How to Build an LLM Web Service Effortlessly with OpenAI Swarm, GPTs, Function Calling, and Anthropic Artifacts",
    "section": "Example:",
    "text": "Example:\n\nWhen a user uploads a document, the Router GPT directs it to the summarization GPT.\nIf the user asks for real-time information, the Router GPT automatically triggers Function Calling to fetch data from an external API.\n\nWhy Router GPT? - Smart task routing: It optimizes your service by ensuring each task goes to the most appropriate GPT, enhancing performance and user experience."
  },
  {
    "objectID": "blogs/Low_Rank_Approximation-Part1.html",
    "href": "blogs/Low_Rank_Approximation-Part1.html",
    "title": "Low-rank approximation 1/4",
    "section": "",
    "text": "Low-rank approximation is a technique used to simplify complex data by approximating it with a smaller or “lower-rank” version while keeping the important information. It’s like compressing data, but without losing the essential patterns or structure.\n\n\nMatrix Representation: In math, data can often be represented as a matrix (a grid of numbers). For example, if you have a large dataset, it might be represented by a matrix with many rows and columns. The “rank” of this matrix refers to the number of linearly independent rows or columns—basically, how complex the data is.\nLow-Rank Approximation: Instead of using the entire matrix, low-rank approximation tries to find a smaller matrix that captures the same important features of the original one. You can think of this smaller matrix as a “compressed” version that focuses on the key relationships in the data, ignoring the less important details (like noise or redundancy).\n\n\n\n\nwe have dsd\n\nFaster Computation: Instead of directly multiplying large matrices, low-rank approximation allows us to work with smaller matrices, which speeds up computation. The time complexity can reduce from \\(O(nm)\\) to \\(O((n+m)r)\\), where r is the rank of the approximation.\nEfficiency: Large datasets or images require a lot of memory and computational power. By using low-rank approximation, you can reduce the amount of data you need to work with while still keeping the important information.\nData Compression: It helps compress the data, making it easier to store and transmit.\nNoise Reduction: In real-world data, there’s often noise (unnecessary or irrelevant information). Low-rank approximation can help filter out this noise and keep the meaningful structure.\n\n\n\n\n\nLow-rank approximation techniques are methods used to approximate a large, complex matrix with a smaller matrix that has lower rank, meaning it captures the most important information while reducing redundancy and noise. Here are the most common techniques used for low-rank approximation, along with their advantages and disadvantages:\n\n\nSingular Value Decomposition is one of the most commonly used techniques for low-rank approximation. It decomposes a matrix \\(A\\) into three components:\n\\[ A = U \\Sigma V^T \\]\nWhere:\n\n\\(U\\) and \\(V\\) are orthogonal matrices,\n\\(\\Sigma\\) is a diagonal matrix containing the singular values (which represent the importance of different components).\n\nYou can then select the top \\(k\\) singular values and their corresponding vectors to create a low-rank approximation.\n\n\n\nOptimal Approximation: SVD provides the best low-rank approximation in terms of minimizing the error (Frobenius norm or 2-norm).\nWidely Used: SVD is robust and well-understood, making it a standard technique in fields like machine learning, data compression, and image processing.\nNoise Filtering: SVD automatically filters out the “noise” (small singular values) when approximating, keeping the dominant trends.\n\n\n\n\n\nComputationally Expensive: Computing SVD for large matrices can be slow and requires a lot of memory, especially for big data applications.\nGlobal Information: SVD captures global structure but might miss fine, local patterns in data (like small clusters in a dataset).\n\n\n\n\n\n\nPCA is essentially a variation of SVD applied to the covariance matrix. It identifies the directions (principal components) where the variance in the data is the largest, and projects the data onto those directions. This allows you to represent the data using fewer dimensions while retaining the most important patterns.\n\n\n\nDimensionality Reduction: PCA is ideal for reducing the dimensionality of data while keeping the key patterns, which makes it easier to visualize or process.\nEfficient for Small Datasets: PCA works well on relatively small datasets and is fast for low-rank approximations in these cases.\nNoise Reduction: Like SVD, it helps in removing noise and focusing on the key features.\n\n\n\n\n\nLinear Assumptions: PCA assumes that the important relationships in the data are linear, which may not be the case for more complex, non-linear datasets.\nSensitive to Scaling: PCA is sensitive to how the data is scaled. You need to normalize or standardize the data for the best results.\nLoss of Interpretability: The new principal components are linear combinations of the original features, which can make the results hard to interpret.\n\n\n\n\n\n\nNMF is a technique used for matrices where all values are non-negative. It decomposes the matrix into two smaller non-negative matrices \\(W\\) and \\(H\\):\n\\[ A \\approx WH \\]\nThe idea is to represent the original matrix as a combination of “basis vectors” in \\(W\\) and “coefficients” in \\(H\\), both constrained to be non-negative, which can be helpful when dealing with data that naturally contains non-negative values (like images or word counts).\n\n\n\nInterpretability: Since both the basis vectors and coefficients are non-negative, the results are often easier to interpret. For example, in image processing, NMF will break down an image into parts that represent things like edges, textures, etc.\nSparsity: NMF often produces sparse solutions, meaning that many of the elements in \\(W\\) and \\(H\\) are zero, which can make the results more efficient and interpretable.\n\n\n\n\n\nNot Globally Optimal: Unlike SVD, NMF does not guarantee an optimal approximation. It may get stuck in local minima during the optimization process.\nComputational Complexity: While it can be faster than SVD in some cases, NMF is still computationally expensive for large datasets, especially due to its iterative optimization process.\n\n\n\n\n\n\nQR decomposition breaks down a matrix \\(A\\) into two matrices \\(Q\\) (orthogonal matrix) and \\(R\\) (upper triangular matrix). For low-rank approximation, the matrix can be reduced to its top \\(k\\) components by selecting a subset of columns from \\(Q\\) and rows from \\(R\\).\n\n\n\nEfficient for Solving Systems: QR decomposition is particularly efficient for solving linear systems and least-squares problems, especially when combined with column-pivoting.\nNumerical Stability: It is often numerically more stable than other techniques like Gaussian elimination.\n\n\n\n\n\nNot the Best for Low-Rank Approximation: QR decomposition is not specifically designed for low-rank approximation, so it doesn’t minimize approximation error as effectively as SVD.\nInterpretation: While it’s fast, QR decomposition is less interpretable compared to SVD or NMF, which can give a clearer sense of what the reduced components represent.\n\n\n\n\n\n\nRandomized algorithms approximate low-rank decompositions by using random projections to quickly identify the most important components of the matrix. The idea is to approximate the original matrix with random sampling and then refine the approximation using techniques like SVD.\n\n\n\nSpeed: Randomized algorithms are much faster than traditional methods like SVD, making them ideal for very large datasets (like big data or streaming data).\nScalability: Because they are faster, randomized algorithms are more scalable to huge datasets.\n\n\n\n\n\nApproximation Error: Randomized methods introduce some additional approximation error compared to traditional techniques like SVD. The trade-off is between speed and accuracy.\nUncertainty: The randomization aspect can lead to different results each time the algorithm runs, making the results somewhat non-deterministic.\n\n\n\n\n\n\nCUR decomposition is a matrix approximation technique that selects actual rows and columns from the original matrix to form two low-rank matrices \\(C\\) (columns) and \\(R\\) (rows). The matrix \\(U\\) is constructed by relating the selected rows and columns.\n\n\n\nInterpretability: Since CUR selects actual rows and columns from the original matrix, the results are more interpretable than SVD, which gives linear combinations.\nSparsity: CUR often results in sparse representations that can be computationally efficient and easier to store.\n\n\n\n\n\nLess Accurate: It tends to be less accurate than SVD for minimizing approximation error.\nComplexity: While it has some advantages in specific cases, CUR is more complex to implement and may not work well for all types of data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nAdvantages\nDisadvantages\n\n\n\n\nSVD\nBest approximation, noise filtering\nComputationally expensive\n\n\nPCA\nDimensionality reduction, fast for small data\nAssumes linearity, sensitive to scaling\n\n\nNMF\nInterpretability, good for non-negative data\nCan get stuck in local minima, computational cost\n\n\nQR Decomposition\nEfficient for solving systems\nNot ideal for low-rank approximations\n\n\nRandomized\nFast, scalable\nIntroduces approximation error\n\n\nCUR\nInterpretability, actual rows and columns\nLess accurate than SVD, complex\n\n\n\nIn conclusion, the choice of low-rank approximation technique depends on the specific problem you’re solving. If you need the most accurate approximation and don’t mind computational cost, SVD or PCA are good choices. For non-negative data or interpretability, NMF and CUR are useful, while randomized methods excel when speed is critical.\nAmong the low-rank approximation techniques discussed, Randomized Low-Rank Approximation is typically the fastest method, especially for very large datasets.\n\n\n\n\n\nRandom Sampling: Instead of analyzing the entire matrix directly (as in SVD or PCA), randomized methods first perform a random sampling or projection of the data to a lower-dimensional space. This drastically reduces the computational workload right from the start.\nDimensionality Reduction Early: By quickly approximating the matrix structure through random projections, the method avoids performing computations on the full matrix, which saves time.\nScalability: It scales efficiently with large datasets, making it suitable for scenarios where traditional methods like SVD would take too long (for example, in big data applications).\n\n\n\n\n\nSVD/PCA: These are slower because they require calculating the full decomposition of the matrix. SVD, in particular, is computationally expensive, with a time complexity of \\(O(mn^2)\\) for an \\(m \\times n\\) matrix.\nNMF: NMF is slower than randomized algorithms because it involves iterative optimization that can take time to converge.\nQR Decomposition: QR can be faster than SVD but isn’t primarily designed for low-rank approximation, and for large datasets, randomized methods will still outperform QR.\nCUR: CUR is not optimized for speed, as it involves selecting actual rows and columns and then performing further matrix operations, which can be slow for large matrices.\n\n\n\n\n\n\nRandomized Low-Rank Approximation is fast but may introduce some approximation error compared to more precise methods like SVD. The trade-off here is between speed and accuracy.\nHowever, for very large datasets or situations where speed is critical (like real-time processing or working with big data), randomized methods strike a good balance between performance and quality.\n\nIn summary, randomized low-rank approximation is generally the fastest technique, especially when working with large-scale matrices or datasets."
  },
  {
    "objectID": "blogs/Low_Rank_Approximation-Part1.html#low-rank-approximation-techniques",
    "href": "blogs/Low_Rank_Approximation-Part1.html#low-rank-approximation-techniques",
    "title": "Low-rank approximation 1/4",
    "section": "",
    "text": "Low-rank approximation techniques are methods used to approximate a large, complex matrix with a smaller matrix that has lower rank, meaning it captures the most important information while reducing redundancy and noise. Here are the most common techniques used for low-rank approximation, along with their advantages and disadvantages:\n\n\nSingular Value Decomposition is one of the most commonly used techniques for low-rank approximation. It decomposes a matrix \\(A\\) into three components:\n\\[ A = U \\Sigma V^T \\]\nWhere:\n\n\\(U\\) and \\(V\\) are orthogonal matrices,\n\\(\\Sigma\\) is a diagonal matrix containing the singular values (which represent the importance of different components).\n\nYou can then select the top \\(k\\) singular values and their corresponding vectors to create a low-rank approximation.\n\n\n\nOptimal Approximation: SVD provides the best low-rank approximation in terms of minimizing the error (Frobenius norm or 2-norm).\nWidely Used: SVD is robust and well-understood, making it a standard technique in fields like machine learning, data compression, and image processing.\nNoise Filtering: SVD automatically filters out the “noise” (small singular values) when approximating, keeping the dominant trends.\n\n\n\n\n\nComputationally Expensive: Computing SVD for large matrices can be slow and requires a lot of memory, especially for big data applications.\nGlobal Information: SVD captures global structure but might miss fine, local patterns in data (like small clusters in a dataset).\n\n\n\n\n\n\nPCA is essentially a variation of SVD applied to the covariance matrix. It identifies the directions (principal components) where the variance in the data is the largest, and projects the data onto those directions. This allows you to represent the data using fewer dimensions while retaining the most important patterns.\n\n\n\nDimensionality Reduction: PCA is ideal for reducing the dimensionality of data while keeping the key patterns, which makes it easier to visualize or process.\nEfficient for Small Datasets: PCA works well on relatively small datasets and is fast for low-rank approximations in these cases.\nNoise Reduction: Like SVD, it helps in removing noise and focusing on the key features.\n\n\n\n\n\nLinear Assumptions: PCA assumes that the important relationships in the data are linear, which may not be the case for more complex, non-linear datasets.\nSensitive to Scaling: PCA is sensitive to how the data is scaled. You need to normalize or standardize the data for the best results.\nLoss of Interpretability: The new principal components are linear combinations of the original features, which can make the results hard to interpret.\n\n\n\n\n\n\nNMF is a technique used for matrices where all values are non-negative. It decomposes the matrix into two smaller non-negative matrices \\(W\\) and \\(H\\):\n\\[ A \\approx WH \\]\nThe idea is to represent the original matrix as a combination of “basis vectors” in \\(W\\) and “coefficients” in \\(H\\), both constrained to be non-negative, which can be helpful when dealing with data that naturally contains non-negative values (like images or word counts).\n\n\n\nInterpretability: Since both the basis vectors and coefficients are non-negative, the results are often easier to interpret. For example, in image processing, NMF will break down an image into parts that represent things like edges, textures, etc.\nSparsity: NMF often produces sparse solutions, meaning that many of the elements in \\(W\\) and \\(H\\) are zero, which can make the results more efficient and interpretable.\n\n\n\n\n\nNot Globally Optimal: Unlike SVD, NMF does not guarantee an optimal approximation. It may get stuck in local minima during the optimization process.\nComputational Complexity: While it can be faster than SVD in some cases, NMF is still computationally expensive for large datasets, especially due to its iterative optimization process.\n\n\n\n\n\n\nQR decomposition breaks down a matrix \\(A\\) into two matrices \\(Q\\) (orthogonal matrix) and \\(R\\) (upper triangular matrix). For low-rank approximation, the matrix can be reduced to its top \\(k\\) components by selecting a subset of columns from \\(Q\\) and rows from \\(R\\).\n\n\n\nEfficient for Solving Systems: QR decomposition is particularly efficient for solving linear systems and least-squares problems, especially when combined with column-pivoting.\nNumerical Stability: It is often numerically more stable than other techniques like Gaussian elimination.\n\n\n\n\n\nNot the Best for Low-Rank Approximation: QR decomposition is not specifically designed for low-rank approximation, so it doesn’t minimize approximation error as effectively as SVD.\nInterpretation: While it’s fast, QR decomposition is less interpretable compared to SVD or NMF, which can give a clearer sense of what the reduced components represent.\n\n\n\n\n\n\nRandomized algorithms approximate low-rank decompositions by using random projections to quickly identify the most important components of the matrix. The idea is to approximate the original matrix with random sampling and then refine the approximation using techniques like SVD.\n\n\n\nSpeed: Randomized algorithms are much faster than traditional methods like SVD, making them ideal for very large datasets (like big data or streaming data).\nScalability: Because they are faster, randomized algorithms are more scalable to huge datasets.\n\n\n\n\n\nApproximation Error: Randomized methods introduce some additional approximation error compared to traditional techniques like SVD. The trade-off is between speed and accuracy.\nUncertainty: The randomization aspect can lead to different results each time the algorithm runs, making the results somewhat non-deterministic.\n\n\n\n\n\n\nCUR decomposition is a matrix approximation technique that selects actual rows and columns from the original matrix to form two low-rank matrices \\(C\\) (columns) and \\(R\\) (rows). The matrix \\(U\\) is constructed by relating the selected rows and columns.\n\n\n\nInterpretability: Since CUR selects actual rows and columns from the original matrix, the results are more interpretable than SVD, which gives linear combinations.\nSparsity: CUR often results in sparse representations that can be computationally efficient and easier to store.\n\n\n\n\n\nLess Accurate: It tends to be less accurate than SVD for minimizing approximation error.\nComplexity: While it has some advantages in specific cases, CUR is more complex to implement and may not work well for all types of data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nAdvantages\nDisadvantages\n\n\n\n\nSVD\nBest approximation, noise filtering\nComputationally expensive\n\n\nPCA\nDimensionality reduction, fast for small data\nAssumes linearity, sensitive to scaling\n\n\nNMF\nInterpretability, good for non-negative data\nCan get stuck in local minima, computational cost\n\n\nQR Decomposition\nEfficient for solving systems\nNot ideal for low-rank approximations\n\n\nRandomized\nFast, scalable\nIntroduces approximation error\n\n\nCUR\nInterpretability, actual rows and columns\nLess accurate than SVD, complex\n\n\n\nIn conclusion, the choice of low-rank approximation technique depends on the specific problem you’re solving. If you need the most accurate approximation and don’t mind computational cost, SVD or PCA are good choices. For non-negative data or interpretability, NMF and CUR are useful, while randomized methods excel when speed is critical.\nAmong the low-rank approximation techniques discussed, Randomized Low-Rank Approximation is typically the fastest method, especially for very large datasets.\n\n\n\n\n\nRandom Sampling: Instead of analyzing the entire matrix directly (as in SVD or PCA), randomized methods first perform a random sampling or projection of the data to a lower-dimensional space. This drastically reduces the computational workload right from the start.\nDimensionality Reduction Early: By quickly approximating the matrix structure through random projections, the method avoids performing computations on the full matrix, which saves time.\nScalability: It scales efficiently with large datasets, making it suitable for scenarios where traditional methods like SVD would take too long (for example, in big data applications).\n\n\n\n\n\nSVD/PCA: These are slower because they require calculating the full decomposition of the matrix. SVD, in particular, is computationally expensive, with a time complexity of \\(O(mn^2)\\) for an \\(m \\times n\\) matrix.\nNMF: NMF is slower than randomized algorithms because it involves iterative optimization that can take time to converge.\nQR Decomposition: QR can be faster than SVD but isn’t primarily designed for low-rank approximation, and for large datasets, randomized methods will still outperform QR.\nCUR: CUR is not optimized for speed, as it involves selecting actual rows and columns and then performing further matrix operations, which can be slow for large matrices.\n\n\n\n\n\n\nRandomized Low-Rank Approximation is fast but may introduce some approximation error compared to more precise methods like SVD. The trade-off here is between speed and accuracy.\nHowever, for very large datasets or situations where speed is critical (like real-time processing or working with big data), randomized methods strike a good balance between performance and quality.\n\nIn summary, randomized low-rank approximation is generally the fastest technique, especially when working with large-scale matrices or datasets."
  },
  {
    "objectID": "blogs/Low_Rank_Approximation-Part4.html",
    "href": "blogs/Low_Rank_Approximation-Part4.html",
    "title": "Low-rank approximation 4/4",
    "section": "",
    "text": "Low-Rank Approximation of Attention Mechanisms:\n\nWhat is Attention Mechanism?\nThe attention mechanism is a concept from machine learning, particularly in natural language processing and computer vision. It allows models (like transformers) to focus on different parts of the input data when making predictions or generating outputs. For example, when translating a sentence, attention helps the model focus on the most relevant words in the source sentence at each step of the translation.\nThe standard attention mechanism, while powerful, can be computationally expensive. This is because it involves creating a large attention matrix for every input, where the size of this matrix grows with the number of input tokens. For a sequence of length \\(N\\), the attention matrix is of size \\(N \\times N\\). This can lead to high memory usage and slow processing times, especially with long sequences.\n\n\nKey Concepts of Attention Mechanisms\n\nFocus on Relevant Information:\n\nImagine reading a book; you might focus on certain sentences or words that are more important to understand the context. Attention mechanisms do the same for models.\nFor example, when translating a sentence, the model uses attention to focus on the most relevant words from the source sentence while generating each word of the translation.\n\nCalculation:\n\nIn simple terms, attention computes a weighted sum of the input features, where the weights determine how much focus to give to each feature.\nThis is often done using three key components:\n\nQuery: Represents what we are currently focusing on.\nKey: Represents the information we have.\nValue: The actual information corresponding to the keys.\n\nThe attention score is computed by taking the dot product of the query with all the keys, followed by applying a softmax function to obtain weights. These weights are then used to compute a weighted sum of the values.\n\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n\\]\nWhere:\n\n\\(Q\\): Query matrix\n\\(K\\): Key matrix\n\\(V\\): Value matrix\n\\(d_k\\): Dimension of the keys, used for scaling.\n\nApplications:\n\nAttention mechanisms are widely used in models like Transformers, which have revolutionized NLP tasks such as translation, summarization, and question answering.\n\n\n\n\nHow Low-Rank Approximation Works in Attention Mechanisms\n\nMatrix Decomposition:\n\nInstead of computing the full attention matrix, we can decompose it into two smaller matrices. This means we approximate the large matrix with a product of two smaller matrices, which captures most of the information but is much cheaper to compute.\n\nFor example, given a matrix \\(A\\), we can approximate it as: \\[\nA \\approx U \\times V\n\\] where \\(U\\) and \\(V\\) are smaller matrices.\nEfficiency:\n\nBy reducing the size of the matrices, the computations needed to process the data become significantly faster and require less memory.\nThis allows models to handle longer sequences without running into performance issues.\n\nPreserving Information:\n\nThe key idea is to keep the most important features of the attention mechanism. By using low-rank approximation, we focus on the dominant patterns in the data, ensuring that the model still performs well.\n\n\n\n\nBenefits of Using Low-Rank Approximation in Attention\n\nSpeed: Faster training and inference times, as calculations involve smaller matrices.\nMemory Efficiency: Reduced memory usage, allowing models to work with longer sequences or larger batch sizes.\nMaintain Performance: Often retains good accuracy even with reduced computational costs.\n\nSure! Let’s break down kernel methods and attention mechanisms in a simple and understandable way.\n\n\nWhat are Kernel Methods?\nKernel methods are a class of algorithms used in machine learning that can operate in high-dimensional spaces without explicitly transforming the data into those dimensions. They are particularly useful in support vector machines (SVM) and other algorithms that rely on measuring the similarity between data points.\n\nKey Concepts of Kernel Methods\n\nFeature Space:\n\nMany machine learning algorithms work better in high-dimensional spaces. However, directly transforming data into high dimensions can be computationally expensive.\nKernel methods enable us to operate in this high-dimensional space using a trick called the kernel trick.\n\nKernel Trick:\n\nInstead of transforming the input data \\(X\\) into a higher-dimensional feature space explicitly, kernel methods use a kernel function \\(K\\) that computes the inner product of the data points in this high-dimensional space.\nFor example, a common kernel is the Gaussian (RBF) kernel, defined as:\n\\[\nK(x_i, x_j) = \\exp\\left(-\\frac{||x_i - x_j||^2}{2\\sigma^2}\\right)\n\\]\nThis function calculates the similarity between two data points \\(x_i\\) and \\(x_j\\) without needing to compute their coordinates in the higher-dimensional space.\n\nApplications:\n\nKernel methods are used for classification, regression, and clustering tasks. They are particularly effective for problems where the relationship between features is non-linear.\n\n\n\n\n\nConnecting Kernel Methods and Attention Mechanisms\nWhile kernel methods and attention mechanisms serve different purposes, there are some interesting connections between them:\n\nSimilarity Measurement:\n\nBoth approaches involve measuring the similarity between data points. Kernel methods explicitly compute similarities through kernel functions, while attention mechanisms compute weighted similarities dynamically based on the query, keys, and values.\n\nHigh-Dimensional Spaces:\n\nKernel methods implicitly work in high-dimensional feature spaces, whereas attention mechanisms effectively operate in a dynamic context, allowing the model to learn what features are most relevant based on the task at hand.\n\nFlexible Representations:\n\nAttention mechanisms can be viewed as a flexible way to represent relationships in data, similar to how kernel methods represent data in high-dimensional spaces. Both methods enhance model performance by focusing on relevant aspects of the data.\n\n\n\n\nConclusion\n\nKernel methods provide a way to work with high-dimensional data using similarity measurements without explicitly transforming the data, making them efficient and powerful for various machine learning tasks.\nAttention mechanisms allow models to dynamically focus on important parts of the input data, improving their ability to handle tasks in natural language processing and beyond.\n\nDefining the kernel function is an essential aspect of understanding kernel methods in machine learning. Let’s break it down into clear components, explaining what a kernel function is, its properties, and some common examples.\n\n\nWhat is a Kernel Function?\nA kernel function is a mathematical function that computes the similarity between two data points in a potentially high-dimensional feature space without explicitly mapping the data into that space. This allows algorithms to operate efficiently even in complex spaces.\nThe kernel function can be thought of as a measure of similarity between two input vectors,\\(x_i\\) and\\(x_j\\).\n\n\nMathematical Definition\nFormally, a kernel function\\(K\\) takes two input vectors\\(x_i\\) and\\(x_j\\) and produces a scalar value that represents their similarity:\n\\[\nK(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle\n\\]\nWhere:\n-\\(\\phi\\) is the mapping function that transforms the input vectors into a higher-dimensional space.\n-\\(\\langle \\cdot, \\cdot \\rangle\\) denotes the inner product in that space.\nThe beauty of kernel functions is that you don’t need to know\\(\\phi\\) explicitly. Instead, you can directly compute\\(K(x_i, x_j)\\) using the kernel function, which simplifies computations significantly.\n\n\nProperties of Kernel Functions\n\nSymmetry: \\[\nK(x_i, x_j) = K(x_j, x_i)\n\\] The similarity between\\(x_i\\) and\\(x_j\\) is the same regardless of the order of the inputs.\nPositive Semi-Definiteness: For any set of points\\(x_1, x_2, \\ldots, x_n\\) and any coefficients\\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_n\\), the following holds: \\[\n\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j K(x_i, x_j) \\geq 0\n\\] This property ensures that the kernel represents a valid inner product in some feature space.\n\nRandom features are a technique used to approximate kernel methods in machine learning, particularly useful when dealing with large datasets and high-dimensional feature spaces. They allow models to leverage the advantages of kernel methods while improving computational efficiency. Let’s break this down step by step.\n\n\nWhat are Random Features?\nRandom features involve using a randomized approach to create new features from the original input data. Instead of computing the kernel function directly, random features approximate the kernel mapping, enabling the application of linear models in an implicit high-dimensional space.\n\n\nWhy Use Random Features?\n\nScalability: Directly using kernel methods can be computationally expensive, especially for large datasets. Random features provide a way to scale kernel methods to large datasets efficiently.\nSpeed: By transforming the data into a lower-dimensional space using random features, models can be trained and evaluated much faster.\nFlexibility: Random features can approximate various types of kernels, making them versatile for different applications.\n\n\n\nHow Do Random Features Work?\n\n1. Kernel Approximation\nThe idea behind random features is based on the kernel trick. The kernel function\\(K(x_i, x_j)\\) can be approximated using random projections. The key concept is that you can express the kernel function as an inner product in a new feature space.\nFor example, consider a kernel function\\(K(x_i, x_j)\\) that corresponds to some mapping\\(\\phi(x)\\):\n\\[\nK(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle\n\\]\nThe goal is to find a random mapping\\(\\psi\\) such that:\n\\[\n\\psi(x) \\approx \\phi(x)\n\\]\n\n\n2. Random Feature Generation\nTo create random features, you typically follow these steps:\n\nRandom Projections:\n\nUse a random matrix to project the original data into a higher-dimensional space.\nFor example, if you want to approximate the Gaussian kernel, you can generate random vectors from a Gaussian distribution.\n\nFeature Mapping:\n\nMap the input data\\(x\\) using random projections. For the Gaussian kernel, you could compute the following:\n\n\\[\n\\phi(x) = \\sqrt{\\frac{2}{D}} \\left[\\cos(w_1^T x + b_1), \\cos(w_2^T x + b_2), \\ldots, \\cos(w_D^T x + b_D)\\right]\n\\]\nWhere: -\\(w_d\\) is a random vector (typically sampled from a Gaussian distribution). -\\(b_d\\) is a random bias (often uniformly sampled from\\([0, 2\\pi]\\)). -\\(D\\) is the number of random features.\nApproximate Kernel:\n\nThe kernel function can be approximated using these random features, allowing for efficient computations.\n\n\n\n\n\nAdvantages of Random Features\n\nReduced Complexity: They reduce the computational burden associated with calculating kernel matrices directly.\nLinear Models: They enable the use of linear models to capture complex relationships in the data through the randomized feature space.\nFlexible: They can be adapted to various kernel functions, making them versatile for different applications.\n\n\n\nDisadvantages of Random Features\n\nApproximation: Since random features are an approximation, the quality of the approximation may vary depending on the number of features\\(D\\) used.\nStochastic Nature: The randomness in generating features can lead to variability in model performance. It’s important to tune the number of random features and consider methods like averaging to improve results.\n\n\n\nApproximation of Kernel Methods Using Random Features\nThe approximation of kernel methods using random features is a powerful technique that allows us to leverage the benefits of kernel methods while improving computational efficiency. This approach is particularly useful in large-scale machine learning tasks where direct computation of kernel functions can be expensive.\n\n\nHow Does Random Feature Approximation Work?\nThe random feature approximation can be broken down into several steps:\n\n1. Kernel Function Representation\nAssume we have a kernel function\\(K(x_i, x_j)\\) that we want to approximate. For instance, for the Gaussian kernel, the kernel function is defined as:\n\\[\nK(x_i, x_j) = \\exp\\left(-\\frac{||x_i - x_j||^2}{2\\sigma^2}\\right)\n\\]\nThis kernel function can be interpreted as an inner product in an infinite-dimensional space.\n\n\n2. Random Feature Mapping\nTo approximate the kernel function, we can use random features based on the idea of Fourier feature mapping. The basic idea is to approximate the kernel function as follows:\n\\[\nK(x_i, x_j) \\approx \\phi(x_i)^T \\phi(x_j)\n\\]\nWhere\\(\\phi(x)\\) is a mapping that transforms the input data into a new feature space. For the Gaussian kernel, we can derive a finite-dimensional approximation using random features:\n\nGenerate Random Weights: Create random weights\\(w_d\\) sampled from a Gaussian distribution (e.g.,\\(\\mathcal{N}(0, 1)\\)).\nGenerate Random Biases: Create random biases\\(b_d\\) uniformly sampled from\\([0, 2\\pi]\\).\nConstruct the Feature Mapping: The feature mapping\\(\\phi(x)\\) can be defined as:\n\\[\n\\phi(x) = \\sqrt{\\frac{2}{D}} \\left[ \\cos(w_1^T x + b_1), \\cos(w_2^T x + b_2), \\ldots, \\cos(w_D^T x + b_D) \\right]\n\\]\nHere,\\(D\\) is the number of random features we want to generate.\n\n\n\n3. Approximate the Kernel\nUsing the random features, we can now approximate the kernel function as follows:\n\\[\nK(x_i, x_j) \\approx \\frac{1}{D} \\sum_{d=1}^{D} \\cos(w_d^T x_i + b_d) \\cos(w_d^T x_j + b_d)\n\\]\nThis expression gives us an efficient way to compute an approximation of the kernel using the generated random features.\n\n\n\nAdvantages of Using Random Features\n\nEfficiency: Significantly reduces the computational complexity associated with kernel methods, making them feasible for large datasets.\nScalability: Random features allow models to scale better, as you can adjust the number of features\\(D\\) based on computational resources.\nFlexibility: They can approximate various kernel functions, making them versatile for different tasks.\n\n\n\nDisadvantages\n\nApproximation Quality: The quality of the approximation can depend on the number of random features\\(D\\). A larger\\(D\\) generally leads to better approximations but at the cost of increased computation.\nVariability: The stochastic nature of random feature generation can introduce variability in model performance. Multiple runs may yield different results, so averaging over several trials can be beneficial.\n\nApproximating attention mechanisms using random features is a promising approach to make attention computations more efficient, especially in scenarios where the input sequences are long. Traditional attention mechanisms, such as those used in the Transformer model, can become computationally expensive due to their quadratic complexity with respect to the sequence length. Using random features can help mitigate this issue by approximating the attention scores while maintaining the essential characteristics of the attention mechanism.\n\n\nApproximation Using Random Features\nTo approximate the attention mechanism using random features, we can follow a similar idea as approximating kernel methods. The key is to approximate the attention scores in a way that reduces computational complexity.\n\nSteps for Random Feature Approximation of Attention\n\nRandom Projection:\n\nInstead of computing the full attention matrix using dot products between\\(Q\\) and\\(K\\), we can use random projections to reduce the dimensionality and computational cost.\nThe main idea is to project the query and key representations into a lower-dimensional space using random feature mappings.\n\nFeature Mapping:\n\nWe can use random Fourier features or similar mappings to transform\\(Q\\) and\\(K\\).\nFor instance, we can define random feature mappings\\(\\phi(Q)\\) and\\(\\phi(K)\\) for the query and key matrices, respectively.\n\nThe mapping could look like this:\n\\[\n\\phi(x) = \\sqrt{\\frac{2}{D}} \\left[\\cos(w_1^T x + b_1), \\cos(w_2^T x + b_2), \\ldots, \\cos(w_D^T x + b_D)\\right]\n\\]\nWhere\\(D\\) is the number of random features,\\(w_d\\) is a random weight vector, and\\(b_d\\) is a random bias.\nApproximate Attention Scores:\n\nInstead of directly computing\\(QK^T\\), we can compute\\(\\phi(Q)\\) and\\(\\phi(K)\\) and then approximate the attention as follows:\n\n\\[\n\\text{Attention}(Q, K, V) \\approx \\text{softmax}\\left(\\frac{\\phi(Q) \\phi(K)^T}{\\sqrt{d_k}}\\right)V\n\\]\nThis reduces the dimensionality of the operations involved, allowing for faster computations.\nReduce Complexity:\n\nBy using random features, we can reduce the complexity of the attention computation from\\(O(n^2)\\) to\\(O(nD)\\), where\\(D\\) is the number of random features. This is particularly useful for long sequences.\n\n\n\n\n\nAdvantages of Random Feature Approximation of Attention\n\nImproved Efficiency: The computational cost of attention mechanisms is significantly reduced, making it feasible to apply attention to longer sequences.\nScalability: This approach allows models to handle larger datasets and longer sequences effectively.\nFlexibility: Random feature approximation can be adapted to various attention mechanisms, making it a versatile approach for different architectures.\n\n\n\nDisadvantages\n\nApproximation Quality: The quality of the attention approximation depends on the number of random features used. A larger number of features generally leads to better approximations but increases computation.\nVariability: Since random features are generated randomly, the model performance may vary across runs. This can introduce instability, and techniques such as averaging results over multiple runs may be needed.\n\nPositive Orthogonal Random Features (P-ORFs) and Performers are advanced techniques that enhance the efficiency and scalability of attention mechanisms in deep learning models, particularly in Transformer architectures. Below is an overview of both concepts, their motivations, and their applications.\n\n\n1. Positive Orthogonal Random Features (P-ORFs)\nPositive Orthogonal Random Features (P-ORFs) are a way to approximate kernel functions and attention mechanisms efficiently. The idea is to construct random features that maintain certain mathematical properties, such as orthogonality and positivity, to better capture the relationships in the data while keeping computational costs low.\n\nProperties of P-ORFs\n\nOrthogonality:\n\nThe random features are designed to be orthogonal, which helps in maintaining the diversity of the features and reduces redundancy.\nThis property allows the model to learn a richer representation of the input data, making it more expressive.\n\nPositivity:\n\nThe features are positive, meaning they do not take on negative values. This is particularly useful in contexts where negative values may not make sense, such as certain types of similarity measures.\n\nDimensionality Reduction:\n\nP-ORFs approximate the original high-dimensional space by projecting data into a lower-dimensional space, thus reducing the complexity of operations involved in models like attention mechanisms.\n\n\n\n\nMathematical Representation\nThe P-ORF mapping can be represented as follows:\n\\[\n\\phi(x) = \\sqrt{\\frac{2}{D}} \\left[ \\cos(w_1^T x + b_1), \\cos(w_2^T x + b_2), \\ldots, \\cos(w_D^T x + b_D) \\right]\n\\]\nWhere: -\\(w_d\\) are random weight vectors sampled from a Gaussian distribution. -\\(b_d\\) are random biases uniformly sampled from\\([0, 2\\pi]\\). -\\(D\\) is the number of random features.\n\n\n\n2. Performers\nPerformers are a type of Transformer architecture that leverage positive orthogonal random features for approximating attention mechanisms. The core idea is to replace the traditional attention mechanism with a more efficient computation that uses P-ORFs.\n\nKey Features of Performers\n\nEfficient Attention Approximation:\n\nPerformers approximate the scaled dot-product attention mechanism by using P-ORFs to create a low-rank approximation of the attention scores. This reduces the computational complexity of the attention mechanism, especially for long sequences.\n\nLinear Time Complexity:\n\nThe original attention mechanism has a time complexity of\\(O(n^2)\\), where\\(n\\) is the sequence length. Performers reduce this complexity to\\(O(n \\cdot D)\\), where\\(D\\) is the number of random features.\nThis makes it feasible to apply attention to longer sequences and larger datasets.\n\nStability and Robustness:\n\nThe use of positive orthogonal features helps maintain numerical stability and robustness in training deep models.\nThe orthogonality property also ensures that the representations learned by the model are diverse and informative.\n\n\n\n\nAttention Mechanism in Performers\nThe attention mechanism in Performers can be described as follows:\n\nRandom Feature Mapping:\n\nQueries\\(Q\\) and keys\\(K\\) are mapped into a lower-dimensional space using P-ORFs.\n\nApproximation of Attention:\n\nThe attention scores are approximated using the mapped queries and keys. The softmax operation is applied to the approximated scores to obtain the final attention distribution.\n\nOutput Calculation:\n\nThe output is computed by multiplying the attention distribution with the values\\(V\\) as in standard attention mechanisms.\n\n\n\n\n\nAdvantages of Performers and P-ORFs\n\nScalability: Both techniques allow for scalable attention mechanisms that can handle long sequences efficiently.\nReduced Computational Burden: By approximating the attention scores with random features, Performers significantly reduce the computational overhead, enabling faster training and inference.\nExpressive Power: The use of positive orthogonal features ensures that the learned representations are rich and meaningful, improving model performance.\n\n\n\nDisadvantages\n\nApproximation Quality: The quality of the approximation may depend on the number of random features\\(D\\). A smaller number of features may lead to poorer approximations, while a larger number increases computational requirements.\nStochastic Variability: The randomness in generating features can lead to variability in performance across different runs. Techniques such as averaging results over multiple runs or using ensemble methods may be needed to stabilize performance.\n\n\n\n\nNyström approximation\nThe Nyström approximation is a powerful technique used in machine learning and statistics to approximate kernel methods, particularly in scenarios where dealing with large datasets is computationally expensive. This approximation method is particularly beneficial for approximating the kernel matrix, making it feasible to use algorithms that are otherwise intractable due to high computational costs.\nThe Nyström method is named after the Swedish mathematician Gunnar Nyström, who introduced the idea. It provides a way to efficiently approximate a positive semi-definite kernel matrix using a subset of the data points. The primary goal is to reduce the computational complexity involved in kernel methods, especially when working with large datasets.\n\n1. Kernel Matrix\nIn many machine learning tasks, especially in kernel-based methods (like Support Vector Machines or Gaussian Processes), we need to compute a kernel matrix\\(K\\). This matrix is constructed from the pairwise evaluations of a kernel function\\(k(x_i, x_j)\\) for data points\\(x_i\\) and\\(x_j\\).\n\\[\nK = \\begin{bmatrix}\nk(x_1, x_1) & k(x_1, x_2) & \\ldots & k(x_1, x_n) \\\\\nk(x_2, x_1) & k(x_2, x_2) & \\ldots & k(x_2, x_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk(x_n, x_1) & k(x_n, x_2) & \\ldots & k(x_n, x_n) \\\\\n\\end{bmatrix}\n\\]\nWhere\\(n\\) is the number of data points.\nThe computational complexity of constructing this matrix is\\(O(n^2)\\), which can be prohibitive for large datasets.\n\n\n2. Nyström Approximation Steps\nThe Nyström method approximates the kernel matrix using a smaller set of points, typically referred to as “landmark” points. The steps involved in the Nyström approximation are as follows:\n\nStep 1: Select Landmark Points\nRandomly select a subset of\\(m\\) data points from the original dataset, where\\(m &lt; n\\). Let’s denote these selected points as\\(X_m\\).\n\n\nStep 2: Compute the Kernel Matrix for Landmark Points\nCompute the kernel matrix\\(K_{mm}\\) for the selected landmark points:\n\\[\nK_{mm} = \\begin{bmatrix}\nk(x_{i_1}, x_{i_1}) & k(x_{i_1}, x_{i_2}) & \\ldots & k(x_{i_1}, x_{i_m}) \\\\\nk(x_{i_2}, x_{i_1}) & k(x_{i_2}, x_{i_2}) & \\ldots & k(x_{i_2}, x_{i_m}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk(x_{i_m}, x_{i_1}) & k(x_{i_m}, x_{i_2}) & \\ldots & k(x_{i_m}, x_{i_m}) \\\\\n\\end{bmatrix}\n\\]\n\n\nStep 3: Compute the Kernel Matrix Between Landmark and All Points\nCompute the kernel matrix\\(K_{mn}\\) between the landmark points and all other points in the dataset:\n\\[\nK_{mn} = \\begin{bmatrix}\nk(x_{i_1}, x_1) & k(x_{i_1}, x_2) & \\ldots & k(x_{i_1}, x_n) \\\\\nk(x_{i_2}, x_1) & k(x_{i_2}, x_2) & \\ldots & k(x_{i_2}, x_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk(x_{i_m}, x_1) & k(x_{i_m}, x_2) & \\ldots & k(x_{i_m}, x_n) \\\\\n\\end{bmatrix}\n\\]\n\n\nStep 4: Construct the Approximate Kernel Matrix\nUsing\\(K_{mm}\\) and\\(K_{mn}\\), the Nyström approximation of the original kernel matrix\\(K\\) is given by:\n\\[\nK \\approx K_{mn} K_{mm}^{+} K_{mn}^T\n\\]\nWhere\\(K_{mm}^{+}\\) denotes the pseudoinverse of\\(K_{mm}\\). This step is crucial because it allows for the interpolation of the full kernel matrix based on the landmark points.\n\n\n\n3. Advantages of the Nyström Approximation\n\nScalability: The Nyström method reduces the computational burden associated with calculating the full kernel matrix, making it feasible to work with large datasets.\nFlexibility: It can be applied to various kernel functions, allowing for adaptability in different contexts.\nEfficiency: By selecting only a subset of points, the Nyström method can provide good approximations with significantly reduced computation time.\n\n\n\n4. Disadvantages of the Nyström Approximation\n\nApproximation Quality: The quality of the approximation depends on the choice of landmark points. Poor choices may lead to inaccuracies.\nRandomness: If the landmark points are chosen randomly, the results may vary between runs. To improve stability, multiple sets of landmark points can be evaluated.\nInherent Bias: Since the approximation is based on a subset of the data, there may be inherent biases in the approximation.\n\n\n\nConclusion\nThe Nyström approximation is a valuable technique for efficiently approximating kernel matrices, significantly reducing computational costs in large-scale machine learning tasks. By leveraging a subset of the data points, it provides a scalable solution to the challenges posed by traditional kernel methods, making it a crucial tool in modern machine learning.\n\n\nReferences\nhttps://arxiv.org/html/2401.10341v1\nhttps://arxiv.org/pdf/2401.10341v1\nhttps://gaussian37.github.io/dl-concept-quantization/\nhttps://en.wikipedia.org/wiki/Singular_value_decomposition\nhttps://arxiv.org/abs/2102.03902\nhttps://www.youtube.com/watch?v=vSczTbgc8Rc\nhttps://arxiv.org/abs/2408.16289"
  },
  {
    "objectID": "blogs/ericsson_blog1.html",
    "href": "blogs/ericsson_blog1.html",
    "title": "TinyML as-a-Service and the challenges of machine learning at the edge",
    "section": "",
    "text": "https://www.ericsson.com/en/blog/2019/12/tinyml-as-a-service\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge"
  },
  {
    "objectID": "blogs/Low_Rank_Approximation-Part3.html",
    "href": "blogs/Low_Rank_Approximation-Part3.html",
    "title": "Low-rank approximation 3/4",
    "section": "",
    "text": "Low-rank approximation techniques used in Convolutional Neural Networks\n\n1. Spatial Low-Rank Approximation\nExplanation: This technique focuses on simplifying the convolutional filters in terms of their spatial dimensions (width and height). Instead of using one large filter, it approximates the filter using smaller filters that capture spatial features.\n\nExample: A \\(3 \\times 3\\) filter might be approximated by the product of two smaller matrices.\n\nPros:\n\nReduced Computational Cost: Smaller filters lead to fewer computations, speeding up the processing time.\nLower Memory Usage: Uses fewer parameters, which helps save memory.\n\nCons:\n\nPotential Accuracy Loss: Simplifying filters may reduce the model’s ability to capture complex features, leading to decreased accuracy.\nImplementation Complexity: Requires additional steps in model design and training.\n\n\n\n\n2. Filter Low-Rank Approximation\nExplanation: This technique focuses on approximating the entire convolutional filter (not just spatially) by expressing it as a combination of lower-rank matrices. It reduces the complexity of filters in multiple dimensions, including depth.\n\nExample: A filter of size \\(h \\times w \\times d\\) could be approximated as the product of smaller filters for each dimension.\n\nPros:\n\nSignificant Parameter Reduction: This can greatly decrease the number of parameters and computation, especially in deep networks.\nImproved Training Speed: A smaller model can train faster due to reduced computational complexity.\n\nCons:\n\nRisk of Overfitting: If the model is too simple, it may not generalize well to complex datasets.\nPotential for Decreased Performance: Similar to spatial low-rank approximation, it may not capture all the necessary features.\n\n\n\n\n3. Weight Approximation\nExplanation: This method focuses on approximating the weights of the neural network, including weights in convolutional layers. It involves approximating the weight matrices directly to achieve a more efficient representation.\n\nExample: Using techniques like quantization or low-rank factorization to represent weight matrices more compactly.\n\nPros:\n\nMemory Efficiency: Reduces the memory footprint by using less space for weights.\nFaster Inference: Optimized weights can lead to quicker model predictions.\n\nCons:\n\nAccuracy Trade-offs: Approximation can lead to some loss in accuracy, especially if not carefully executed.\nComplexity in Training: Requires careful tuning and validation to maintain performance.\n\n\n\n\n4. Output Approximation\nExplanation: This technique approximates the output of the layers instead of directly modifying the filters or weights. It involves simplifying the output feature maps produced by the CNN.\n\nExample: Instead of using full-resolution feature maps, a smaller approximation can be created.\n\nPros:\n\nReduced Computational Load: Simplifying the output reduces the amount of computation needed in subsequent layers.\nFlexibility: Can be applied after the feature extraction process, making it easier to integrate into existing architectures.\n\nCons:\n\nInformation Loss: Simplifying outputs may lead to a loss of important feature details, affecting the final predictions.\nIntegration Complexity: It may complicate the architecture as it modifies how subsequent layers process information.\n\n\n\n\n5. Combining Spatial and Filter Low-Rank Approximation\nExplanation: This technique combines both spatial and filter low-rank approximations to achieve a more comprehensive reduction in complexity. It approximates filters in both their spatial dimensions and their depth.\n\nExample: Using low-rank approximations for both the height/width of the filter and the depth of the filter at the same time.\n\nPros:\n\nMaximal Efficiency: By combining both methods, you can achieve greater reductions in computation and memory.\nBetter Trade-off: It can lead to better performance retention while significantly lowering resource usage.\n\nCons:\n\nIncreased Complexity: The implementation can become more complicated, requiring careful tuning of multiple aspects of the model.\nDiminishing Returns: The benefits may not always scale linearly, and there could be a threshold beyond which combining methods yields minimal additional gains.\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nTechnique\nPros\nCons\n\n\n\n\nSpatial Low-Rank Approximation\nReduced computational cost, lower memory usage\nPotential accuracy loss, implementation complexity\n\n\nFilter Low-Rank Approximation\nSignificant parameter reduction, improved training speed\nRisk of overfitting, potential for decreased performance\n\n\nWeight Approximation\nMemory efficiency, faster inference\nAccuracy trade-offs, complexity in training\n\n\nOutput Approximation\nReduced computational load, flexibility\nInformation loss, integration complexity\n\n\nCombining Spatial and Filter Approximation\nMaximal efficiency, better trade-off\nIncreased complexity, diminishing returns\n\n\n\n\n\nConclusion\nEach of these low-rank approximation techniques offers unique benefits and challenges. Choosing the right approach depends on the specific application, the complexity of the data, and the performance requirements of the CNN. Balancing the trade-offs effectively can lead to more efficient models suitable for a variety of applications, especially on resource-constrained devices."
  },
  {
    "objectID": "blogs/HU_Summer_2023.html",
    "href": "blogs/HU_Summer_2023.html",
    "title": "TinyML as-a-Service with Helsinki University summer course 2023",
    "section": "",
    "text": "TinyMLaaS\n\n\nRead more…"
  },
  {
    "objectID": "blogs/imagimobstudio.html",
    "href": "blogs/imagimobstudio.html",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is an end-to-end platform designed for developing Edge AI and Machine Learning applications. The platform covers the machine learning workflow from data collection to model deployment in embedded devices to support both experts and non-experts in building production grade models.\n\n\n\n\nImagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C.\n\n\n\n\n\n\nPredictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars.\n\n\n\n\nImagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/imagimobstudio.html#key-features",
    "href": "blogs/imagimobstudio.html#key-features",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C."
  },
  {
    "objectID": "blogs/imagimobstudio.html#use-cases",
    "href": "blogs/imagimobstudio.html#use-cases",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Predictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars."
  },
  {
    "objectID": "blogs/imagimobstudio.html#conclusion",
    "href": "blogs/imagimobstudio.html#conclusion",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/Low_Rank_Approximation-Part2.html",
    "href": "blogs/Low_Rank_Approximation-Part2.html",
    "title": "Low-rank approximation 2/4",
    "section": "",
    "text": "1. Matrix Decomposition Techniques\n\nSingular Value Decomposition (SVD): SVD is a linear algebra technique that decomposes a matrix into three other matrices. For a convolutional filter represented as a matrix \\(W\\), SVD finds matrices \\(U\\), \\(\\Sigma\\), and \\(V\\) such that \\(W = U \\Sigma V^T\\).\n\nApplication: By keeping only the top \\(k\\) singular values from \\(\\Sigma\\), and the corresponding columns in \\(U\\) and \\(V\\), we can create a low-rank approximation of the original filter.\nBenefits: This significantly reduces the number of parameters and computations involved.\n\n\n\n\n\n\nTucker Decomposition: Tucker decomposition generalizes SVD for tensors (multi-dimensional arrays). It decomposes a tensor into a core tensor and a matrix factorization of each mode (dimension).\n\nApplication: In CNNs, filters can be represented as 3D tensors (width, height, depth). Tucker decomposition approximates the filter as a smaller core tensor with smaller matrices for each dimension.\nBenefits: Allows a more flexible approximation with potentially better representation capabilities compared to SVD.\n\n\n\n\n\n\nCP (CANDECOMP/PARAFAC) Decomposition: CP decomposition also approximates a tensor by expressing it as the sum of a finite number of rank-one tensors.\n\nApplication: In CNNs, it can help decompose filters into simpler components that capture the same information with fewer parameters.\nBenefits: It can be simpler to implement in certain situations than Tucker decomposition.\n\n\n\n\n2. Filter Factorization Techniques\n\nTwo-Dimensional Factorization: Instead of using one large convolutional kernel, it decomposes it into two smaller kernels, typically one for spatial dimensions (width and height) and one for the depth dimension.\n\nApplication: A \\(k \\times k \\times d\\) filter can be approximated by multiplying a \\(k \\times k\\) filter with a \\(k \\times 1\\) filter for each depth slice.\nBenefits: This reduces the computational complexity from \\(O(k^2 \\cdot d)\\) to \\(O(k^2 + k \\cdot d)\\).\n\nDepthwise Separable Convolutions: This technique separates the convolution process into two layers: a depthwise convolution (applies a single filter per input channel) followed by a pointwise convolution (1x1 convolution to combine the outputs).\n\nApplication: MobileNets, for instance, use this technique extensively to reduce the number of parameters while maintaining performance.\nBenefits: It reduces both computational and memory requirements significantly compared to standard convolutions.\n\n\n\n\n3. Quantization Techniques\n\nWeight Quantization: Involves reducing the precision of the weights (e.g., using 8-bit integers instead of 32-bit floating-point numbers).\n\nApplication: After performing low-rank approximation, quantization can further reduce the model size and speed up inference on hardware with limited precision.\nBenefits: Lower memory usage and faster computation, especially on specialized hardware like GPUs or TPUs.\n\n\n\n\n\n\n\n4. Structured Low-Rank Approximations\n\nLow-Rank Convolutional Filters: Instead of fully dense filters, structured filters with fewer connections can be designed to maintain performance while reducing rank.\n\nApplication: This may involve using predefined patterns or structures for the filters based on the spatial hierarchies present in the data.\nBenefits: More efficient use of parameters while exploiting spatial correlation in images.\n\n\n\n\n5. Regularization Techniques\n\nLow-Rank Regularization: Adding a low-rank regularization term to the loss function during training encourages the network to learn low-rank filters.\n\nApplication: This could be achieved by penalizing the nuclear norm (sum of singular values) of the filter matrices in the loss function.\nBenefits: Helps in obtaining low-rank approximations directly from the training process, maintaining a good trade-off between accuracy and model size."
  },
  {
    "objectID": "blogs/llm_on_camera.html",
    "href": "blogs/llm_on_camera.html",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO proposes a 1.5-month Proof of Concept (PoC) to demonstrate the transformative potential of Local Large Language Models (LLMs) on edge cameras as a Universal Sensor for smart mobility. Utilizing NVIDIA JETSON Orin AGX for high-performance, real-time image and data processing, our solution will empower both public authorities and citizens with localized insights.\nBy ensuring full GDPR compliance through on-device processing, this solution opens new possibilities for smart city services. Citizens, via a public app interface, will be able to ask real-time questions about their immediate environment—from parking availability to public safety insights—unlocking a wide range of unseen use cases.\n\n\n\nIn modern cities, there is a growing demand for context-aware, real-time insights for mobility, safety, and general urban well-being. Current solutions may rely heavily on inflexible fixed purpose sensors or security camera with cloud infrastructure, which raises privacy concerns and introduces latency. Our camera as an Universal Sensor approach, leveraging NVIDIA JETSON Orin AGX, processes information locally, providing flexible, actionable insights without compromising privacy.\nThis PoC will not only demonstrate how this system improves mobility management and public safety, but it will also introduce new unseen citizen-driven use cases by allowing anyone to communite with LLM on CAM at real-time via an WebApp, effectively democratizing urban data.\n\n\n\nOur LLM on Camera as a Universal Sensor solution utilizes multimodal LLMs to interpret visual data (i.e. camera /video capture images) and answer questions in real-time. With the NVIDIA JETSON Orin AGX, which provides industry-leading computational power, the solution will demonstrate several impactful use cases:\n\n\n\nVacant Parking Slot Detection: Edge cameras detect available parking spaces and provide real-time updates to users by asking LLM, even with number of vacancy, size of such slot, and anything as long as LLM can understand or interpret.\nPublic Safety Monitoring: Detect disturbances or suspicious activity in public spaces. The system can instantly alert authorities to potential security threats, improving urban safety without exposing personal data by asking LLM with “any fights heppening?”.\nEnvironmental Monitoring: Citizens or local governments can query the system about feeling of air quality, noise levels, or local weather conditions (e.g., slippery sidewalks during winter) by asking LLM what kind of cloths people wear, ensuring timely responses to environmental changes.\nTraffic Flow and Congestion Analysis: The system tracks traffic patterns and provides real-time feedback on road congestion, alerting users to alternate routes or times to travel if needed via LLM.\nPublic Event Reporting: The app can report real-time crowd density and event status during festivals, concerts, or protests, helping citizens avoid overcrowded areas or plan their attendance.\nEmergency Support: In the case of a disaster (e.g., a fire or flood), the system can help first responders by providing any type of localized data on obstacles, congestion, or affected areas.\nLocalized Queries from Citizens: Any citizen can ask the LLM on Camera specific questions about the local environment—e.g., “Are there available seats in the park?” or “Is there a bike rack near this building?”—unlocking real-time insights and enhancing city navigation.\n\nThe flexibility of the Universal Sensor allows it to be expanded into sectors like tourism, disaster response, and public infrastructure management, enabling both real-time insights and enhanced citizen engagement as a part of City infrastructure.\n\n\n\nNormally the image itself won’t be seen by users but only conversation.\n\nAnd you can withdraw any insights via LLM.\n\n\n\n\n\nThe NVIDIA JETSON Orin AGX provides significant computational capacity for real-time AI applications, making it the ideal platform for running multimodal LLMs. The PoC will utilize the following:\n\nNVIDIA JETSON Orin AGX: A powerful edge AI platform capable of handling advanced computations with up to 200 TOPS of performance.\nMultimodal LLM: Pre-trained LLMs that can interpret visual, textual, and contextual data in real-time, optimized for efficient on-device processing.\nAdvanced Model Compression: Techniques like quantization and pruning ensure that LLMs are optimized for running efficiently on the Orin AGX.\n\nBy keeping data processing local, we eliminate the need for external data transmission, reducing latency while preserving privacy.\n\n\n\nBy the end of this 1.5-month PoC, we expect to demonstrate:\n\nParking Slot Detection: Detecting vacant parking spaces and providing real-time updates.\nPublic Safety Monitoring: Detecting and responding to public safety incidents.\nCitizen Engagement via WebApp: Offering citizens real-time access to the system’s insights by allowing them to query localized information through the app. This democratization of data will unlock unforeseen use cases that expand beyond mobility and safety.\nEnvironmental and Traffic Monitoring: Demonstrating how the Universal Sensor can track and respond to environmental and traffic conditions in real time, supporting various urban planning initiatives.\n\n\n\n\n\nWeek 1 - Initial Setup:\n\nInstall and configure NVIDIA JETSON Orin AGX and integrate multimodal LLMs for the core and extended use cases.\nBegin small-scale testing in a controlled environment.\n\nWeek 2-3 - Use Case Development:\n\nFine-tune parking slot detection, public safety monitoring, and citizen query capabilities.\nDevelop app functionality for citizen interaction with the Universal Sensor.\n\nWeek 4-6 - Deployment and Validation:\n\nDeploy the system in a small urban environment (e.g., a parking lot, public square).\nTest accuracy, latency, and app engagement, collecting feedback from stakeholders and citizens.\nAnalyze results for further scalability.\n\n\n\n\n\nWe will collaborate with local municipalities and NVIDIA to optimize the NVIDIA JETSON Orin AGX platform for smart mobility applications. Citizen engagement will be supported through partnerships with local app developers and parking management services.\n\n\n\nNinjaLABO’s LLM on Camera as a Universal Sensor is a powerful, privacy-preserving tool designed to transform smart city services. By leveraging the advanced capabilities of NVIDIA JETSON Orin AGX and introducing an open citizen engagement app, this PoC will demonstrate the potential for real-time mobility management, public safety enhancement, and localized insights for citizens. The possibilities of new use cases emerging from public queries are vast, making this technology a cornerstone of smart city innovation.\nWe look forward to showcasing how Universal Sensors can transform the way citizens interact with and benefit from their urban environments, fully aligning with Helsinki’s goals of AI-powered mobility."
  },
  {
    "objectID": "blogs/llm_on_camera.html#executive-summary",
    "href": "blogs/llm_on_camera.html#executive-summary",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO proposes a 1.5-month Proof of Concept (PoC) to demonstrate the transformative potential of Local Large Language Models (LLMs) on edge cameras as a Universal Sensor for smart mobility. Utilizing NVIDIA JETSON Orin AGX for high-performance, real-time image and data processing, our solution will empower both public authorities and citizens with localized insights.\nBy ensuring full GDPR compliance through on-device processing, this solution opens new possibilities for smart city services. Citizens, via a public app interface, will be able to ask real-time questions about their immediate environment—from parking availability to public safety insights—unlocking a wide range of unseen use cases."
  },
  {
    "objectID": "blogs/llm_on_camera.html#challenge-context",
    "href": "blogs/llm_on_camera.html#challenge-context",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "In modern cities, there is a growing demand for context-aware, real-time insights for mobility, safety, and general urban well-being. Current solutions may rely heavily on inflexible fixed purpose sensors or security camera with cloud infrastructure, which raises privacy concerns and introduces latency. Our camera as an Universal Sensor approach, leveraging NVIDIA JETSON Orin AGX, processes information locally, providing flexible, actionable insights without compromising privacy.\nThis PoC will not only demonstrate how this system improves mobility management and public safety, but it will also introduce new unseen citizen-driven use cases by allowing anyone to communite with LLM on CAM at real-time via an WebApp, effectively democratizing urban data."
  },
  {
    "objectID": "blogs/llm_on_camera.html#solution-overview",
    "href": "blogs/llm_on_camera.html#solution-overview",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "Our LLM on Camera as a Universal Sensor solution utilizes multimodal LLMs to interpret visual data (i.e. camera /video capture images) and answer questions in real-time. With the NVIDIA JETSON Orin AGX, which provides industry-leading computational power, the solution will demonstrate several impactful use cases:\n\n\n\nVacant Parking Slot Detection: Edge cameras detect available parking spaces and provide real-time updates to users by asking LLM, even with number of vacancy, size of such slot, and anything as long as LLM can understand or interpret.\nPublic Safety Monitoring: Detect disturbances or suspicious activity in public spaces. The system can instantly alert authorities to potential security threats, improving urban safety without exposing personal data by asking LLM with “any fights heppening?”.\nEnvironmental Monitoring: Citizens or local governments can query the system about feeling of air quality, noise levels, or local weather conditions (e.g., slippery sidewalks during winter) by asking LLM what kind of cloths people wear, ensuring timely responses to environmental changes.\nTraffic Flow and Congestion Analysis: The system tracks traffic patterns and provides real-time feedback on road congestion, alerting users to alternate routes or times to travel if needed via LLM.\nPublic Event Reporting: The app can report real-time crowd density and event status during festivals, concerts, or protests, helping citizens avoid overcrowded areas or plan their attendance.\nEmergency Support: In the case of a disaster (e.g., a fire or flood), the system can help first responders by providing any type of localized data on obstacles, congestion, or affected areas.\nLocalized Queries from Citizens: Any citizen can ask the LLM on Camera specific questions about the local environment—e.g., “Are there available seats in the park?” or “Is there a bike rack near this building?”—unlocking real-time insights and enhancing city navigation.\n\nThe flexibility of the Universal Sensor allows it to be expanded into sectors like tourism, disaster response, and public infrastructure management, enabling both real-time insights and enhanced citizen engagement as a part of City infrastructure.\n\n\n\nNormally the image itself won’t be seen by users but only conversation.\n\nAnd you can withdraw any insights via LLM."
  },
  {
    "objectID": "blogs/llm_on_camera.html#technical-approach",
    "href": "blogs/llm_on_camera.html#technical-approach",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "The NVIDIA JETSON Orin AGX provides significant computational capacity for real-time AI applications, making it the ideal platform for running multimodal LLMs. The PoC will utilize the following:\n\nNVIDIA JETSON Orin AGX: A powerful edge AI platform capable of handling advanced computations with up to 200 TOPS of performance.\nMultimodal LLM: Pre-trained LLMs that can interpret visual, textual, and contextual data in real-time, optimized for efficient on-device processing.\nAdvanced Model Compression: Techniques like quantization and pruning ensure that LLMs are optimized for running efficiently on the Orin AGX.\n\nBy keeping data processing local, we eliminate the need for external data transmission, reducing latency while preserving privacy."
  },
  {
    "objectID": "blogs/llm_on_camera.html#expected-outcomes-and-impact",
    "href": "blogs/llm_on_camera.html#expected-outcomes-and-impact",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "By the end of this 1.5-month PoC, we expect to demonstrate:\n\nParking Slot Detection: Detecting vacant parking spaces and providing real-time updates.\nPublic Safety Monitoring: Detecting and responding to public safety incidents.\nCitizen Engagement via WebApp: Offering citizens real-time access to the system’s insights by allowing them to query localized information through the app. This democratization of data will unlock unforeseen use cases that expand beyond mobility and safety.\nEnvironmental and Traffic Monitoring: Demonstrating how the Universal Sensor can track and respond to environmental and traffic conditions in real time, supporting various urban planning initiatives."
  },
  {
    "objectID": "blogs/llm_on_camera.html#project-timeline-1.5-months",
    "href": "blogs/llm_on_camera.html#project-timeline-1.5-months",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "Week 1 - Initial Setup:\n\nInstall and configure NVIDIA JETSON Orin AGX and integrate multimodal LLMs for the core and extended use cases.\nBegin small-scale testing in a controlled environment.\n\nWeek 2-3 - Use Case Development:\n\nFine-tune parking slot detection, public safety monitoring, and citizen query capabilities.\nDevelop app functionality for citizen interaction with the Universal Sensor.\n\nWeek 4-6 - Deployment and Validation:\n\nDeploy the system in a small urban environment (e.g., a parking lot, public square).\nTest accuracy, latency, and app engagement, collecting feedback from stakeholders and citizens.\nAnalyze results for further scalability."
  },
  {
    "objectID": "blogs/llm_on_camera.html#collaboration-and-ecosystem",
    "href": "blogs/llm_on_camera.html#collaboration-and-ecosystem",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "We will collaborate with local municipalities and NVIDIA to optimize the NVIDIA JETSON Orin AGX platform for smart mobility applications. Citizen engagement will be supported through partnerships with local app developers and parking management services."
  },
  {
    "objectID": "blogs/llm_on_camera.html#conclusion",
    "href": "blogs/llm_on_camera.html#conclusion",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO’s LLM on Camera as a Universal Sensor is a powerful, privacy-preserving tool designed to transform smart city services. By leveraging the advanced capabilities of NVIDIA JETSON Orin AGX and introducing an open citizen engagement app, this PoC will demonstrate the potential for real-time mobility management, public safety enhancement, and localized insights for citizens. The possibilities of new use cases emerging from public queries are vast, making this technology a cornerstone of smart city innovation.\nWe look forward to showcasing how Universal Sensors can transform the way citizens interact with and benefit from their urban environments, fully aligning with Helsinki’s goals of AI-powered mobility."
  },
  {
    "objectID": "blogs/mvp6.html",
    "href": "blogs/mvp6.html",
    "title": "AI Compression as-a-Service MVP6 review",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html",
    "href": "blogs/hiring_2024_fall.html",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Location: Remote (Occasional meetings at A-Grid, Otaniemi)\nCompany: NinjaLABO\nDuration: Full-time or Part-time for 3 months (ASAP to the end of December).\n\n\n\nNinjaLABO is a fresh, finnish startup at the forefront of AI innovation, focusing on AI model compression and deep neural network (DNN) runtime optimization for IoT, particularly for enabling onboard Edge AI in satellite systems.\n\n\n\n\n\nImplement DNN model compression techniques.\nIntegrate the above into our SaaS platform, TinyML as-a-Service.\nPort and optimize DNN runtimes for Edge devices such as Nvidia Jetson and Orange Pi 5+.\n\n\n\n\n\nStrong proficiency in Python and C/C++.\nFamiliarity with PyTorch or TensorFlow.\n\n\n\n\n\nHands-on experience with Nvidia Jetson & Orange Pi 5+.\nExperience in AI model compression and DNN runtime development.\nKnowledge of TensorFlow Lite for Microcontrollers, MLIR, and IREE.\nKnowledge of HyperSpectral Imaging (HSI).\n\n\n\n\n\nSuccessful completion of CS-E4890 - Deep Learning D with good grades.\nSuccessful completion of CS-E4580 - Programming Parallel Computers D with good grades.\n\n\n\n\n\nFull-time preferred, part-time candidates must be available for more than 3 days per week.\nDiscord for daily communication\nDaily Scrum at 8:00am every weekday via Discord\nGitHuB Project as Kanban board\nGitHub Workflow/Actions as CI/CD\n2 weeks Sprint planning / review / retro under SCRUM\n\n\n\n\n\n3,000+€ for PhD students\n2,600-2,800€ for Master’s students\n2,100-2,400€ for Bachelor’s students\n\nContracts facilitated through UKKO.fi.\n\n\n\nInterested? Apply to Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#about-us",
    "href": "blogs/hiring_2024_fall.html#about-us",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "NinjaLABO is a fresh, finnish startup at the forefront of AI innovation, focusing on AI model compression and deep neural network (DNN) runtime optimization for IoT, particularly for enabling onboard Edge AI in satellite systems."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#key-responsibilities",
    "href": "blogs/hiring_2024_fall.html#key-responsibilities",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Implement DNN model compression techniques.\nIntegrate the above into our SaaS platform, TinyML as-a-Service.\nPort and optimize DNN runtimes for Edge devices such as Nvidia Jetson and Orange Pi 5+."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#requirements",
    "href": "blogs/hiring_2024_fall.html#requirements",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Strong proficiency in Python and C/C++.\nFamiliarity with PyTorch or TensorFlow."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#preferred-qualifications",
    "href": "blogs/hiring_2024_fall.html#preferred-qualifications",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Hands-on experience with Nvidia Jetson & Orange Pi 5+.\nExperience in AI model compression and DNN runtime development.\nKnowledge of TensorFlow Lite for Microcontrollers, MLIR, and IREE.\nKnowledge of HyperSpectral Imaging (HSI)."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#preferred-education",
    "href": "blogs/hiring_2024_fall.html#preferred-education",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Successful completion of CS-E4890 - Deep Learning D with good grades.\nSuccessful completion of CS-E4580 - Programming Parallel Computers D with good grades."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#wow",
    "href": "blogs/hiring_2024_fall.html#wow",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Full-time preferred, part-time candidates must be available for more than 3 days per week.\nDiscord for daily communication\nDaily Scrum at 8:00am every weekday via Discord\nGitHuB Project as Kanban board\nGitHub Workflow/Actions as CI/CD\n2 weeks Sprint planning / review / retro under SCRUM"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#monthly-pay",
    "href": "blogs/hiring_2024_fall.html#monthly-pay",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "3,000+€ for PhD students\n2,600-2,800€ for Master’s students\n2,100-2,400€ for Bachelor’s students\n\nContracts facilitated through UKKO.fi."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#how-to-apply",
    "href": "blogs/hiring_2024_fall.html#how-to-apply",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Interested? Apply to Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "SVD is a powerful mathematical tool often used in image processing to reduce the amount of data while still maintaining an acceptable image quality. This technique effectively reduces the dimensionality of an image, making it possible to store it using fewer values.\n\nAn image can be represented as a 2D matrix of pixel values.\nFor grayscale images, the pixel intensities range from 0 (black) to 255 (white).\nFor RGB images, we have three matrices (one for each color channel: Red, Green, Blue).\nAssume a grayscale image for simplicity.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import io, color\n\n\n\n\n\n# Load the image\nimage = io.imread('./imagenette/imagenette2-320/train/n02102040/n02102040_3514.JPEG')\n# Show the original image\nplt.imshow(image)\nplt.title('Original Image')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Load the image\nimage = io.imread('./imagenette/imagenette2-320/train/n02102040/n02102040_3514.JPEG')\n\n\n# Convert the image to grayscale\ngrayscale_image = color.rgb2gray(image)\n\n# Show the original image\nplt.imshow(grayscale_image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\nplt.show()\n\n# Get the image matrix\nA = np.array(grayscale_image)\n\n\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition breaks down the matrix \\(A\\) into three other matrices \\(U\\), \\(Σ\\) (Sigma), and \\(V^T\\).\nThe formula is: $ A = U V^T $\n\n\\(U\\) and \\(V^T\\) are orthogonal matrices.\n\\(Σ\\) is a diagonal matrix containing singular values in descending order.\n\n\n\n# Perform Singular Value Decomposition\nU, S, VT = np.linalg.svd(A, full_matrices=False)\n\n# S is returned as a 1D array, convert it into a diagonal matrix\nSigma = np.diag(S)\n\n\n\n\nThe goal of compression is to reduce the amount of data. To do this, we can keep only the top k singular values in \\(Σ\\), and truncate the corresponding columns in U and rows in \\(V^T\\). Choosing a smaller \\(k\\) reduces the image size while retaining the most significant features.\n\\(k\\) is the number of singular values you want to keep. A larger k gives higher quality but less compression, whereas a smaller \\(k\\) gives more compression but lower quality.\nReconstruct the image by multiplying the truncated \\(U\\), \\(Σ\\), and \\(V^T\\). This results in an approximation of the original image but using fewer components.\nStorage Calculation: * The original image matrix A is of size \\(m \\times n\\).\n\nAfter SVD, the storage required is:\n\n\\(U_k\\): \\(m \\times k\\)\n\\(Σ_k\\): \\(k \\times k\\)\n\\(V^T_k\\): \\(k \\times n\\)\n\nThe total number of values needed to store the compressed image is \\(mk+k2+kn\\), which is much smaller compared to mnmnmn when k is small.\n\n\n# Choose the number of singular values to keep\nk = 1  # adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 5  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 10  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 20  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#singular-value-decomposition-svd---dimensionality-reduction-single-image-numpy",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#singular-value-decomposition-svd---dimensionality-reduction-single-image-numpy",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "SVD is a powerful mathematical tool often used in image processing to reduce the amount of data while still maintaining an acceptable image quality. This technique effectively reduces the dimensionality of an image, making it possible to store it using fewer values.\n\nAn image can be represented as a 2D matrix of pixel values.\nFor grayscale images, the pixel intensities range from 0 (black) to 255 (white).\nFor RGB images, we have three matrices (one for each color channel: Red, Green, Blue).\nAssume a grayscale image for simplicity.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import io, color"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#read-the-original-image",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#read-the-original-image",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "# Load the image\nimage = io.imread('./imagenette/imagenette2-320/train/n02102040/n02102040_3514.JPEG')\n# Show the original image\nplt.imshow(image)\nplt.title('Original Image')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#read-the-image-and-convert-it-to-a-matrix",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#read-the-image-and-convert-it-to-a-matrix",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "# Load the image\nimage = io.imread('./imagenette/imagenette2-320/train/n02102040/n02102040_3514.JPEG')\n\n\n# Convert the image to grayscale\ngrayscale_image = color.rgb2gray(image)\n\n# Show the original image\nplt.imshow(grayscale_image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\nplt.show()\n\n# Get the image matrix\nA = np.array(grayscale_image)"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#apply-singular-value-decomposition-svd",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#apply-singular-value-decomposition-svd",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "Singular Value Decomposition breaks down the matrix \\(A\\) into three other matrices \\(U\\), \\(Σ\\) (Sigma), and \\(V^T\\).\nThe formula is: $ A = U V^T $\n\n\\(U\\) and \\(V^T\\) are orthogonal matrices.\n\\(Σ\\) is a diagonal matrix containing singular values in descending order.\n\n\n\n# Perform Singular Value Decomposition\nU, S, VT = np.linalg.svd(A, full_matrices=False)\n\n# S is returned as a 1D array, convert it into a diagonal matrix\nSigma = np.diag(S)"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#reduce-dimensionality-by-truncating-singular-values",
    "href": "blogs/SVD_Dimensionality_Reduction_Single_Image_V4.html#reduce-dimensionality-by-truncating-singular-values",
    "title": "Low Rank Approximation Implementation 4/4",
    "section": "",
    "text": "The goal of compression is to reduce the amount of data. To do this, we can keep only the top k singular values in \\(Σ\\), and truncate the corresponding columns in U and rows in \\(V^T\\). Choosing a smaller \\(k\\) reduces the image size while retaining the most significant features.\n\\(k\\) is the number of singular values you want to keep. A larger k gives higher quality but less compression, whereas a smaller \\(k\\) gives more compression but lower quality.\nReconstruct the image by multiplying the truncated \\(U\\), \\(Σ\\), and \\(V^T\\). This results in an approximation of the original image but using fewer components.\nStorage Calculation: * The original image matrix A is of size \\(m \\times n\\).\n\nAfter SVD, the storage required is:\n\n\\(U_k\\): \\(m \\times k\\)\n\\(Σ_k\\): \\(k \\times k\\)\n\\(V^T_k\\): \\(k \\times n\\)\n\nThe total number of values needed to store the compressed image is \\(mk+k2+kn\\), which is much smaller compared to mnmnmn when k is small.\n\n\n# Choose the number of singular values to keep\nk = 1  # adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 5  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 10  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Choose the number of singular values to keep\nk = 20  #  adjust k based on the desired level of compression\n\n# Truncate U, Sigma, and VT to keep only the first k components\nU_k = U[:, :k]\nSigma_k = Sigma[:k, :k]\nVT_k = VT[:k, :]\n\n# Reconstruct the compressed image\nA_k = np.dot(U_k, np.dot(Sigma_k, VT_k))\n# Plot the reconstructed image\nplt.imshow(A_k, cmap='gray')\nplt.title(f'Compressed Image with k={k}')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "blogs/yolofaceid.html",
    "href": "blogs/yolofaceid.html",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "",
    "text": "In today’s world, face identification systems are becoming increasingly crucial in various applications, from security and surveillance to personalized customer experiences. Building a real-time face identification system can seem daunting, but with the right tools and approaches, it becomes manageable. In this article, we will walk you through a project to develop such a system, using some of the best tools available, including Ultralytics YOLOv8, managed cloud services (e.g. AWS Rekognition and Azure Face API), and more. We’ll also compare different technologies to help you choose the best one for your needs."
  },
  {
    "objectID": "blogs/yolofaceid.html#setting-up-yolov8",
    "href": "blogs/yolofaceid.html#setting-up-yolov8",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Setting Up YOLOv8",
    "text": "Setting Up YOLOv8\n\nInstall the Ultralytics Package: Begin by installing the necessary software using pip:\npip install ultralytics\nLoad the YOLOv8 Model: Load a pre-trained YOLOv8 model or start with a custom model:\nfrom ultralytics import YOLO\n\n# Load a YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n\n# Perform inference on an image\nresults = model('path/to/your/image.jpg')\nresults.show()  # Display results"
  },
  {
    "objectID": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "href": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Training on a Custom Dataset",
    "text": "Training on a Custom Dataset\nTo fine-tune YOLOv8 for face detection, you need a dataset of face images with annotations in YOLO format. You can use public datasets like WIDER FACE or CelebA.\n\nPrepare Your Dataset: Ensure your dataset is properly annotated and structured.\nConfigure and Train the Model: Train YOLOv8 using your dataset:\nmodel = YOLO('yolov8n.pt')\nmodel.train(data='path/to/data.yaml', epochs=100, imgsz=640, batch=16)"
  },
  {
    "objectID": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "href": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Deploying for Real-Time Face Detection",
    "text": "Deploying for Real-Time Face Detection\nOnce trained, you can deploy YOLOv8 for real-time face detection. This can be done by feeding live video streams into the model and processing each frame.\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the trained model\nmodel = YOLO('runs/train/exp/weights/best.pt')\n\n# Capture video from webcam\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Perform inference\n    results = model(frame)\n\n    # Display results\n    results.show()\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "blogs/yolofaceid.html#key-factors",
    "href": "blogs/yolofaceid.html#key-factors",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Factors:",
    "text": "Key Factors:\n\nGPU is superiour to CPU at inference from cost vs perf comparison.\nProcessing Power: Each GPU’s ability to handle a specific number of frames per second (FPS).\nTotal FPS Requirement: The total computational load imposed by the 200 cameras.\nRack-Mount Server Specifications: The number of GPUs each server can support."
  },
  {
    "objectID": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "href": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Calculating Total FPS Requirement:",
    "text": "Calculating Total FPS Requirement:\n\n200 Cameras at 30 FPS each:\n\nTotal FPS required = 200 cameras * 30 FPS = 6000 FPS."
  },
  {
    "objectID": "blogs/yolofaceid.html#gpu-capacity",
    "href": "blogs/yolofaceid.html#gpu-capacity",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "GPU Capacity:",
    "text": "GPU Capacity:\n\nNVIDIA RTX 3080: Can handle approximately 400 FPS for YOLOv8-tiny and around 80-100 FPS for standard YOLOv8.\nNVIDIA RTX 3090: Can handle approximately 600 FPS for YOLOv8-tiny and around 120-150 FPS for standard YOLOv8.\nNVIDIA RTX 4090: Can handle approximately 1000 FPS for YOLOv8-tiny and around 200-250 FPS for standard YOLOv8."
  },
  {
    "objectID": "blogs/yolofaceid.html#example-server-configurations",
    "href": "blogs/yolofaceid.html#example-server-configurations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Example Server Configurations:",
    "text": "Example Server Configurations:\n\nSupermicro 4U GPU Server (SYS-4029GP-TRT):\n\nSupports up to 4 GPUs.\nIf each server uses 4x RTX 4090 GPUs, the total processing capability per server would be:\n\nYOLOv8-tiny: 4000 FPS (4 GPUs * 1000 FPS).\nStandard YOLOv8: 800-1000 FPS (4 GPUs * 200-250 FPS).\n\n\nASUS ESC8000A-E11:\n\nSupports up to 8 GPUs.\nWith 8x RTX 4090 GPUs:\n\nYOLOv8-tiny: 8000 FPS (8 GPUs * 1000 FPS).\nStandard YOLOv8: 1600-2000 FPS (8 GPUs * 200-250 FPS)."
  },
  {
    "objectID": "blogs/yolofaceid.html#number-of-servers-needed",
    "href": "blogs/yolofaceid.html#number-of-servers-needed",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Number of Servers Needed:",
    "text": "Number of Servers Needed:\n\nFor YOLOv8-tiny:\n\nWith Supermicro 4U Server: 1.5 servers (round up to 2 servers).\nWith ASUS ESC8000A-E11: 1 server is sufficient.\n\nFor Standard YOLOv8:\n\nWith Supermicro 4U Server: 6-8 servers.\nWith ASUS ESC8000A-E11: 3-4 servers."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary",
    "href": "blogs/yolofaceid.html#summary",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\n\nYOLOv8-tiny: You would need 1-2 servers (depending on GPU configuration) to handle the 200 cameras.\nStandard YOLOv8: You would need 3-8 servers depending on the specific GPU and server model you choose.\n\nThese estimates assume maximum utilization of each GPU’s capabilities, and the actual number might vary based on real-world conditions, such as CPU bottlenecks, memory limitations, and other factors."
  },
  {
    "objectID": "blogs/yolofaceid.html#key-parameters",
    "href": "blogs/yolofaceid.html#key-parameters",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Parameters",
    "text": "Key Parameters\n\nResolution:\n\n2MP (1080p): Approximately 3 Mbps\n4MP: Approximately 6 Mbps\n\nFrame Rate: 30 frames per second (FPS)\nRecording Time: 24 hours per day, 30 days per month\nNumber of Cameras: 200\nCompression: H.264 assumed (H.265 could reduce storage by up to 50%)"
  },
  {
    "objectID": "blogs/yolofaceid.html#storage-calculation",
    "href": "blogs/yolofaceid.html#storage-calculation",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Storage Calculation",
    "text": "Storage Calculation\nThe total storage required can be calculated using the following equation:\n\\[\n\\text{Total Storage (TB)} = \\left(\\frac{\\text{Bitrate (Mbps)} \\times 86,400 \\times 30 \\times \\text{Number of Cameras}}{8 \\times 1024^2}\\right)\n\\]\nWhere:\n\n86,400: Number of seconds in a day\n30: Number of days in a month\n8: Conversion factor from bits to bytes\n1024^2: Conversion from MB to TB"
  },
  {
    "objectID": "blogs/yolofaceid.html#results",
    "href": "blogs/yolofaceid.html#results",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Results",
    "text": "Results\n\nFor 2MP Cameras (3 Mbps):\n\nTotal Storage: Approximately 185.2 TB per month.\n\nFor 4MP Cameras (6 Mbps):\n\nTotal Storage: Approximately 370.5 TB per month."
  },
  {
    "objectID": "blogs/yolofaceid.html#considerations",
    "href": "blogs/yolofaceid.html#considerations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Considerations",
    "text": "Considerations\n\nCompression: Using H.265 compression could potentially reduce the storage requirement by 50%.\nMotion Detection: If motion detection is used instead of continuous recording, the required storage could be significantly less."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary-1",
    "href": "blogs/yolofaceid.html#summary-1",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\nFor 200 cameras recording continuously at 2-4MP resolution, the storage requirement ranges from approximately 185.2 TB to 370.5 TB per month. Adjustments to compression methods, frame rates, or recording strategies could alter these estimates."
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html",
    "href": "blogs/KD_Self_Distillation_V3.html",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "Self-distillation is a technique in which a model distills knowledge into itself. In this case, the same model is used both as a teacher and a student, a process where intermediate layers’ outputs help guide earlier layers in the same model. The idea is that the student network learns from itself by taking advantage of various self-regularization strategies, and intermediate outputs can be utilized for knowledge transfer.\nIn this case, instead of the classical teacher-student approach, we’ll consider ResNet18, with the student learning from its own intermediate outputs. Self-distillation usually results in a student model that generalizes better without needing an external teacher.\n\n\n\n\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge Distillation: A Survey,” May 20, 2021, arXiv: arXiv:2006.05525. doi: 10.48550/arXiv.2006.05525. https://arxiv.org/abs/2006.05525\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport time\n\n\n# Define image transformations for training and validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Load Imagenette2-320 dataset\ndata_dir = './data/imagenette2-320/imagenette2-320'\nimage_datasets = {x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n               for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\nThis 1x1 convolution layer reduces the number of channels from 512 (layer4 output in ResNet18) to 128 (layer2 output) to ensure compatibility when comparing the feature maps from these two layers in the distillation process.\n\n# Load the pre-trained ResNet18 model (used as the student)\nstudent_model = models.resnet18(pretrained=True)\n\n# Modify the last layer to match the number of classes in Imagenette (10 classes)\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move the model to the appropriate device\nstudent_model = student_model.to(device)\n\n\n# Define a 1x1 convolution to match the number of channels\nconv1x1 = nn.Conv2d(512, 128, kernel_size=1).to(device)\n\n\n\n\nFeature Extraction Hooks\n\nHooks: Forward hooks are used to extract the intermediate feature maps from layer2 and layer4 of ResNet18 during the forward pass. The extracted features are stored in the intermediate_features dictionary.\n\n\n# Helper function to register hooks and extract intermediate features\ndef get_intermediate_features(module, input, output):\n    return output\n\n# Dictionaries to store the intermediate features during forward pass\nintermediate_features = {}\n\n# Register forward hooks to capture features from desired layers\nstudent_model.layer2[1].register_forward_hook(lambda m, i, o: intermediate_features.update({\"layer2\": o}))\nstudent_model.layer4[1].register_forward_hook(lambda m, i, o: intermediate_features.update({\"layer4\": o}))\n\n&lt;torch.utils.hooks.RemovableHandle at 0x2bb98f80f10&gt;\n\n\n\n\n\n\nCross-Entropy Loss (ce_loss): Computes the loss between the predicted class labels (logits) and the ground truth labels.\nMSE Loss (distillation_loss): Encourages the student model to match its intermediate feature maps (layer2) with those of the deeper layers (layer4) by applying Mean Squared Error.\n1x1 Convolution: Before applying MSE, the deeper layer’s output (layer4) is passed through a 1x1 convolution to match the number of channels with layer2.\nInterpolation: The student’s layer2 feature map is resized to match the spatial dimensions of the deeper layer4 features.\nAlpha: Balances the importance of distillation loss and cross-entropy loss.\n\n\n\nclass SelfDistillationLoss(nn.Module):\n    def __init__(self, alpha=0.5):\n        super(SelfDistillationLoss, self).__init__()\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.mse_loss = nn.MSELoss()\n        self.alpha = alpha  # Weight for self-distillation\n\n    def forward(self, student_logits, labels, student_intermediate, teacher_intermediate):\n        # Standard cross-entropy loss on the output logits\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        # Apply 1x1 convolution to teacher_intermediate to match the number of channels\n        teacher_intermediate_reduced = conv1x1(teacher_intermediate)\n        \n        # Resize the student_intermediate feature map to match teacher_intermediate's spatial size\n        student_intermediate_resized = F.interpolate(student_intermediate, size=teacher_intermediate_reduced.shape[2:], mode='bilinear', align_corners=False)\n        \n        # Self-distillation loss (MSE between resized student intermediate outputs and teacher intermediate outputs)\n        distillation_loss = self.mse_loss(student_intermediate_resized, teacher_intermediate_reduced)\n        \n        # Combine the two losses\n        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss\n\n\n\n\nThis function trains the student model using the self-distillation loss:\n\nTraining Loop: Iterates over the training data for a specified number of epochs (25 by default).\nForward Pass: For each batch, the model computes the outputs, and intermediate features are extracted via hooks.\nLoss Computation: The total loss is computed by combining the cross-entropy loss and the self-distillation loss.\nBackward Pass: The loss is used to perform backpropagation and update the model’s weights.\nAccuracy: Tracks the accuracy for each epoch, and the best model weights are saved.\n\n\ndef train_student(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Set the model to training mode\n        model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over the data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Forward pass through the student model\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            # Get the intermediate feature maps\n            layer2_features = intermediate_features['layer2']\n            layer4_features = intermediate_features['layer4']\n\n            # Compute the loss (self-distillation loss)\n            loss = criterion(outputs, labels, layer2_features, layer4_features)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        # Calculate epoch loss and accuracy\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Deep copy the model if it's the best so far\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = model.state_dict()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n\n\n\n# Define optimizer\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define the self-distillation loss function\ncriterion = SelfDistillationLoss(alpha=0.5)\n\n# Train the student model using self-distillation\ntrained_student = train_student(student_model, dataloaders, criterion, optimizer, num_epochs=5)\n\nEpoch 0/4\n----------\nLoss: 0.4253 Acc: 0.8775\nEpoch 1/4\n----------\nLoss: 0.2328 Acc: 0.9135\nEpoch 2/4\n----------\nLoss: 0.2063 Acc: 0.9139\nEpoch 3/4\n----------\nLoss: 0.1861 Acc: 0.9177\nEpoch 4/4\n----------\nLoss: 0.1656 Acc: 0.9272\nTraining complete in 48m 9s\nBest Acc: 0.9272\n\n\n\n\n\nThis function evaluates the trained model on the validation set, computing the accuracy by comparing predictions to ground truth labels.\n\ndef evaluate_model(model, dataloaders):\n    model.eval()\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate the trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9610"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#define-models",
    "href": "blogs/KD_Self_Distillation_V3.html#define-models",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "This 1x1 convolution layer reduces the number of channels from 512 (layer4 output in ResNet18) to 128 (layer2 output) to ensure compatibility when comparing the feature maps from these two layers in the distillation process.\n\n# Load the pre-trained ResNet18 model (used as the student)\nstudent_model = models.resnet18(pretrained=True)\n\n# Modify the last layer to match the number of classes in Imagenette (10 classes)\nnum_ftrs_student = student_model.fc.in_features\nstudent_model.fc = nn.Linear(num_ftrs_student, 10)\n\n# Move the model to the appropriate device\nstudent_model = student_model.to(device)\n\n\n# Define a 1x1 convolution to match the number of channels\nconv1x1 = nn.Conv2d(512, 128, kernel_size=1).to(device)"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#self-distillation-strategy",
    "href": "blogs/KD_Self_Distillation_V3.html#self-distillation-strategy",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "Feature Extraction Hooks\n\nHooks: Forward hooks are used to extract the intermediate feature maps from layer2 and layer4 of ResNet18 during the forward pass. The extracted features are stored in the intermediate_features dictionary.\n\n\n# Helper function to register hooks and extract intermediate features\ndef get_intermediate_features(module, input, output):\n    return output\n\n# Dictionaries to store the intermediate features during forward pass\nintermediate_features = {}\n\n# Register forward hooks to capture features from desired layers\nstudent_model.layer2[1].register_forward_hook(lambda m, i, o: intermediate_features.update({\"layer2\": o}))\nstudent_model.layer4[1].register_forward_hook(lambda m, i, o: intermediate_features.update({\"layer4\": o}))\n\n&lt;torch.utils.hooks.RemovableHandle at 0x2bb98f80f10&gt;"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#custom-self-distillation-loss",
    "href": "blogs/KD_Self_Distillation_V3.html#custom-self-distillation-loss",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "Cross-Entropy Loss (ce_loss): Computes the loss between the predicted class labels (logits) and the ground truth labels.\nMSE Loss (distillation_loss): Encourages the student model to match its intermediate feature maps (layer2) with those of the deeper layers (layer4) by applying Mean Squared Error.\n1x1 Convolution: Before applying MSE, the deeper layer’s output (layer4) is passed through a 1x1 convolution to match the number of channels with layer2.\nInterpolation: The student’s layer2 feature map is resized to match the spatial dimensions of the deeper layer4 features.\nAlpha: Balances the importance of distillation loss and cross-entropy loss.\n\n\n\nclass SelfDistillationLoss(nn.Module):\n    def __init__(self, alpha=0.5):\n        super(SelfDistillationLoss, self).__init__()\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.mse_loss = nn.MSELoss()\n        self.alpha = alpha  # Weight for self-distillation\n\n    def forward(self, student_logits, labels, student_intermediate, teacher_intermediate):\n        # Standard cross-entropy loss on the output logits\n        ce_loss = self.ce_loss(student_logits, labels)\n        \n        # Apply 1x1 convolution to teacher_intermediate to match the number of channels\n        teacher_intermediate_reduced = conv1x1(teacher_intermediate)\n        \n        # Resize the student_intermediate feature map to match teacher_intermediate's spatial size\n        student_intermediate_resized = F.interpolate(student_intermediate, size=teacher_intermediate_reduced.shape[2:], mode='bilinear', align_corners=False)\n        \n        # Self-distillation loss (MSE between resized student intermediate outputs and teacher intermediate outputs)\n        distillation_loss = self.mse_loss(student_intermediate_resized, teacher_intermediate_reduced)\n        \n        # Combine the two losses\n        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#training-loop",
    "href": "blogs/KD_Self_Distillation_V3.html#training-loop",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "This function trains the student model using the self-distillation loss:\n\nTraining Loop: Iterates over the training data for a specified number of epochs (25 by default).\nForward Pass: For each batch, the model computes the outputs, and intermediate features are extracted via hooks.\nLoss Computation: The total loss is computed by combining the cross-entropy loss and the self-distillation loss.\nBackward Pass: The loss is used to perform backpropagation and update the model’s weights.\nAccuracy: Tracks the accuracy for each epoch, and the best model weights are saved.\n\n\ndef train_student(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Set the model to training mode\n        model.train()\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over the data\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Forward pass through the student model\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            # Get the intermediate feature maps\n            layer2_features = intermediate_features['layer2']\n            layer4_features = intermediate_features['layer4']\n\n            # Compute the loss (self-distillation loss)\n            loss = criterion(outputs, labels, layer2_features, layer4_features)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        # Calculate epoch loss and accuracy\n        epoch_loss = running_loss / dataset_sizes['train']\n        epoch_acc = running_corrects.double() / dataset_sizes['train']\n\n        print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n        # Deep copy the model if it's the best so far\n        if epoch_acc &gt; best_acc:\n            best_acc = epoch_acc\n            best_model_wts = model.state_dict()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    return model"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#optimizer-and-training",
    "href": "blogs/KD_Self_Distillation_V3.html#optimizer-and-training",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "# Define optimizer\noptimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n\n# Define the self-distillation loss function\ncriterion = SelfDistillationLoss(alpha=0.5)\n\n# Train the student model using self-distillation\ntrained_student = train_student(student_model, dataloaders, criterion, optimizer, num_epochs=5)\n\nEpoch 0/4\n----------\nLoss: 0.4253 Acc: 0.8775\nEpoch 1/4\n----------\nLoss: 0.2328 Acc: 0.9135\nEpoch 2/4\n----------\nLoss: 0.2063 Acc: 0.9139\nEpoch 3/4\n----------\nLoss: 0.1861 Acc: 0.9177\nEpoch 4/4\n----------\nLoss: 0.1656 Acc: 0.9272\nTraining complete in 48m 9s\nBest Acc: 0.9272"
  },
  {
    "objectID": "blogs/KD_Self_Distillation_V3.html#evaluation",
    "href": "blogs/KD_Self_Distillation_V3.html#evaluation",
    "title": "Knowledge Distillation Implementation 3/3",
    "section": "",
    "text": "This function evaluates the trained model on the validation set, computing the accuracy by comparing predictions to ground truth labels.\n\ndef evaluate_model(model, dataloaders):\n    model.eval()\n    running_corrects = 0\n\n    for inputs, labels in dataloaders['val']:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n\n    accuracy = running_corrects.double() / dataset_sizes['val']\n    print(f'Validation Accuracy: {accuracy:.4f}')\n\n# Evaluate the trained student model\nevaluate_model(trained_student, dataloaders)\n\nValidation Accuracy: 0.9610"
  },
  {
    "objectID": "blogs/claude_vs_chatgpt.html",
    "href": "blogs/claude_vs_chatgpt.html",
    "title": "Anthropic Projects vs. OpenAI GPTs: A Comparison for LLM-Based Web Service Development",
    "section": "",
    "text": "As AI continues to evolve, organizations are seeking the best platforms to build scalable, efficient, and secure web services using large language models (LLMs). Two key players in this space are Anthropic with their Claude model and Projects feature, and OpenAI with their well-known ChatGPT and customizable GPTs. Both offer powerful tools, but they take different approaches to customization, integration, and project management.\nIn this post, we’ll explore the differences between Anthropic’s Projects and OpenAI’s GPTs, and how each platform can support the development of advanced AI-powered web services. We’ll also compare features like customization, external data integration, and scalability.\n\n\n\nWhat Are Anthropic Projects?\nAnthropic’s Projects provide a structured way to organize and manage AI usage by grouping resources, artifacts (such as files and external data), and workflows under a single environment. This setup is particularly useful for teams or developers working on large-scale AI implementations, as it helps keep tasks and resources organized for more efficient development.\nThe Projects feature allows users to: - Organize resources and workflows. - Integrate artifacts (files, data) directly into the project. - Structure different tasks or workflows within the same environment.\nThis built-in project management system is designed to simplify collaboration and the handling of external data, which is essential for efficient development in larger, more complex AI applications.\n\n\nHow Does OpenAI Compare?\nOn the other hand, OpenAI provides developers with extensive flexibility through their ChatGPT and GPTs ecosystem. GPTs allow users to customize AI models for specific tasks, all within a no-code environment, and can be shared or deployed through the GPT Store. OpenAI doesn’t have a built-in project management system like Anthropic’s Projects, but users can still organize resources, workflows, and model interactions independently, often through their APIs or external tools.\nLet’s dive into a detailed comparison of the two platforms:\n\n\n\nComparison Table: Anthropic Projects vs. OpenAI GPTs\n\n\n\nFeature\nAnthropic (Claude & Projects)\nOpenAI (ChatGPT & GPTs)\n\n\n\n\nPrimary Model\nClaude\nChatGPT (GPT-4)\n\n\nProjects\nProjects for organizing AI usage by grouping resources, artifacts, and workflows\nNo direct equivalent; GPTs and API usage managed independently with user-defined projects\n\n\nCustomization of Models\nLimited customization; structured via Projects\nGPTs allow full customization and fine-tuning for specific tasks\n\n\nArtifacts (External Data Handling)\nArtifacts for file uploads and data interaction\nFile upload + ADA (Advanced Data Analysis) for data interaction\n\n\nProject Management\nBuilt-in project organization and resource grouping\nNo built-in project grouping; managed externally via API workflows or GPT organization\n\n\nFunction Calling\nNot as prominently integrated\nFunction Calling available for external function interaction\n\n\nIntegration with External Data\nArtifacts and Projects to manage external data\nFunction Calling, File Upload, Plugins\n\n\nEase of Use (No-code Customization)\nProject organization focuses on resource grouping\nGPTs offer no-code customization for various tasks\n\n\nStore/Marketplace for Custom Models\nNot available\nGPT Store for sharing and downloading custom GPTs\n\n\nScalability\nProject-based scaling and resource management\nHigh scalability via Swarm and GPT orchestration with Function Calling\n\n\nDevelopment Ecosystem\nProject-oriented resource management and artifact handling\nExtensive ecosystem with APIs, GPT Store, and Plugins\n\n\n\n\n\n\nChoosing the Right Platform for Your Project\nThe choice between Anthropic Projects and OpenAI GPTs depends largely on your development goals and the needs of your team:\n\nAnthropic Projects are an excellent choice if you’re working on large, collaborative AI projects where resource organization and structure are essential. Its built-in management tools make it easier for teams to keep track of data and workflows.\nOpenAI GPTs are more suited for teams that require flexibility and customization. With GPT Store, no-code customization, and rich plugin integration, OpenAI offers a broad ecosystem for developers looking to build AI-powered services tailored to specific industries or use cases.\n\n\n\nKey Takeaways:\n\nCustomization: OpenAI’s GPTs provide much more flexibility and customization options compared to Anthropic, which is more focused on safety and structure.\nData Handling: Anthropic’s Artifacts and Projects offer a simple way to manage files and external data, while OpenAI provides Function Calling and advanced file handling with broader plugin support.\nScalability: OpenAI’s Swarm and GPT orchestration allow for dynamic scaling of tasks and AI models, giving developers the ability to handle high traffic and large-scale data processing.\n\n\n\n\nConclusion\nBoth platforms are powerful tools for LLM-based web service development, but they cater to different types of users. Anthropic Projects emphasize structure and safety, making them ideal for larger, organized teams. Meanwhile, OpenAI GPTs excel in customization, flexibility, and scalability, offering developers a versatile toolkit for building tailored AI solutions.\nIf your team is looking for maximum flexibility with a vibrant ecosystem of plugins and shared models, OpenAI may be the right choice. However, if your priority is maintaining a structured environment with a focus on safe AI usage, Anthropic Projects might be more aligned with your needs.\nFor more information on these platforms, explore their offerings here:\n- Anthropic Claude & Projects\n- OpenAI ChatGPT & GPTs\nHappy building!"
  },
  {
    "objectID": "blogs/pytorch_staticq.html",
    "href": "blogs/pytorch_staticq.html",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "",
    "text": "In the world of machine learning, optimizing model performance and efficiency is crucial, especially for deploying models on edge devices with limited resources. One powerful technique to achieve this is quantization, which reduces the precision of the numbers used in a model’s computations. PyTorch supports two types of quantization: dynamic and static. Dynamic quantization adjusts the precision of weights at runtime, while static quantization involves converting the model’s weights and activations to lower precision based on calibration data. This article will focus on statically quantized models, breaking down the core concepts and steps involved in PyTorch’s approach to inference with these models.\nNote: This article assumes you are already familiar with quantization, particularly static quantization. If not, I recommend checking out the some materials, e.g., our technology page for an introduction.\nfrom fastai.vision.all import *\n\nimport torch\nfrom torch.ao.quantization import get_default_qconfig_mapping\nimport torch.ao.quantization.quantize_fx as quantize_fx\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\nLet’s start by creating a Quantizer class to quantize a PyTorch model. For an introduction to PyTorch quantization, you can refer to the official documentation. As an example, I will use the Imagenette2-320 dataset and the ResNet18 model. For convenience, I will leverage the Fastai learner to streamline this process.\nclass Quantizer():\n    def __init__(self, backend=\"x86\"):\n        self.qconfig = get_default_qconfig_mapping(backend)\n        torch.backends.quantized.engine = backend\n\n    def quantize(self, model, calibration_dls):\n        x, _ = calibration_dls.valid.one_batch()\n        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n        with torch.no_grad():\n            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n\n        return model_prepared, convert_fx(model_prepared)\npath = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\ndls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\nlearn = vision_learner(dls, resnet18)\nmodel_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)\nIn static quantization, the scaling factors and zero points for weights and activations are determined after model calibration but before inference. In this context, we are using per-tensor quantization, which means that there is a single scaling factor and zero point applied uniformly across all elements in each tensor of a layer. This approach is straightforward and computationally efficient, as it simplifies the quantization process by treating the entire tensor as a whole.\nIn the above cell, model_prepared instance represents the model after it has recorded the range of activations across a validation dataset. This model contains the necessary information about the model structure and activation ranges, from which the scaling factors and zero points are calculated. Below is an example of the quantization parameters for some activations. The HistogramObserver is used to record the activation ranges. The first output shows the quantized parameters of the first activation, which is the model input, while the second output shows the quantization parameters of the second activation, which is the output of the first Conv2d + ReLU layer. In PyTorch, to avoid redundant quantization and dequantization processes between layers, batch normalization is folded into the preceding layer (batch normalization folding), and the ReLU layer is fused with the layer it follows.\n# Example activation quantization parameters\nfor i in range(3):\n    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n    scale, zero_p = attr.calculate_qparams()\n    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))\n\nHistogramObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\nScaling Factor: 0.018649335950613022\nZero Point: 114\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\nqmodel instance represents the quantized model. It contains quantized weights, along with their associated scaling factor and zero point, as well as the scaling factor and zero point for activations. Additionally, it includes some non-quantized parameters, which I will explain later.\nqmodel\n\nGraphModule(\n  (0): Module(\n    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008901300840079784, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024013830348849297, zero_point=149, padding=(1, 1))\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007031331304460764, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031252723187208176, zero_point=156, padding=(1, 1))\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007301042787730694, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019116230309009552, zero_point=124, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.01664934679865837, zero_point=135)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008282394148409367, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02566305175423622, zero_point=137, padding=(1, 1))\n      )\n    )\n    (6): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010484358295798302, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02675902470946312, zero_point=90, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008271278813481331, zero_point=162)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00832998938858509, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.027811763808131218, zero_point=142, padding=(1, 1))\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006999513134360313, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.023119885474443436, zero_point=140, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.02033478580415249, zero_point=128)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.006345659960061312, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12105856835842133, zero_point=88, padding=(1, 1))\n      )\n    )\n  )\n  (1): Module(\n    (0): Module(\n      (mp): AdaptiveMaxPool2d(output_size=1)\n      (ap): AdaptiveAvgPool2d(output_size=1)\n    )\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): QuantizedDropout(p=0.25, inplace=False)\n    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.08005672693252563, zero_point=0, qscheme=torch.per_tensor_affine)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): QuantizedDropout(p=0.5, inplace=False)\n    (8): QuantizedLinear(in_features=512, out_features=10, scale=0.10456003248691559, zero_point=150, qscheme=torch.per_tensor_affine)\n  )\n)\nLet’s investigate the first layer of qmodel, i.e., quantized Conv2d + ReLU layer.\nlayer = qmodel._modules['0']._modules['0']\nprint(layer)\nprint(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n                                                       layer.weight().q_zero_point()))\nprint(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n                                                                  layer.zero_point))\n\nprint(\"Example weights:\", layer.weight()[0, 0, 0])\nprint(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())\n\nQuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\nWeight Scale: 0.0030892190989106894, Weight Zero Point: 0\nOutput Scaling Factor: 0.011327190324664116, Output Zero Point: 0\n\nExample weights: tensor([-0.0031,  0.0000,  0.0000,  0.0185,  0.0124,  0.0031, -0.0031],\n       size=(7,), dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n       scale=0.0030892190989106894, zero_point=0)\nIn integer representation: tensor([-1,  0,  0,  6,  4,  1, -1], dtype=torch.int8)\nAs shown above, the quantized layer contains two scaling factors and zero points: one for the weights and another for the output activation. You may have noticed that the output scaling factor and zero point are the same as those displayed in the second cell above, as they represent the same activation.\nWhat about biases, which I haven’t discussed yet? In PyTorch, bias quantization depends on the backend you use. For example, if you specify the x86 backend, biases are not quantized and are instead used as floating-point values. On the other hand, the QNNPACK backend quantizes biases. However, biases are not quantized during the initial quantization stage; they are quantized at inference time. Thus, even the inference uses quantized biases, PyTorch does not display the quantized biases at before inference. The formula for bias quantization in PyTorch is: \\[\nb_q = round(b / (si * sw))\n\\] , where \\(b_q\\) is quantized bias, \\(b\\) is bias before quantization, \\(si\\) is input activation scale and \\(sw\\) is weight scale. For more details, you can refer to this discussion.\nIn addition, the model may include other non-quantized parameters, such as parameters in batch normalization layers that are not fused. This is likely because quantizing the activations in these layers would not provide significant benefits."
  },
  {
    "objectID": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "href": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "What happens during inference?",
    "text": "What happens during inference?\nThis section demonstrates how calculations are performed in the quantized model during inference. To illustrate this, I calculate the output of the first convolutional layer and validate it against the actual result.\n\nlayer_input = None\nlayer_output = None\n\ndef hook_fn(module, input, output):\n    global layer_output, layer_input\n    layer_input = input\n    layer_output = output\n\nimg = torch.rand([1, 3, 224, 224])\nhook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\noutput = qmodel(img)\nhook.remove()\nprint(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\nprint(\"Example output:\", layer_output[0,0,0,:10].int_repr())\n\nExample input: tensor([163, 119, 155, 138, 126, 164, 115, 132, 115, 166], dtype=torch.uint8)\nExample output: tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.uint8)\n\n\n\nimport numpy as np\n\ndef quantize(x, qparams, itype):\n    xtype = torch.iinfo(itype)\n    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n\ndef dequantize(x, qparams):\n    return (x - qparams[1]) * qparams[0]\n\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    N, C, H, W = input_data.shape\n    out_h = (H + 2 * pad - filter_h) // stride + 1\n    out_w = (W + 2 * pad - filter_w) // stride + 1\n\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride * out_h\n        for x in range(filter_w):\n            x_max = x + stride * out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n    return torch.tensor(col)\n\n# first use im2col, which is efficient way to perform Conv2d operation\ninp = im2col(img, 7, 7, 2, 3).float()\n# quantize input values using input scale and zero point\ninp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n# get quantized weights, weight scale and quantize biases\nw = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\nsw = qmodel._modules['0']._modules['0'].weight().q_scale()\nb = quantize(qmodel._modules['0']._modules['0'].bias(),\n             [layer_input[0].q_scale() * sw, 0], torch.int32)\nb = b.reshape(1,64,1,1).detach()\n# calculate matmul in Conv2d and add biases\nout = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n# dequantize, perform ReLU and quantize based on output scale and zero point\nout = out * sw * layer_input[0].q_scale()\nout = torch.relu(out)\nout = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)\n\n\ntorch.allclose(out, layer_output.int_repr().float())\nprint(\"Output: \", out[0, 0, 0, :10])\n\nOutput:  tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n\nOur calculation matches the actual result, which is a good sign. Although some operations in PyTorch’s implementation might be performed in a different order, the overall process is likely very similar."
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "# !pip install torch torchvision\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\n\n\n\nfrom torchvision.datasets.utils import download_and_extract_archive\n\n# Download Imagenette2-320\nurl = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\nroot = \"./data\"\ndownload_and_extract_archive(url, download_root=root)\n\n# Set dataset path\ndataset_path = os.path.join(root, \"imagenette2-320\")\n\nDownloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz to ./data\\imagenette2-320.tgz\n\n\n\n\n\nExtracting ./data\\imagenette2-320.tgz to ./data\n\n\n\n\n\nWe need to prepare our images for the model. We do this by resizing them to a uniform size and converting them into tensors, which is a format that PyTorch can understand.\n\n# Define transformations with data augmentation\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),          # Resize images to 224x224\n    transforms.RandomHorizontalFlip(),      # Augment data with random horizontal flips\n    transforms.RandomRotation(10),          # Augment data with random rotation\n    transforms.ToTensor(),                  # Convert images to PyTorch tensors\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalization\n])\n\n\n\n\n\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=transform)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n\n\n\n\nTo apply SVD, we will take a batch of images from the DataLoader, flatten them, and perform SVD on the flattened matrix.\nThink of each image as a large grid of numbers. We want to compress these images while keeping the most important information. SVD breaks the image into three components that can be used to find a reduced version that still contains the important features.\n\nFlatten Images: We reshape each image into a long row of numbers.\nSVD: We apply SVD to the flattened images to find patterns in the data.\nRetain Top k Features: We keep the top 100 components, which means we are compressing the data but keeping the main information.\n\n\n# Get a batch of images and labels\nimages, labels = next(iter(train_loader))\n\n# Flatten images to apply SVD\nn_samples, c, h, w = images.shape\nimages_flat = images.view(n_samples, -1)  # Shape: (n_samples, c*h*w)\n\n# Apply SVD\nU, S, V = torch.svd(images_flat)\n\n# Retain top k singular values for dimensionality reduction\nk = 100  # Choose top k components\nU_reduced = U[:, :k]  # Shape: (n_samples, k)\n\nprint(f'Original shape: {images_flat.shape}')\nprint(f'Reduced shape after SVD: {U_reduced.shape}')\n\nOriginal shape: torch.Size([32, 150528])\nReduced shape after SVD: torch.Size([32, 32])\n\n\n\n\n\nNow that we have reduced features using SVD, let’s train a simple feed-forward neural network on these features.\n\n\n\n\nclass SVDImageDataset(torch.utils.data.Dataset):\n    def __init__(self, loader, k):\n        self.loader = loader\n        self.data = []\n        self.labels = []\n        self.k = k  # Number of singular values/components to retain\n\n        # Process each batch\n        for images, labels in loader:\n            for i in range(images.size(0)):\n                # Flatten the image to 1D\n                image_flat = images[i].view(-1).float()\n\n                # Make the flattened image a 2D matrix for SVD application\n                image_flat_matrix = image_flat.unsqueeze(0)  # Shape: (1, flattened_length)\n\n                # Apply SVD on the image, consider it as a 1-row matrix\n                U, S, V = torch.svd(image_flat_matrix)\n\n                # Keep only the top `k` components (the first `k` values of U)\n                if U.shape[1] &gt;= k:\n                    U_reduced = U[:, :self.k]  # Shape will be (1, k)\n                else:\n                    # If `U` has fewer components than `k`, pad it to ensure consistency\n                    U_reduced = torch.cat([U, torch.zeros((U.shape[0], k - U.shape[1]))], dim=1)\n\n                # Store the reduced representation\n                self.data.append(U_reduced.view(-1))  # Flatten the matrix to have a size (k,)\n                self.labels.append(labels[i])\n\n        # Stack data and labels to create the dataset\n        self.data = torch.stack(self.data)  # Shape: (number_of_images, k)\n        self.labels = torch.tensor(self.labels)  # Shape: (number_of_images,)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Set the number of components to retain\nk = 100\n\n# Create reduced dataset using SVD\nsvd_train_dataset = SVDImageDataset(train_loader, k)\nsvd_valid_dataset = SVDImageDataset(valid_loader, k)\n\n# Create DataLoaders\nsvd_train_loader = DataLoader(svd_train_dataset, batch_size=32, shuffle=True)\nsvd_valid_loader = DataLoader(svd_valid_dataset, batch_size=32, shuffle=False)\n\n\n\n\nThe model is a simple neural network with just a few layers. It takes the reduced data as input and learns to classify the images.\n\nSimpleClassifier: A small neural network with a couple of layers.\nLoss Function & Optimizer: We use cross-entropy loss and Adam optimizer to train the model.\n\n\n# Define a simple fully connected neural network for classification\nclass SimpleClassifier(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(SimpleClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize model, loss function, and optimizer\ninput_size = k  # We reduced the original dimensions to k with SVD\nnum_classes = len(train_dataset.classes)\nmodel = SimpleClassifier(input_size, num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n\n\n# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for features, labels in svd_train_loader:\n        # Move features and labels to the appropriate device\n        features, labels = features.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(svd_train_loader):.4f}')\n\nprint(\"Training complete.\")\n\nEpoch [1/100], Loss: 2.3018\nEpoch [2/100], Loss: 2.3018\nEpoch [3/100], Loss: 2.3018\nEpoch [4/100], Loss: 2.3018\nEpoch [5/100], Loss: 2.3018\nEpoch [6/100], Loss: 2.3018\nEpoch [7/100], Loss: 2.3018\nEpoch [8/100], Loss: 2.3018\nEpoch [9/100], Loss: 2.3018\nEpoch [10/100], Loss: 2.3018\nEpoch [11/100], Loss: 2.3018\nEpoch [12/100], Loss: 2.3018\nEpoch [13/100], Loss: 2.3018\nEpoch [14/100], Loss: 2.3018\nEpoch [15/100], Loss: 2.3018\nEpoch [16/100], Loss: 2.3018\nEpoch [17/100], Loss: 2.3017\nEpoch [18/100], Loss: 2.3018\nEpoch [19/100], Loss: 2.3018\nEpoch [20/100], Loss: 2.3017\nEpoch [21/100], Loss: 2.3018\nEpoch [22/100], Loss: 2.3018\nEpoch [23/100], Loss: 2.3018\nEpoch [24/100], Loss: 2.3018\nEpoch [25/100], Loss: 2.3018\nEpoch [26/100], Loss: 2.3018\nEpoch [27/100], Loss: 2.3017\nEpoch [28/100], Loss: 2.3018\nEpoch [29/100], Loss: 2.3018\nEpoch [30/100], Loss: 2.3017\nEpoch [31/100], Loss: 2.3018\nEpoch [32/100], Loss: 2.3018\nEpoch [33/100], Loss: 2.3017\nEpoch [34/100], Loss: 2.3018\nEpoch [35/100], Loss: 2.3018\nEpoch [36/100], Loss: 2.3018\nEpoch [37/100], Loss: 2.3017\nEpoch [38/100], Loss: 2.3018\nEpoch [39/100], Loss: 2.3018\nEpoch [40/100], Loss: 2.3018\nEpoch [41/100], Loss: 2.3018\nEpoch [42/100], Loss: 2.3018\nEpoch [43/100], Loss: 2.3018\nEpoch [44/100], Loss: 2.3018\nEpoch [45/100], Loss: 2.3018\nEpoch [46/100], Loss: 2.3018\nEpoch [47/100], Loss: 2.3017\nEpoch [48/100], Loss: 2.3018\nEpoch [49/100], Loss: 2.3018\nEpoch [50/100], Loss: 2.3017\nEpoch [51/100], Loss: 2.3018\nEpoch [52/100], Loss: 2.3018\nEpoch [53/100], Loss: 2.3018\nEpoch [54/100], Loss: 2.3017\nEpoch [55/100], Loss: 2.3018\nEpoch [56/100], Loss: 2.3018\nEpoch [57/100], Loss: 2.3018\nEpoch [58/100], Loss: 2.3018\nEpoch [59/100], Loss: 2.3018\nEpoch [60/100], Loss: 2.3018\nEpoch [61/100], Loss: 2.3018\nEpoch [62/100], Loss: 2.3017\nEpoch [63/100], Loss: 2.3018\nEpoch [64/100], Loss: 2.3018\nEpoch [65/100], Loss: 2.3017\nEpoch [66/100], Loss: 2.3018\nEpoch [67/100], Loss: 2.3018\nEpoch [68/100], Loss: 2.3018\nEpoch [69/100], Loss: 2.3018\nEpoch [70/100], Loss: 2.3018\nEpoch [71/100], Loss: 2.3018\nEpoch [72/100], Loss: 2.3018\nEpoch [73/100], Loss: 2.3018\nEpoch [74/100], Loss: 2.3018\nEpoch [75/100], Loss: 2.3018\nEpoch [76/100], Loss: 2.3017\nEpoch [77/100], Loss: 2.3018\nEpoch [78/100], Loss: 2.3017\nEpoch [79/100], Loss: 2.3018\nEpoch [80/100], Loss: 2.3017\nEpoch [81/100], Loss: 2.3018\nEpoch [82/100], Loss: 2.3018\nEpoch [83/100], Loss: 2.3017\nEpoch [84/100], Loss: 2.3018\nEpoch [85/100], Loss: 2.3018\nEpoch [86/100], Loss: 2.3017\nEpoch [87/100], Loss: 2.3017\nEpoch [88/100], Loss: 2.3018\nEpoch [89/100], Loss: 2.3018\nEpoch [90/100], Loss: 2.3018\nEpoch [91/100], Loss: 2.3018\nEpoch [92/100], Loss: 2.3018\nEpoch [93/100], Loss: 2.3018\nEpoch [94/100], Loss: 2.3017\nEpoch [95/100], Loss: 2.3018\nEpoch [96/100], Loss: 2.3018\nEpoch [97/100], Loss: 2.3018\nEpoch [98/100], Loss: 2.3017\nEpoch [99/100], Loss: 2.3018\nEpoch [100/100], Loss: 2.3018\nTraining complete.\n\n\n\n\n\n\n #Validation loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for features, labels in svd_valid_loader:\n        outputs = model(features)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Validation Accuracy: {100 * correct / total:.2f}%')\n\nValidation Accuracy: 9.10%"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#singular-value-decomposition-svd---dimensionality-reduction-single-image-pytorch",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#singular-value-decomposition-svd---dimensionality-reduction-single-image-pytorch",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "# !pip install torch torchvision\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom PIL import Image\nimport os"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#download-and-load-the-dataset",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#download-and-load-the-dataset",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "from torchvision.datasets.utils import download_and_extract_archive\n\n# Download Imagenette2-320\nurl = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\nroot = \"./data\"\ndownload_and_extract_archive(url, download_root=root)\n\n# Set dataset path\ndataset_path = os.path.join(root, \"imagenette2-320\")\n\nDownloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz to ./data\\imagenette2-320.tgz\n\n\n\n\n\nExtracting ./data\\imagenette2-320.tgz to ./data"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#define-data-transformations",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#define-data-transformations",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "We need to prepare our images for the model. We do this by resizing them to a uniform size and converting them into tensors, which is a format that PyTorch can understand.\n\n# Define transformations with data augmentation\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),          # Resize images to 224x224\n    transforms.RandomHorizontalFlip(),      # Augment data with random horizontal flips\n    transforms.RandomRotation(10),          # Augment data with random rotation\n    transforms.ToTensor(),                  # Convert images to PyTorch tensors\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalization\n])"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#load-dataset-with-dataloader",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#load-dataset-with-dataloader",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "# Load datasets\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=transform)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#perform-svd-for-dimensionality-reduction",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#perform-svd-for-dimensionality-reduction",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "To apply SVD, we will take a batch of images from the DataLoader, flatten them, and perform SVD on the flattened matrix.\nThink of each image as a large grid of numbers. We want to compress these images while keeping the most important information. SVD breaks the image into three components that can be used to find a reduced version that still contains the important features.\n\nFlatten Images: We reshape each image into a long row of numbers.\nSVD: We apply SVD to the flattened images to find patterns in the data.\nRetain Top k Features: We keep the top 100 components, which means we are compressing the data but keeping the main information.\n\n\n# Get a batch of images and labels\nimages, labels = next(iter(train_loader))\n\n# Flatten images to apply SVD\nn_samples, c, h, w = images.shape\nimages_flat = images.view(n_samples, -1)  # Shape: (n_samples, c*h*w)\n\n# Apply SVD\nU, S, V = torch.svd(images_flat)\n\n# Retain top k singular values for dimensionality reduction\nk = 100  # Choose top k components\nU_reduced = U[:, :k]  # Shape: (n_samples, k)\n\nprint(f'Original shape: {images_flat.shape}')\nprint(f'Reduced shape after SVD: {U_reduced.shape}')\n\nOriginal shape: torch.Size([32, 150528])\nReduced shape after SVD: torch.Size([32, 32])"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#train-a-simple-neural-network-on-svd-reduced-features",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#train-a-simple-neural-network-on-svd-reduced-features",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "Now that we have reduced features using SVD, let’s train a simple feed-forward neural network on these features."
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#define-custom-dataset",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#define-custom-dataset",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "class SVDImageDataset(torch.utils.data.Dataset):\n    def __init__(self, loader, k):\n        self.loader = loader\n        self.data = []\n        self.labels = []\n        self.k = k  # Number of singular values/components to retain\n\n        # Process each batch\n        for images, labels in loader:\n            for i in range(images.size(0)):\n                # Flatten the image to 1D\n                image_flat = images[i].view(-1).float()\n\n                # Make the flattened image a 2D matrix for SVD application\n                image_flat_matrix = image_flat.unsqueeze(0)  # Shape: (1, flattened_length)\n\n                # Apply SVD on the image, consider it as a 1-row matrix\n                U, S, V = torch.svd(image_flat_matrix)\n\n                # Keep only the top `k` components (the first `k` values of U)\n                if U.shape[1] &gt;= k:\n                    U_reduced = U[:, :self.k]  # Shape will be (1, k)\n                else:\n                    # If `U` has fewer components than `k`, pad it to ensure consistency\n                    U_reduced = torch.cat([U, torch.zeros((U.shape[0], k - U.shape[1]))], dim=1)\n\n                # Store the reduced representation\n                self.data.append(U_reduced.view(-1))  # Flatten the matrix to have a size (k,)\n                self.labels.append(labels[i])\n\n        # Stack data and labels to create the dataset\n        self.data = torch.stack(self.data)  # Shape: (number_of_images, k)\n        self.labels = torch.tensor(self.labels)  # Shape: (number_of_images,)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Set the number of components to retain\nk = 100\n\n# Create reduced dataset using SVD\nsvd_train_dataset = SVDImageDataset(train_loader, k)\nsvd_valid_dataset = SVDImageDataset(valid_loader, k)\n\n# Create DataLoaders\nsvd_train_loader = DataLoader(svd_train_dataset, batch_size=32, shuffle=True)\nsvd_valid_loader = DataLoader(svd_valid_dataset, batch_size=32, shuffle=False)"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#define-neural-network",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#define-neural-network",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "The model is a simple neural network with just a few layers. It takes the reduced data as input and learns to classify the images.\n\nSimpleClassifier: A small neural network with a couple of layers.\nLoss Function & Optimizer: We use cross-entropy loss and Adam optimizer to train the model.\n\n\n# Define a simple fully connected neural network for classification\nclass SimpleClassifier(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(SimpleClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize model, loss function, and optimizer\ninput_size = k  # We reduced the original dimensions to k with SVD\nnum_classes = len(train_dataset.classes)\nmodel = SimpleClassifier(input_size, num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)"
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#train-the-model",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#train-the-model",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for features, labels in svd_train_loader:\n        # Move features and labels to the appropriate device\n        features, labels = features.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(svd_train_loader):.4f}')\n\nprint(\"Training complete.\")\n\nEpoch [1/100], Loss: 2.3018\nEpoch [2/100], Loss: 2.3018\nEpoch [3/100], Loss: 2.3018\nEpoch [4/100], Loss: 2.3018\nEpoch [5/100], Loss: 2.3018\nEpoch [6/100], Loss: 2.3018\nEpoch [7/100], Loss: 2.3018\nEpoch [8/100], Loss: 2.3018\nEpoch [9/100], Loss: 2.3018\nEpoch [10/100], Loss: 2.3018\nEpoch [11/100], Loss: 2.3018\nEpoch [12/100], Loss: 2.3018\nEpoch [13/100], Loss: 2.3018\nEpoch [14/100], Loss: 2.3018\nEpoch [15/100], Loss: 2.3018\nEpoch [16/100], Loss: 2.3018\nEpoch [17/100], Loss: 2.3017\nEpoch [18/100], Loss: 2.3018\nEpoch [19/100], Loss: 2.3018\nEpoch [20/100], Loss: 2.3017\nEpoch [21/100], Loss: 2.3018\nEpoch [22/100], Loss: 2.3018\nEpoch [23/100], Loss: 2.3018\nEpoch [24/100], Loss: 2.3018\nEpoch [25/100], Loss: 2.3018\nEpoch [26/100], Loss: 2.3018\nEpoch [27/100], Loss: 2.3017\nEpoch [28/100], Loss: 2.3018\nEpoch [29/100], Loss: 2.3018\nEpoch [30/100], Loss: 2.3017\nEpoch [31/100], Loss: 2.3018\nEpoch [32/100], Loss: 2.3018\nEpoch [33/100], Loss: 2.3017\nEpoch [34/100], Loss: 2.3018\nEpoch [35/100], Loss: 2.3018\nEpoch [36/100], Loss: 2.3018\nEpoch [37/100], Loss: 2.3017\nEpoch [38/100], Loss: 2.3018\nEpoch [39/100], Loss: 2.3018\nEpoch [40/100], Loss: 2.3018\nEpoch [41/100], Loss: 2.3018\nEpoch [42/100], Loss: 2.3018\nEpoch [43/100], Loss: 2.3018\nEpoch [44/100], Loss: 2.3018\nEpoch [45/100], Loss: 2.3018\nEpoch [46/100], Loss: 2.3018\nEpoch [47/100], Loss: 2.3017\nEpoch [48/100], Loss: 2.3018\nEpoch [49/100], Loss: 2.3018\nEpoch [50/100], Loss: 2.3017\nEpoch [51/100], Loss: 2.3018\nEpoch [52/100], Loss: 2.3018\nEpoch [53/100], Loss: 2.3018\nEpoch [54/100], Loss: 2.3017\nEpoch [55/100], Loss: 2.3018\nEpoch [56/100], Loss: 2.3018\nEpoch [57/100], Loss: 2.3018\nEpoch [58/100], Loss: 2.3018\nEpoch [59/100], Loss: 2.3018\nEpoch [60/100], Loss: 2.3018\nEpoch [61/100], Loss: 2.3018\nEpoch [62/100], Loss: 2.3017\nEpoch [63/100], Loss: 2.3018\nEpoch [64/100], Loss: 2.3018\nEpoch [65/100], Loss: 2.3017\nEpoch [66/100], Loss: 2.3018\nEpoch [67/100], Loss: 2.3018\nEpoch [68/100], Loss: 2.3018\nEpoch [69/100], Loss: 2.3018\nEpoch [70/100], Loss: 2.3018\nEpoch [71/100], Loss: 2.3018\nEpoch [72/100], Loss: 2.3018\nEpoch [73/100], Loss: 2.3018\nEpoch [74/100], Loss: 2.3018\nEpoch [75/100], Loss: 2.3018\nEpoch [76/100], Loss: 2.3017\nEpoch [77/100], Loss: 2.3018\nEpoch [78/100], Loss: 2.3017\nEpoch [79/100], Loss: 2.3018\nEpoch [80/100], Loss: 2.3017\nEpoch [81/100], Loss: 2.3018\nEpoch [82/100], Loss: 2.3018\nEpoch [83/100], Loss: 2.3017\nEpoch [84/100], Loss: 2.3018\nEpoch [85/100], Loss: 2.3018\nEpoch [86/100], Loss: 2.3017\nEpoch [87/100], Loss: 2.3017\nEpoch [88/100], Loss: 2.3018\nEpoch [89/100], Loss: 2.3018\nEpoch [90/100], Loss: 2.3018\nEpoch [91/100], Loss: 2.3018\nEpoch [92/100], Loss: 2.3018\nEpoch [93/100], Loss: 2.3018\nEpoch [94/100], Loss: 2.3017\nEpoch [95/100], Loss: 2.3018\nEpoch [96/100], Loss: 2.3018\nEpoch [97/100], Loss: 2.3018\nEpoch [98/100], Loss: 2.3017\nEpoch [99/100], Loss: 2.3018\nEpoch [100/100], Loss: 2.3018\nTraining complete."
  },
  {
    "objectID": "blogs/SVD_Dimensionality_Reduction_V3.html#evaluate-the-model",
    "href": "blogs/SVD_Dimensionality_Reduction_V3.html#evaluate-the-model",
    "title": "Low Rank Approximation Implementation 3/4",
    "section": "",
    "text": "#Validation loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for features, labels in svd_valid_loader:\n        outputs = model(features)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Validation Accuracy: {100 * correct / total:.2f}%')\n\nValidation Accuracy: 9.10%"
  },
  {
    "objectID": "blogs/tinyruntime.html",
    "href": "blogs/tinyruntime.html",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Performance comparison with PyTorch. For more details, visit https://ninjalabo.ai/performance.html.\n\n\nIn the evolving landscape of machine learning, efficient model deployment is paramount, especially in resource-limited environments. That’s where tinyRuntime comes in—a lightweight runtime designed to facilitate the fast inference of compressed ML models. Current tinyRuntime performance compared to PyTorch can be checked on our performance page.\n\n\nWritten in C, tinyRuntime is engineered for optimal performance on arm64 and x86 architectures. It addresses the growing need for a runtime that operates efficiently with minimal dependencies, making it ideal for applications where memory and computational resources are constrained.\n\n\n\n\nFast: tinyRuntime is optimized for speed, enabling quick model inference that can meet the demands of real-time applications.\nLightweight: With significantly lower memory requirements compared to traditional frameworks like PyTorch, tinyRuntime ensures that even devices with limited resources can effectively run ML models.\nScalable: Its design allows for scalability across various applications and use cases, making it adaptable to different industry needs.\n\n\n\n\nWhile tinyRuntime offers significant advantages, it does come with its own set of challenges:\n\nMaintenance Effort: The custom implementation of tinyRuntime necessitates ongoing effort to maintain and update the codebase, ensuring it remains efficient and relevant.\nHardware Support: Providing broad support for diverse hardware platforms and various ML architectures can be challenging, necessitating continuous optimization.\n\n\n\n\nMoving forward, our focus will be on:\n\nContinued Optimization: We will work on enhancing performance and expanding support for different hardware and ML architectures, ensuring tinyRuntime remains competitive and efficient.\nExploring Alternatives: We are considering exploring other runtimes like IREE, which already supports a variety of hardware. This could provide insights and solutions that may benefit tinyRuntime’s development.\n\n\n\n\ntinyRuntime is paving the way for efficient ML model inference in resource-constrained environments. With its fast performance and lightweight design, it can be an option for developers and organizations looking to deploy machine learning solutions effectively. We invite you to follow our journey as we continue to enhance tinyRuntime and explore new possibilities in the world of machine learning."
  },
  {
    "objectID": "blogs/tinyruntime.html#what-is-tinyruntime",
    "href": "blogs/tinyruntime.html#what-is-tinyruntime",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Written in C, tinyRuntime is engineered for optimal performance on arm64 and x86 architectures. It addresses the growing need for a runtime that operates efficiently with minimal dependencies, making it ideal for applications where memory and computational resources are constrained."
  },
  {
    "objectID": "blogs/tinyruntime.html#key-benefits-of-tinyruntime",
    "href": "blogs/tinyruntime.html#key-benefits-of-tinyruntime",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Fast: tinyRuntime is optimized for speed, enabling quick model inference that can meet the demands of real-time applications.\nLightweight: With significantly lower memory requirements compared to traditional frameworks like PyTorch, tinyRuntime ensures that even devices with limited resources can effectively run ML models.\nScalable: Its design allows for scalability across various applications and use cases, making it adaptable to different industry needs."
  },
  {
    "objectID": "blogs/tinyruntime.html#challenges-ahead",
    "href": "blogs/tinyruntime.html#challenges-ahead",
    "title": "tinyRuntime review",
    "section": "",
    "text": "While tinyRuntime offers significant advantages, it does come with its own set of challenges:\n\nMaintenance Effort: The custom implementation of tinyRuntime necessitates ongoing effort to maintain and update the codebase, ensuring it remains efficient and relevant.\nHardware Support: Providing broad support for diverse hardware platforms and various ML architectures can be challenging, necessitating continuous optimization."
  },
  {
    "objectID": "blogs/tinyruntime.html#next-steps",
    "href": "blogs/tinyruntime.html#next-steps",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Moving forward, our focus will be on:\n\nContinued Optimization: We will work on enhancing performance and expanding support for different hardware and ML architectures, ensuring tinyRuntime remains competitive and efficient.\nExploring Alternatives: We are considering exploring other runtimes like IREE, which already supports a variety of hardware. This could provide insights and solutions that may benefit tinyRuntime’s development."
  },
  {
    "objectID": "blogs/tinyruntime.html#conclusion",
    "href": "blogs/tinyruntime.html#conclusion",
    "title": "tinyRuntime review",
    "section": "",
    "text": "tinyRuntime is paving the way for efficient ML model inference in resource-constrained environments. With its fast performance and lightweight design, it can be an option for developers and organizations looking to deploy machine learning solutions effectively. We invite you to follow our journey as we continue to enhance tinyRuntime and explore new possibilities in the world of machine learning."
  }
]