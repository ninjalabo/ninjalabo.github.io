[
  {
    "objectID": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "href": "docs/tinymlaas/Software-Project-Summer-Kick-off.html",
    "title": "Project starting point",
    "section": "",
    "text": "This was a successor of the previous software project in Winter. Here are the info:\n\nVideo Sprint3 demo, Mid\nVideo Sprint7 demo, Final\nSource code in GH Repo\nProject documentation, generated automatically\n\n\n\n\nimage.png\n\n\n     \n\nGoal for summer 2023 project\nRight now there’s no clear boundary between UI and its backend. We want to make them separted. The current Streamlit UI should be a pure frontend. The backend logic should be a REST backend server (e.g. fast API) Finall we want a CLI tool to control in additon to the current UI. A CLI tool should do the exact same things as the UI does right now.\n$ tmlaas device list\n&lt;list device name&gt;\n$ tmlaas model list\n&lt;list device name&gt;\n$ tmlass device=&lt;device id&gt; install model=&lt;model id&gt;\nYou may want to refer to this project as CLI example, https://ghapi.fast.ai/\n\n\nDevelopment environment\n\nSCRUM, User story mapping to set common goals with all stakeholders.\nNbdev, Jupyter notebook framework for code, (unit)tests & doc at once, Nbdev tutorial video.\nDocker compose to run the whole system at once, turorial video.\nAcceptance Test Driven Development (ATDD) to sync up with a client.\nStreamlit for UI framework used in this project.\nGH Project as Kanban\nGH Workflow as CI/CD\n\n\n\nCommunication\n\nDiscord, Click to join.\n\n\n\nNext\n\nWho’s SCRUM master for Sprint1?\nSprint1 (Week20)\n\nInitial research of this project by Students\n\nSprint1 review & planning at 10:00AM 22nd May\n\nReview WoW proposal from students\nQ&A for a client\nPrioritize user story?\n\nWhich Kanban board to share with customer?",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Project starting point"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Use_cases.html",
    "href": "docs/tinymlaas/Use_cases.html",
    "title": "Use case scenarios",
    "section": "",
    "text": "As a security-conscious homeowner, I want to use TinyML(*) for human-detection to monitor my home and receive alerts when unexpected activity is detected, so that I can take action to ensure my home is safe.\n\nData Collection: User collects video footage from their smart camera, which will be used to train the TinyML model for human-detection.\nModel Training: User trains a TinyML human-detection model using the collected video footage and publicly available datasets.\nModel Squeezing: User optimizes the model size to ensure it can be deployed on TinyML-enabled devices.\nModel Deployment: User deploys the optimized TinyML model on their smart camera.\nInference: The smart camera uses the deployed TinyML model to perform real-time human-detection, identifying humans and activities within its field of view.\nAlerts: If the camera detects unusual activity, such as a person entering the home when no one is expected to be there, it sends alerts to the user’s smartphone.\nModel Update: User periodically updates the TinyML model with new data to ensure its accuracy and improve its performance over time.\nUser Access: User can access real-time video footage and receive notifications to confirm the alert and take appropriate action, such as contacting the police or checking in on the home from a remote location.\n\n(*) The alternative to TinyML would be running human detection in the cloud, with all the possible network latency issue we may face. TinyML-based human-detection can trigger an alarm installed “in place” where the camera is. By relying on cloud, we would need to transfer the video footage to the cloud and then running inference at there.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Use case scenarios"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html",
    "href": "docs/tinymlaas/TinyML-frontend_README.html",
    "title": "TinyML-frontend",
    "section": "",
    "text": "GitHub Actions\n\n\nfrontend for TinyMLaaS.\n\n\nDepends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils\n\n\n\nThe frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.\n\n\n\nRun backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py\n\n\n\nThis project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#dependecies",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Depends on package usbutils.\nOn Debian-based systems install it with:\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#usb-detection",
    "title": "TinyML-frontend",
    "section": "",
    "text": "The frontend uses the external command lsusb to find suitable usb-devices.\nThis means that the frontend can’t natively find usb devices on windows and needs to be run in a docker container for this feature.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#running",
    "title": "TinyML-frontend",
    "section": "",
    "text": "Run backend from this repository\nActivate virtual environment with:\nsource /venv/bin/activate\nInstall dependencies with:\npip install requirements.txt\nCreate an .env file in frontend root directory that points to backend:\nBACKEND_URL = \"http://localhost:8000\"\nRun frontend with:\nstreamlit run TinyMLaaS.py",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-frontend_README.html#testing",
    "title": "TinyML-frontend",
    "section": "",
    "text": "This project uses Robot Framework to run end-to-end testing. For testing you need to have both the backend and frontend running. Before running frontend, environment variable ROBOT_TESTS should be set to true. On bash, you can do that with\nROBOT_TESTS=true && export ROBOT_TESTS\nThis makes it that the robot tests don’t access actual usb-devices, but rather use sepcifically defined mock data.\nIn the backend you will need to have enough test data in the database to run the robot tests. You can set up the database in the backend folder with\ntouch tiny_mlaas.db\nsqlite3 tiny_mlaas.db &lt; schema.sql\nsqlite3 tiny_mlaas.db &lt; populate.sql\nRun Robot Framework tests with:\nrobot -d robot_output tests/\nThe -d flag directs the robot test outputs, which can be quite generous, to a named folder.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-frontend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html",
    "href": "docs/tinymlaas/Technologies.html",
    "title": "Technological choices",
    "section": "",
    "text": "This document is meant to answer why certain technological choices have been made and why certain frameworks have been chosen.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#main",
    "href": "docs/tinymlaas/Technologies.html#main",
    "title": "Technological choices",
    "section": "Main",
    "text": "Main\nThe TinyMLaaS-main -repository is meant for mostly building the whole software with docker and documentation. However, some development is also done in this repository. All tensorflow modules are coded here in Jupyter notebooks.\n\nJupyter notebooks and NBDEV\nAs jupyter notebooks can not be used as modules in python, they need to be exported into python modules. This is done with NBDEV. Nbdev also automatically creates documentation from the jupyter notebooks and deploys them to Github pages\n\n\nDocker\nRunning the software is meant to be done with docker. Docker allows running the software on different computers, without the software being platform spesific. Also, all the dependencies required for the software do not need to be installed on the host, rather, they will all be installed in the seperate docker container. If you are not familiar with docker, check out University of Helsinkis course Devops with Docker materials to get a basic understanding of docker.\n\nSysbox\nThere are parts of the software that require starting their own docker containers. For example, the relay will start containers to compile arduino sketches and to install them to the devices. When running the relay itself inside a docker container, there are a few ways of starting a new docker container from this docker container. First, is by using so called sibling containers. This gives the docker container access to the host machines docker socket, which allows it to control other containers on the host machine. However, this has a big security flaw, as if someone gets access to this docker container, they will be able to control all other docker containers on the host machine and start their own containers. The other way is by using docker inside docker, which allows docker containers to be started inside the docker container recursively. This approach requires privileged mode to be set for the docker container, meaning that it will have root privileges on the host machine. In order to run docker machines without privileged mode, Sysbox runtime is used. This allows starting docker containers inside the docker containers without having the docker container in privileged mode, which is a lot more secure and allows for better isolation of the docker containers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#frontend",
    "href": "docs/tinymlaas/Technologies.html#frontend",
    "title": "Technological choices",
    "section": "Frontend",
    "text": "Frontend\n\nStreamlit\nThe frontend of the software is build with Streamlit. This is done to make the development process faster, as this frontend is mainly meant for demo purposes. Streamlit makes it easy to create good looking websites, however, there isn’t much room for cutomization and some features can be quite difficult to create.\nThere is still a dependecy on usbutils. This will be talked more about in the Bridge-section",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#backend",
    "href": "docs/tinymlaas/Technologies.html#backend",
    "title": "Technological choices",
    "section": "Backend",
    "text": "Backend\nThe backend is the heart of the software. It does all the communication between all the other modules and does a lot of the heavy lifting of the software. It is created as a API.\n\nFastAPI\nThe backend is created with FastAPI. FastAPI is very powerful for creating API:s, as it has great data validation with the help of Pydantic, it automatically creates good documentation about the different API requests and is simple to understand. To checkout more, read the Starting documentation.\nWhen deploying the API to production, the api will most likely be behind a proxy with some URL that has prefixes. For example, it might be deployed to example.uri.com/api/. For the API to function correctly, the root-path of the API needs to be declared for FastAPI, in this case, /api/.\n\n\nSQLAlchemy\nThe backend talks with the database with sqlalchemy. This means that it is able to talk with any SQL-database without any changes to the backends software.\nAs of now, the database in use is sqlite. However, this is meant to be more of a temporary solution to make development easier. For more information, checkout the suggestions in Suggestions for further development",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#bridge",
    "href": "docs/tinymlaas/Technologies.html#bridge",
    "title": "Technological choices",
    "section": "Bridge",
    "text": "Bridge\nThe relay is the part of the software to which the microcontrollers are connected to. It is also done in API style.\n\nFlask\nUnlike the backend, the bridge is created with Flask. Flask is lightweight and easy to understand, which makes sence for the bridge, as the hardware, on which the bridge runs, might not be that powerfull.\n\n\nUsbutils\nTo find USB-devices, the software does not use pythons libraries, such as PyUSB. This is because these softwares also have OS dependencies, that need to be installed and do not work that well in docker containers. USButils works great in contianers and is easy to install, which is why it has been chosen over pythons own libraries.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Technologies.html#command-line-interface",
    "href": "docs/tinymlaas/Technologies.html#command-line-interface",
    "title": "Technological choices",
    "section": "Command-line interface",
    "text": "Command-line interface\nAn API client is automatically generated from the OpenAPI definition provided by FastAPI. The generation is done with OpenAPI Generator. A command line tool interfacing the autogenerated client is built with Typer.\n\nOpen API Generator\nOpenAPI Generator enables building extensive Python clients with documentation. Generating a client makes it easy to design customized workflows around the API. This project uses the client as the main component of the command-line interface. The autogeneration also builds templates for tests. More generally using a generator was a way to test automatic code generation.\n\n\nTyper\nTyper is an easy to use Python library for building command-line interfaces. Typer can be used to build light weight CLI’s so it’s a good fit for the autogenerated client. Typer also uses Python type hints and provides automatic help functions that include required arguments and a command’s description from docstrings.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Technological choices"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html",
    "href": "docs/tinymlaas/TinyML-backend_README.html",
    "title": "TinyML-backend",
    "section": "",
    "text": "GitHub Actions\n\n\nThis is the backend for the TinyMLaaS project.\n\n\nStart by cloning this repository. Then setup and run the app with the following steps.\n\n\n\nSetup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate\n\n\n\nInstall project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt\n\n\n\nIn a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"\n\n\n\nThe projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.\n\n\n\nWith the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload\n\n\n\nUse the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.\n\n\n\nRun unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "href": "docs/tinymlaas/TinyML-backend_README.html#instructions-for-running-the-application",
    "title": "TinyML-backend",
    "section": "",
    "text": "Start by cloning this repository. Then setup and run the app with the following steps.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "href": "docs/tinymlaas/TinyML-backend_README.html#virtual-environment",
    "title": "TinyML-backend",
    "section": "",
    "text": "Setup venv with:\npython -m venv venv\nActivate the virtual environment with:\nsource venv/bin/activate",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-backend_README.html#dependencies",
    "title": "TinyML-backend",
    "section": "",
    "text": "Install project dependencies by running the following command inside the virtual environment:\npip install -r requirements.txt",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#database-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "In a new terminal window, run the following commands to set up an sqlite database:\nsqlite3\n.open tiny_mlaas.db\n.read schema.sql\n.read populate.sql\nTo connect the backend to the database, create a .env file (in the project’s root directory) with the following line:\nDATABASE_URL=\"sqlite:///./tiny_mlaas.db\"",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "href": "docs/tinymlaas/TinyML-backend_README.html#machine-learning-modules-setup",
    "title": "TinyML-backend",
    "section": "",
    "text": "The projects main repository contains the machine learning modules. Download the modules by running the following command in the backend root directory:\nsvn checkout https://github.com/TinyMLaas/TinyMLaaS/trunk/TinyMLaaS_main\nOptionally clone the main repository and copy or symlink the TinyMLaaS_main folder into the backend’s root directory.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "href": "docs/tinymlaas/TinyML-backend_README.html#run-app",
    "title": "TinyML-backend",
    "section": "",
    "text": "With the virtual environment activated in the project’s root directory, run the app with:\nuvicorn main:app --reload",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "href": "docs/tinymlaas/TinyML-backend_README.html#usage",
    "title": "TinyML-backend",
    "section": "",
    "text": "Use the application with the project frontend here, or explore the API by browsing to: BACKEND_URL/docs.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "href": "docs/tinymlaas/TinyML-backend_README.html#testing",
    "title": "TinyML-backend",
    "section": "",
    "text": "Run unit tests in the backend root folder with:\npytest",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-backend"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html",
    "title": "Arduino sketch",
    "section": "",
    "text": "This is a arduino sktech for image recognition with the help of machine learning. It has been designed to be used on the Arduino Nano 33 BLE with the OV7675 camera module.\nAt the moment, it can be used for binary detection, for example weather there is a person in the picture.\n\n\nThis sketch depends on Tensorflow lite for microcontrollers.\n\n\n\n\n\nFirst, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list\n\n\n\n\nIn order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#dependencies",
    "title": "Arduino sketch",
    "section": "",
    "text": "This sketch depends on Tensorflow lite for microcontrollers.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#installing-sktech-to-microcontoller",
    "title": "Arduino sketch",
    "section": "",
    "text": "First, install the required Arduino tflite library. On linux, this is typically in ~/Arduino/libraries, on MacOS in ~/Documents/Arduino/libraries and on Windows in My Documents\\Arduino\\Libraries.\nOnce in this directory, download the library with\ngit clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\nNow you can continue to installation. First, install the core library for the used arduino board.\narduino-cli core install arduino:mbed_nano\nNext, compile the sketch.\narduino-cli compile --fqbn arduino:mbed_nano:nano33ble template/\nFinally, install it to the device. For this, the port to which the device is connected to is required. This can be found with\narduino-cli board list\nCheck the name of the port, for example /dev/ttyACM0. Now, install the sketch with\narduino-cli upload -p &lt;device_port&gt; --fqbn arduino:mbed_nano:nano33ble template/\n\n\n\nThere is a provided Dockerfile which can make the installation a lot easier.\nFirst, build the image with docker. You can give it any other tag, but this example will name it arduino.\ndocker build -t arduino .\nThe image will contain the compiled sketch and will have arduino-cli as its entry point. Now, just install it with\ndocker run arduino upload -p &lt;device_port&gt; --fqbn arduino_mbed_nano:nano33ble template\nAgain, if you dont no what port the device is connected to, you can use the image to find that.\ndocker run arduino board list",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "href": "docs/tinymlaas/TinyML-MCU_arduino_README.html#changing-the-model",
    "title": "Arduino sketch",
    "section": "",
    "text": "In order to use your own tensorflow model, replace the target_model.cpp with your own model. This is a C array generated from the wanted model. To generate the C array from a tflite model, xxd can be used:\nxxd -i &lt;your_tflite_model&gt; &gt; target_model.cc\nThen, replace the the old file with this new generated file.\n\n\nIf you want to rename the model, you need to also change the name of the models header file target_model.h to the same name and change the headerfile name in template.ino.\n:warning: NOTE! Be careful when renaming the model. For some models the model can not be named something that starts with the word model. For this reason, it is adviced to always name the C-array file so that it does not start with model. ⚠️",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Arduino sketch"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Here, we visualize the performance of three runtimes: PyTorch, our custom-built tinyRuntime (both non-quantized and quantized versions), using the ResNet18 model and 100 images from the Imagenette dataset. We focus on four key metrics: accuracy, execution time, model size and memory usage.\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.247210\n473.507812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n9.644408\n98.714844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n7.335336\n62.062500\n11.197475\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1\n\n\n\n\n\n\n\n\n\nCode\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#amd64",
    "href": "performance.html#amd64",
    "title": "Performance",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom IPython.display import display\n\ndef split_dataframe(df):\n    '''Split dataframe based on Runtime (Pytorch, tinyRuntime (no quant) and tinyRuntime (quant).'''\n    df_pytorch = df[df[\"Runtime\"] == \"PyTorch\"]\n    df_trv = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == False)]\n    df_trq = df[(df[\"Runtime\"] == \"tinyRuntime\") & (df[\"Quantization\"] == True)]\n    return df_pytorch, df_trv, df_trq\n\ndef plot_perf_comp(df, save_image=False):\n    '''Plot latest performance comparisons using Plotly.'''\n    dfs = split_dataframe(df)\n    \n    # Create subplots using Plotly Figure Factory\n    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Accuracy\", \"Time\", \"Max memory usage\", \"Model size\"))\n\n    metrics = [\"Accuracy\", \"Time\", \"Max memory\", \"Model size\"]\n    colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', 'rgba(44, 160, 44, 0.8)']\n    names = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n\n    for i, metric in enumerate(metrics):\n        # Retrieve values for each metric\n        y_values = [df[metric].values[-1] for df in dfs]\n        \n        # Add trace for each runtime\n        for j, name in enumerate(names):\n            trace = go.Bar(x=[name], y=[y_values[j]], marker_color=colors[j], showlegend=(i == 0), name=name)\n            fig.add_trace(trace, row=i // 2 + 1, col=i % 2 + 1)\n\n    # Set layout, background color and font size, and disable legend click feature\n    fig.update_layout(\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.1, xanchor=\"right\", x=1),\n        template=\"plotly_dark\", legend_itemclick=False, legend_itemdoubleclick=False, font=dict(size=14),\n        margin=dict(t=90, b=60, l=80, r=50))\n\n    # Update axis labels\n    fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Time (s)\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Max memory usage (MB)\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Model size (MB)\", row=2, col=2)\n    fig.update_xaxes(showticklabels=False)\n    # Show the plot with modebar hidden\n    fig.show(config={'displayModeBar': False})\n    \n    if save_image:\n        fig.write_image(\"images/perf_bar.png\")\n\n    # Create DataFrame\n    data = {\n        \"Accuracy (%)\": [df[\"Accuracy\"].values[-1] for df in dfs],\n        \"Time (s)\": [df[\"Time\"].values[-1] for df in dfs],\n        \"Max memory usage (MB)\": [df[\"Max memory\"].values[-1] for df in dfs],\n        \"Model size (MB)\": [df[\"Model size\"].values[-1] for df in dfs]\n    }\n    df_results = pd.DataFrame(data, index=[\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"])\n    display(df_results)\n\ndf = pd.read_csv('benchmark.csv')\ndf_x86 = df[df[\"Architecture\"] == \"x86_64\"]\nplot_perf_comp(df_x86)\n\n\n\n\n                                                \n\n\nFigure 1: Performance comparison on AMD64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n11.247210\n473.507812\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n9.644408\n98.714844\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n7.335336\n62.062500\n11.197475\n\n\n\n\n\n\n\n\n\nCode\ndef plot_radar(df, save_image=False):\n    # Normalize the data\n    df = df.copy()\n    df[\"Time\"] = df[\"Time\"].apply(lambda x: x / df[\"Time\"].max())\n    df[\"Accuracy\"] = df[\"Accuracy\"].apply(lambda x: x / 100)\n    df[\"Max memory\"] = df[\"Max memory\"].apply(lambda x: x / df[\"Max memory\"].max())\n    # Split based on runtime and take three relevant columns\n    dfs = [df_split[['Time', 'Max memory', 'Accuracy']] for df_split in split_dataframe(df)]\n    # Create a radar chart\n    categories = ['Time', 'Memory usage', 'Accuracy']\n    runtimes = [\"PyTorch\", \"tinyRuntime (no quant)\", \"tinyRuntime (quant)\"]\n    fig = go.Figure()\n    for i, df in enumerate(dfs):\n        fig.add_trace(go.Scatterpolar(\n            r=df.iloc[-1].values,\n            theta=categories,\n            fill='toself',\n            name=runtimes[i]\n        ))\n\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 1]\n            )\n        ),\n        font=dict(size=16),\n        margin=dict(t=50, b=50, l=50, r=50),\n        showlegend=True\n    )\n    fig.add_annotation(\n        text=\"*Values are normalized between 0 and 1\",\n        xref=\"paper\", yref=\"paper\",\n        x=0.5, y=-0.1,\n        showarrow=False,\n        font=dict(size=15)\n    )\n    fig.show()\n    \n    if save_image:\n        fig.write_image(\"images/perf_radar.png\")\n    \nplot_radar(df_x86)\n\n\n\n\n                                                \n\n\nFigure 2: Radar chart of time, memory, and accuracy for AMD64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "performance.html#arm64",
    "href": "performance.html#arm64",
    "title": "Performance",
    "section": "",
    "text": "Code\ndf_arm = df[df[\"Architecture\"] == \"arm64\"]\nplot_perf_comp(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 3: Performance comparison on ARM64\n\n\n\n\n\n\n\n\n\n\n\nAccuracy (%)\nTime (s)\nMax memory usage (MB)\nModel size (MB)\n\n\n\n\nPyTorch\n99.0\n6.832752\n462.390625\n44.938353\n\n\ntinyRuntime (no quant)\n99.0\n2.672380\n143.281250\n44.660263\n\n\ntinyRuntime (quant)\n99.0\n3.812798\n93.515625\n11.949406\n\n\n\n\n\n\n\n\n\nCode\nplot_radar(df_arm, save_image=True)\n\n\n\n\n                                                \n\n\nFigure 4: Radar chart of time, memory, and accuracy for ARM64. Quantities are scaled between 0 and 1",
    "crumbs": [
      "Get Started",
      "Performance"
    ]
  },
  {
    "objectID": "blogs/tinymlusecases.html",
    "href": "blogs/tinymlusecases.html",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "In today’s competitive landscape, every company is leveraging AI or exploring its integration into their business operations. As AI models become increasingly sophisticated, the operational expenditures (OPEX) associated with utilizing expensive GPUs in cloud datacenters also rise. Moreover, large AI models often cannot be executed on small devices without relying on cloud GPUs.\nNinjaLABO’s AI model compression (TinyML as-a-Service) addresses these challenges by offering versatile solutions applicable across various industries. Below, we explore specific focus areas and use cases where these solutions are particularly relevant:\n\n\n\nData Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage.\n\n\n\n\n\n\nData Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy.\n\n\n\n\n\n\nData Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security.\n\n\n\n\n\n\nData Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications.\n\n\n\n\n\n\nData Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications.\n\n\n\n\n\n\nData Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "href": "blogs/tinymlusecases.html#iot-and-smart-cities",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Sensor data (temperature, humidity, air quality, noise levels), traffic data, utility usage data.\nUse Cases: Predictive maintenance, energy management, traffic optimization, environmental monitoring.\n\nAdvantage: Typically, 99% of data transmission is redundant, wasting network bandwidth and storage. Local AI execution with TinyML eliminates this inefficiency by transmitting data only when anomalies occur, thus optimizing communication and storage."
  },
  {
    "objectID": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "href": "blogs/tinymlusecases.html#healthcare-and-wearables",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Biometric data (heart rate, activity levels, sleep patterns), medical imaging data.\nUse Cases: Health monitoring, early disease detection, personalized healthcare, fitness tracking.\n\nAdvantage: Regulatory constraints often prohibit uploading private data to public clouds. Local AI execution with TinyML ensures that only processed, non-sensitive data is uploaded, preserving privacy."
  },
  {
    "objectID": "blogs/tinymlusecases.html#industrial-automation",
    "href": "blogs/tinymlusecases.html#industrial-automation",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Machine performance data, operational data, maintenance logs.\nUse Cases: Predictive maintenance, process optimization, quality control.\n\nAdvantage: Similar to IoT use cases, local AI execution minimizes unnecessary data transmission, enhancing efficiency and security."
  },
  {
    "objectID": "blogs/tinymlusecases.html#agriculture",
    "href": "blogs/tinymlusecases.html#agriculture",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Soil moisture levels, weather data, crop health data.\nUse Cases: Precision farming, crop monitoring, irrigation management.\n\nAdvantage: Agricultural fields often extend beyond network coverage. With TinyML, AI can be executed locally, enabling smart farming even in off-the-grid areas. This benefit extends to other off-the-grid network and battery-powered applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#automotive-and-mobility",
    "href": "blogs/tinymlusecases.html#automotive-and-mobility",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Vehicle performance data, driver behavior data, traffic data.\nUse Cases: Autonomous driving, fleet management, driver safety systems.\n\nAdvantage: Real-time response is critical. Local AI execution with TinyML ensures immediate processing, which is crucial for safety and efficiency in automotive applications."
  },
  {
    "objectID": "blogs/tinymlusecases.html#security-and-surveillance",
    "href": "blogs/tinymlusecases.html#security-and-surveillance",
    "title": "TinyML Use Cases",
    "section": "",
    "text": "Data Types: Video feeds, audio recordings, motion sensor data.\nUse Cases: Intrusion detection, anomaly detection, crowd monitoring.\n\nAdvantage: Local data execution enhances security by reducing the need to transmit sensitive data, mitigating potential breaches.\n\n\nThese examples highlight the broad applicability of NinjaLABO’s solutions. By focusing on specific use cases within these industries, NinjaLABO can tailor its services to meet the unique needs and challenges of each sector, providing efficient, scalable, and impactful TinyML solutions."
  },
  {
    "objectID": "blogs/yolofaceid.html",
    "href": "blogs/yolofaceid.html",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "",
    "text": "In today’s world, face identification systems are becoming increasingly crucial in various applications, from security and surveillance to personalized customer experiences. Building a real-time face identification system can seem daunting, but with the right tools and approaches, it becomes manageable. In this article, we will walk you through a project to develop such a system, using some of the best tools available, including Ultralytics YOLOv8, managed cloud services (e.g. AWS Rekognition and Azure Face API), and more. We’ll also compare different technologies to help you choose the best one for your needs."
  },
  {
    "objectID": "blogs/yolofaceid.html#setting-up-yolov8",
    "href": "blogs/yolofaceid.html#setting-up-yolov8",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Setting Up YOLOv8",
    "text": "Setting Up YOLOv8\n\nInstall the Ultralytics Package: Begin by installing the necessary software using pip:\npip install ultralytics\nLoad the YOLOv8 Model: Load a pre-trained YOLOv8 model or start with a custom model:\nfrom ultralytics import YOLO\n\n# Load a YOLOv8 model\nmodel = YOLO('yolov8n.pt')\n\n# Perform inference on an image\nresults = model('path/to/your/image.jpg')\nresults.show()  # Display results"
  },
  {
    "objectID": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "href": "blogs/yolofaceid.html#training-on-a-custom-dataset",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Training on a Custom Dataset",
    "text": "Training on a Custom Dataset\nTo fine-tune YOLOv8 for face detection, you need a dataset of face images with annotations in YOLO format. You can use public datasets like WIDER FACE or CelebA.\n\nPrepare Your Dataset: Ensure your dataset is properly annotated and structured.\nConfigure and Train the Model: Train YOLOv8 using your dataset:\nmodel = YOLO('yolov8n.pt')\nmodel.train(data='path/to/data.yaml', epochs=100, imgsz=640, batch=16)"
  },
  {
    "objectID": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "href": "blogs/yolofaceid.html#deploying-for-real-time-face-detection",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Deploying for Real-Time Face Detection",
    "text": "Deploying for Real-Time Face Detection\nOnce trained, you can deploy YOLOv8 for real-time face detection. This can be done by feeding live video streams into the model and processing each frame.\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the trained model\nmodel = YOLO('runs/train/exp/weights/best.pt')\n\n# Capture video from webcam\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Perform inference\n    results = model(frame)\n\n    # Display results\n    results.show()\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "blogs/yolofaceid.html#key-factors",
    "href": "blogs/yolofaceid.html#key-factors",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Factors:",
    "text": "Key Factors:\n\nGPU is superiour to CPU at inference from cost vs perf comparison.\nProcessing Power: Each GPU’s ability to handle a specific number of frames per second (FPS).\nTotal FPS Requirement: The total computational load imposed by the 200 cameras.\nRack-Mount Server Specifications: The number of GPUs each server can support."
  },
  {
    "objectID": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "href": "blogs/yolofaceid.html#calculating-total-fps-requirement",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Calculating Total FPS Requirement:",
    "text": "Calculating Total FPS Requirement:\n\n200 Cameras at 30 FPS each:\n\nTotal FPS required = 200 cameras * 30 FPS = 6000 FPS."
  },
  {
    "objectID": "blogs/yolofaceid.html#gpu-capacity",
    "href": "blogs/yolofaceid.html#gpu-capacity",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "GPU Capacity:",
    "text": "GPU Capacity:\n\nNVIDIA RTX 3080: Can handle approximately 400 FPS for YOLOv8-tiny and around 80-100 FPS for standard YOLOv8.\nNVIDIA RTX 3090: Can handle approximately 600 FPS for YOLOv8-tiny and around 120-150 FPS for standard YOLOv8.\nNVIDIA RTX 4090: Can handle approximately 1000 FPS for YOLOv8-tiny and around 200-250 FPS for standard YOLOv8."
  },
  {
    "objectID": "blogs/yolofaceid.html#example-server-configurations",
    "href": "blogs/yolofaceid.html#example-server-configurations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Example Server Configurations:",
    "text": "Example Server Configurations:\n\nSupermicro 4U GPU Server (SYS-4029GP-TRT):\n\nSupports up to 4 GPUs.\nIf each server uses 4x RTX 4090 GPUs, the total processing capability per server would be:\n\nYOLOv8-tiny: 4000 FPS (4 GPUs * 1000 FPS).\nStandard YOLOv8: 800-1000 FPS (4 GPUs * 200-250 FPS).\n\n\nASUS ESC8000A-E11:\n\nSupports up to 8 GPUs.\nWith 8x RTX 4090 GPUs:\n\nYOLOv8-tiny: 8000 FPS (8 GPUs * 1000 FPS).\nStandard YOLOv8: 1600-2000 FPS (8 GPUs * 200-250 FPS)."
  },
  {
    "objectID": "blogs/yolofaceid.html#number-of-servers-needed",
    "href": "blogs/yolofaceid.html#number-of-servers-needed",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Number of Servers Needed:",
    "text": "Number of Servers Needed:\n\nFor YOLOv8-tiny:\n\nWith Supermicro 4U Server: 1.5 servers (round up to 2 servers).\nWith ASUS ESC8000A-E11: 1 server is sufficient.\n\nFor Standard YOLOv8:\n\nWith Supermicro 4U Server: 6-8 servers.\nWith ASUS ESC8000A-E11: 3-4 servers."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary",
    "href": "blogs/yolofaceid.html#summary",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\n\nYOLOv8-tiny: You would need 1-2 servers (depending on GPU configuration) to handle the 200 cameras.\nStandard YOLOv8: You would need 3-8 servers depending on the specific GPU and server model you choose.\n\nThese estimates assume maximum utilization of each GPU’s capabilities, and the actual number might vary based on real-world conditions, such as CPU bottlenecks, memory limitations, and other factors."
  },
  {
    "objectID": "blogs/yolofaceid.html#key-parameters",
    "href": "blogs/yolofaceid.html#key-parameters",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Key Parameters",
    "text": "Key Parameters\n\nResolution:\n\n2MP (1080p): Approximately 3 Mbps\n4MP: Approximately 6 Mbps\n\nFrame Rate: 30 frames per second (FPS)\nRecording Time: 24 hours per day, 30 days per month\nNumber of Cameras: 200\nCompression: H.264 assumed (H.265 could reduce storage by up to 50%)"
  },
  {
    "objectID": "blogs/yolofaceid.html#storage-calculation",
    "href": "blogs/yolofaceid.html#storage-calculation",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Storage Calculation",
    "text": "Storage Calculation\nThe total storage required can be calculated using the following equation:\n\\[\n\\text{Total Storage (TB)} = \\left(\\frac{\\text{Bitrate (Mbps)} \\times 86,400 \\times 30 \\times \\text{Number of Cameras}}{8 \\times 1024^2}\\right)\n\\]\nWhere:\n\n86,400: Number of seconds in a day\n30: Number of days in a month\n8: Conversion factor from bits to bytes\n1024^2: Conversion from MB to TB"
  },
  {
    "objectID": "blogs/yolofaceid.html#results",
    "href": "blogs/yolofaceid.html#results",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Results",
    "text": "Results\n\nFor 2MP Cameras (3 Mbps):\n\nTotal Storage: Approximately 185.2 TB per month.\n\nFor 4MP Cameras (6 Mbps):\n\nTotal Storage: Approximately 370.5 TB per month."
  },
  {
    "objectID": "blogs/yolofaceid.html#considerations",
    "href": "blogs/yolofaceid.html#considerations",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "Considerations",
    "text": "Considerations\n\nCompression: Using H.265 compression could potentially reduce the storage requirement by 50%.\nMotion Detection: If motion detection is used instead of continuous recording, the required storage could be significantly less."
  },
  {
    "objectID": "blogs/yolofaceid.html#summary-1",
    "href": "blogs/yolofaceid.html#summary-1",
    "title": "Building a Real-Time Face Identification System: A Comprehensive Guide",
    "section": "SUMMARY",
    "text": "SUMMARY\nFor 200 cameras recording continuously at 2-4MP resolution, the storage requirement ranges from approximately 185.2 TB to 370.5 TB per month. Adjustments to compression methods, frame rates, or recording strategies could alter these estimates."
  },
  {
    "objectID": "blogs/mvp6.html",
    "href": "blogs/mvp6.html",
    "title": "AI Compression as-a-Service MVP6 review",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review"
  },
  {
    "objectID": "blogs/ericsson_blog1.html",
    "href": "blogs/ericsson_blog1.html",
    "title": "TinyML as-a-Service and the challenges of machine learning at the edge",
    "section": "",
    "text": "https://www.ericsson.com/en/blog/2019/12/tinyml-as-a-service\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge"
  },
  {
    "objectID": "blogs/iree.html",
    "href": "blogs/iree.html",
    "title": "IREE review",
    "section": "",
    "text": "Compiling and optimizing ML models for multiple hardware platforms can be complex and time-consuming. One promising solution on our radar is the Intermediate Representation Execution Environment (IREE), which can significantly simplify this process. Here’s an in-depth look at what I’ve learned about IREE.\n\n\nIREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms.\n\n\n\n\nImport Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware.\n\n\n\n\n\nIntermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments.\n\n\n\n\nHere was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/iree.html#what-is-iree",
    "href": "blogs/iree.html#what-is-iree",
    "title": "IREE review",
    "section": "",
    "text": "IREE, or Intermediate Representation Execution Environment, is an end-to-end compiler designed specifically for machine learning (ML) models. It takes models expressed in an intermediate representation using Multi-Level Intermediate Representation (MLIR) and performs hardware-agnostic optimizations and transformations. Unlike traditional compilers, IREE doesn’t directly generate low-level machine code but uses various optimizers and accelerators to do this, making it a versatile framework that supports a variety of hardware platforms."
  },
  {
    "objectID": "blogs/iree.html#workflow-with-iree",
    "href": "blogs/iree.html#workflow-with-iree",
    "title": "IREE review",
    "section": "",
    "text": "Import Your Model: Begin by importing your machine learning model into IREE. This can be a model developed using one of the supported frameworks such as TensorFlow, PyTorch, JAX, ONNX, or TensorFlow Lite.\nConfigure Deployment Settings: Specify your deployment configuration, including the target platform (CPU, GPU, etc.), accelerators, and any other constraints relevant to your deployment environment.\nCompile Your Model: Utilize IREE to compile your model. During compilation, IREE optimizes the model’s code for the specified deployment configuration using its end-to-end compiler capabilities.\nRun Your Compiled Model: Once compiled, use IREE’s runtime components to execute your optimized model on the target hardware."
  },
  {
    "objectID": "blogs/iree.html#key-features-of-iree",
    "href": "blogs/iree.html#key-features-of-iree",
    "title": "IREE review",
    "section": "",
    "text": "Intermediate Representation (IR) and MLIR: IREE uses MLIR (Multi-Level Intermediate Representation) as its IR, which acts like a programming language for expressing machine learning models. This allows for sophisticated optimizations and transformations that are independent of the underlying hardware. IREE supports conversion from a variety of popular ML frameworks including JAX, ONNX, TensorFlow, TensorFlow Lite, and PyTorch into MLIR.\nAutomatic Optimization: Unlike traditional compilers, IREE integrates scheduling and execution logic during compilation, rather than deferring it to runtime. This approach reduces scheduling overhead and enhances efficiency by compiling optimized code tailored to the target hardware at compile time. Additionally, IREE excels in automatic code optimization through compilers it uses, for example parallelization and vectorization are performed automatically (in the case of CPU). This means it can automatically transform the input model code to leverage hardware capabilities effectively, ensuring efficient execution on different platforms.\nWide Hardware Support: IREE supports a broad range of hardware configurations, including various CPUs and GPUs. For example, it offers support for bare-metal, enabling deployment on edge devices with minimal footprint. With the help of the Hardware Abstraction Layer (HAL), IREE can compile for various hardware platforms on any supported machine. This versatility allows developers to optimize and deploy ML models across different hardware environments without the need for extensive platform-specific modifications.\nBindings IREE also provides bindings, which are interfaces that allow access to the IREE compiler and its components from different programming languages, such as Python. For example, you can compile and run models using IREE in the Python interface. This flexibility is crucial for integrating IREE into diverse workflows and leveraging its capabilities from various development environments."
  },
  {
    "objectID": "blogs/iree.html#conclusion",
    "href": "blogs/iree.html#conclusion",
    "title": "IREE review",
    "section": "",
    "text": "Here was an overview of IREE, emphasizing its capabilities in optimizing models across diverse hardware platforms. For practical examples, please visit IREE’s official documentation at https://iree.dev, where you can find comprehensive guides and examples illustrating how to compile ML models for various hardware configurations."
  },
  {
    "objectID": "blogs/ericsson_blog2.html",
    "href": "blogs/ericsson_blog2.html",
    "title": "How can we democratize machine learning on IoT devices?",
    "section": "",
    "text": "ghttps://www.ericsson.com/en/blog/2020/2/how-can-we-democratize-machine-learning-iot-devices\n\n\n\nHow can we democratize machine learning on IoT devices?"
  },
  {
    "objectID": "blogs/dnn_comp_techs.html",
    "href": "blogs/dnn_comp_techs.html",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Deep Neural Networks (DNNs) have achieved remarkable success across various domains, from image recognition to natural language processing. However, their deployment, especially in resource-constrained environments like mobile devices or edge computing, is often hindered by their size and computational demands. Model compression techniques are essential to address these challenges, enabling the use of DNNs in real-world applications without sacrificing too much performance. This article provides a comprehensive overview of the major DNN model compression techniques, presenting a big picture of how each approach contributes to making neural networks more efficient.\n\n\n\nQuantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage.\n\n\n\nPruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints.\n\n\n\n\nKnowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model.\n\n\n\n\nLow-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode.\n\n\n\n\n\nBatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy.\n\n\n\nWeight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity.\n\n\n\nNeural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#quantization",
    "href": "blogs/dnn_comp_techs.html#quantization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Quantization reduces the precision of the numbers representing model parameters, thus decreasing the memory and computational requirements. There are several quantization approaches and methods, each of which can be used independently or in combination to optimize model performance.\nQuantization Approaches: - Post-Training Quantization (PTQ): This technique converts the weights and activations of a pre-trained model to lower precision (e.g., from 32-bit floating point to 8-bit integers) without requiring further training. - Quantization-Aware Training (QAT): Unlike PTQ, QAT involves training the model with simulated quantization effects, allowing the network to learn to be robust to the lower precision. This typically results in better performance compared to PTQ. Quantization Methods: - Static Quantization: Quantizes weights and activations using a representative dataset. All required quantization parameters are calculated beforehand, making inference faster compared to dynamic quantization. - Dynamic Quantization: Applies quantization to activations dynamically during inference. This method adapts to varying data more effectively than static quantization, providing flexibility for data with a wide range of values. - Weight-Only Quantization: Focuses on quantizing only the model weights while keeping activations at a higher precision. This approach can help maintain model accuracy compared to quantizing both weights and activations, but it results in a larger memory footprint. - Group quantization: Quantizes weights in predefined groups separately, allowing more precise control and potentially better trade-offs between accuracy and memory size. - Symmetric vs. Asymmetric Quantization: Symmetric quantization uses a single scale for both positive and negative values, while asymmetric quantization allows addionally zero-points (shifting values), , fully utilizing the quantization range. Symmetric quantization is generally faster, whereas asymmetric quantization offers improved accuracy. - Mixed Precision Quantization: Utilizes different precision levels (e.g., 16-bit floating point, 8-bit integers) within the same model. This method selects precision dynamically based on the importance of different layers or operations and can be combined with various quantization techniques to optimize performance and efficiency.\nEach quantization method can be used in combination, depending on the specific needs of the model and deployment scenario. By effectively applying these techniques, you can achieve a balance between model accuracy, computational efficiency, and resource usage."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#pruning",
    "href": "blogs/dnn_comp_techs.html#pruning",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Pruning involves removing less important parameters (e.g., weights, neurons, filters) from the network, effectively “trimming” the model without significant loss in performance. Pruning can be categorized into:\n\nUnstructured Pruning: Removes individual weights based on certain criteria, such as magnitude-based pruning, where weights with the smallest magnitude are removed. This type of pruning can lead to sparse matrices that are harder to accelerate on standard hardware.\n\nLottery Ticket Hypothesis: Suggests that within a large network, there exist smaller subnetworks (“winning tickets”) that can be trained to achieve performance similar to the original network.\n\nStructured Pruning: In contrast to unstructured pruning, this method removes entire structures, such as filters or channels, making the resulting model more hardware-friendly.\n\nFilter Pruning: Removes entire filters in convolutional layers.\nChannel Pruning: Prunes entire channels across feature maps.\nBlock Pruning: Removes blocks of weights or layers.\nAutomated Gradual Pruning: Gradually removes parameters during training based on a predefined schedule, as implemented in tools like FasterAI.\n\nDynamic Pruning: Adjusts the pruning strategy during inference or training based on runtime conditions, ensuring the model adapts to changing computational constraints."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "href": "blogs/dnn_comp_techs.html#knowledge-distillation",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Knowledge Distillation is a technique where a smaller “student” model is trained to mimic the behavior of a larger “teacher” model. The student model learns not only from the labeled data but also from the soft predictions of the teacher, which provides richer information about the data.\n\nTeacher-Student Distillation: The classic approach where the student model is trained using the outputs of a pre-trained teacher model.\n\nSoft Targets: The student model is trained to match the teacher’s softened outputs (probabilities) rather than the hard labels.\nIntermediate Layer Distillation: The student learns from the intermediate representations of the teacher model, not just the final output.\n\nSelf-Distillation: A single model distills knowledge from itself by using its own predictions from previous training iterations as targets.\nCross-Model Distillation: Distillation occurs between different types of models, such as distilling from an ensemble of models to a single student model."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "href": "blogs/dnn_comp_techs.html#low-rank-factorization",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Low-Rank Factorization reduces the dimensionality of the model parameters by decomposing matrices or tensors into products of lower-dimensional entities:\n\nMatrix Factorization:\n\nSingular Value Decomposition (SVD): Decomposes weight matrices into products of smaller matrices, effectively reducing the model size.\n\nTensor Factorization:\n\nCP Decomposition: Decomposes tensors (multi-dimensional arrays) into sums of outer products of vectors.\nTucker Decomposition: Generalizes matrix decomposition to higher dimensions by decomposing a tensor into a core tensor multiplied by matrices along each mode."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "href": "blogs/dnn_comp_techs.html#batchnorm-folding",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "BatchNorm Folding combines Batch Normalization layers with preceding layers to reduce the computational load during inference. BatchNorm layers are merged with the preceding convolutional or fully connected layers during inference, reducing the number of operations. This is not an approximation; it means you can achieve model compression without sacrificing accuracy."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#weight-sharing",
    "href": "blogs/dnn_comp_techs.html#weight-sharing",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Weight Sharing is a technique used in neural networks to reduce the number of parameters, thus improving computational efficiency and potentially enhancing generalization. The core idea is to use the same weights across different parts of the network, which can lead to significant reductions in model size and complexity."
  },
  {
    "objectID": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "href": "blogs/dnn_comp_techs.html#neural-architecture-search-nas",
    "title": "Comprehensive Overview of DNN Model Compression Techniques",
    "section": "",
    "text": "Neural Architecture Search automates the design of neural networks, optimizing them for specific constraints, such as size, speed, or accuracy:\n\nReinforcement Learning-Based NAS: Uses reinforcement learning to explore different architectures.\nEvolutionary Algorithm-Based NAS: Applies evolutionary strategies to evolve network architectures over successive generations.\nGradient-Based NAS: Utilizes gradients to guide the search for optimal architectures.\nHardware-Aware NAS: Tailors the search to optimize architectures specifically for target hardware, balancing performance with computational efficiency."
  },
  {
    "objectID": "blogs/summer_insights_2024.html",
    "href": "blogs/summer_insights_2024.html",
    "title": "Summer Insights: Evolving tinyMLaaS with Flexible Pipelines and Distributed Execution",
    "section": "",
    "text": "This summer, our team embarked on an ambitious journey, iterating through eight MVP releases of our tinyMLaaS platform, which focuses on deep neural network (DNN) model compression within a SaaS environment. The feedback we received from a diverse group of reviewers was invaluable—your insights and suggestions were instrumental in shaping the future direction of our platform. Through this collaborative effort, we identified two significant architectural enhancements that are crucial for improving the flexibility and scalability of tinyMLaaS. These enhancements focus on two key areas: abstracting the model transformation process through flexible pipelining and enabling distributed execution of these transformations across various virtual machines (VMs).\n\nFlexible Pipelining for DNN Model Transformations\nAt the heart of DNN model compression lies a series of transformations applied to pre-trained models. These transformations are essential for optimizing models, reducing their size, and minimizing computational demands while maintaining accuracy. The term “DNN model” covers a broad spectrum, from a pure saved representation of computationa grapgh (e.g. a seris of feedforward networks, convolutional and recurrent networks) to an installable OS / docker container images, where a model comes with its runtime enviroments).\nIn the early stages of tinyMLaaS, our platform supported two primary types of transformations: the compiler and the installer:\n\nCompiler: This component applies a variety of compression techniques, both hardware-independent and hardware-aware. These include methods such as quantization (which reduces the precision of the model’s weights), layer fusion (which merges layers to streamline computation), and pruning (which removes redundant or less critical neurons). The goal is to shrink the model’s footprint and speed up inference without significantly sacrificing accuracy.\nInstaller: After compression, the installer packages the model with a runtime execution environment, preparing it for deployment in various formats. This could involve creating an installable operating system image or a Docker container that encapsulates everything needed to run the model.\n\nWhile these transformations proved effective, we recognized a limitation in treating them as distinct processes. By abstracting these processes into a unified concept we now refer to as a transformer, we introduce the ability to create more flexible and modular pipelines. With this new approach, any number of transformers, of any type, can be sequentially applied to a DNN model. This flexibility allows us to customize the compression and packaging workflow to suit the specific needs of each project or use case, enabling the creation of more complex and tailored solutions.\n\nFor instance, a user could now chain multiple transformations, such as applying quantization, followed by pruning, and then wrapping the final model in a Docker container, all within a single streamlined pipeline. This modularity also opens the door to integrating new and emerging techniques into the pipeline, ensuring that tinyMLaaS remains at the cutting edge of model compression technology.\n\n\nDistributed Execution of Model Transformations\nIn our initial design, tinyMLaaS operated on a single, large GPU machine. This centralized approach was chosen for simplicity, but as we explored real-world use cases, we encountered several challenges that highlighted the need for a more distributed architecture.\nOne significant issue arose with Quantization Aware Training (QAT), a technique where the model is retrained with quantization in mind to maintain accuracy post-compression. QAT requires the use of large datasets, which, in our original setup, had to be uploaded to our cloud infrastructure. This process not only consumed considerable time but also raised concerns among customers who needed to keep their proprietary data on-premises for security and compliance reasons.\nTo overcome these challenges, we are evolving tinyMLaaS to support distributed execution of model transformations. In this enhanced architecture, tinyMLaaS would continue to function as a control panel hosted in our data center. However, the actual processing of the models—whether it’s compression, packaging, or retraining—would take place on VMs located within the customer’s environment. This setup allows customers to maintain full control over their data, using their own computational resources to perform the necessary transformations. The benefits are twofold: it significantly reduces data transfer times and enhances security by keeping sensitive data within the customer’s infrastructure.\n\nOur platform’s current components, the compiler and installer, are already encapsulated as Docker containers. This containerization is a crucial advantage as it allows us to seamlessly integrate distributed execution. The missing piece was the ability to specify where each transformer should be executed. We are addressing this by leveraging Docker Swarm, a powerful tool for orchestrating distributed systems. Docker Swarm enables us to manage a cluster of VMs, dynamically assigning tasks based on the specific characteristics and capabilities of each VM. This orchestration allows us to distribute the transformation workload intelligently, whether on-premises or in the cloud, optimizing for performance, security, and efficiency.\n\n\nConclusion\nThe two architectural changes we’re implementing—flexible pipelining of DNN model transformations and distributed execution—are not just incremental improvements; they are transformative steps forward for the tinyMLaaS platform. These enhancements are driven by the real-world challenges and feedback we’ve encountered during our MVP iterations, ensuring that tinyMLaaS is equipped to meet the evolving needs of edge AI and model compression.\nAs we look to the future, we are eager to bring these innovations to life and continue pushing the boundaries of what tinyMLaaS can achieve. We believe these changes will empower our users to tackle more complex problems, deploy models more efficiently, and maintain the highest standards of security and scalability.\nThank you again to everyone who contributed feedback and ideas—your input is shaping the future of tinyMLaaS, and we’re excited to continue this journey together.\n\n\n\nReferences\n\nQuantization Aware Training: A method that simulates the effect of quantization during training, allowing for more accurate compressed models. For more details, see Quantization Aware Training.\nDocker Swarm: A native clustering and orchestration tool for Docker, enabling the management of a cluster of Docker nodes as a single virtual system. Learn more about Docker Swarm here.\nModel Compression Techniques: An overview of various DNN model compression techniques can be found in this comprehensive review."
  },
  {
    "objectID": "blogs/llm_on_camera.html",
    "href": "blogs/llm_on_camera.html",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO proposes a 1.5-month Proof of Concept (PoC) to demonstrate the transformative potential of Local Large Language Models (LLMs) on edge cameras as a Universal Sensor for smart mobility. Utilizing NVIDIA JETSON Orin AGX for high-performance, real-time image and data processing, our solution will empower both public authorities and citizens with localized insights.\nBy ensuring full GDPR compliance through on-device processing, this solution opens new possibilities for smart city services. Citizens, via a public app interface, will be able to ask real-time questions about their immediate environment—from parking availability to public safety insights—unlocking a wide range of unseen use cases.\n\n\n\nIn modern cities, there is a growing demand for context-aware, real-time insights for mobility, safety, and general urban well-being. Current solutions may rely heavily on inflexible fixed purpose sensors or security camera with cloud infrastructure, which raises privacy concerns and introduces latency. Our camera as an Universal Sensor approach, leveraging NVIDIA JETSON Orin AGX, processes information locally, providing flexible, actionable insights without compromising privacy.\nThis PoC will not only demonstrate how this system improves mobility management and public safety, but it will also introduce new unseen citizen-driven use cases by allowing anyone to communite with LLM on CAM at real-time via an WebApp, effectively democratizing urban data.\n\n\n\nOur LLM on Camera as a Universal Sensor solution utilizes multimodal LLMs to interpret visual data (i.e. camera /video capture images) and answer questions in real-time. With the NVIDIA JETSON Orin AGX, which provides industry-leading computational power, the solution will demonstrate several impactful use cases:\n\n\n\nVacant Parking Slot Detection: Edge cameras detect available parking spaces and provide real-time updates to users by asking LLM, even with number of vacancy, size of such slot, and anything as long as LLM can understand or interpret.\nPublic Safety Monitoring: Detect disturbances or suspicious activity in public spaces. The system can instantly alert authorities to potential security threats, improving urban safety without exposing personal data by asking LLM with “any fights heppening?”.\nEnvironmental Monitoring: Citizens or local governments can query the system about feeling of air quality, noise levels, or local weather conditions (e.g., slippery sidewalks during winter) by asking LLM what kind of cloths people wear, ensuring timely responses to environmental changes.\nTraffic Flow and Congestion Analysis: The system tracks traffic patterns and provides real-time feedback on road congestion, alerting users to alternate routes or times to travel if needed via LLM.\nPublic Event Reporting: The app can report real-time crowd density and event status during festivals, concerts, or protests, helping citizens avoid overcrowded areas or plan their attendance.\nEmergency Support: In the case of a disaster (e.g., a fire or flood), the system can help first responders by providing any type of localized data on obstacles, congestion, or affected areas.\nLocalized Queries from Citizens: Any citizen can ask the LLM on Camera specific questions about the local environment—e.g., “Are there available seats in the park?” or “Is there a bike rack near this building?”—unlocking real-time insights and enhancing city navigation.\n\nThe flexibility of the Universal Sensor allows it to be expanded into sectors like tourism, disaster response, and public infrastructure management, enabling both real-time insights and enhanced citizen engagement as a part of City infrastructure.\n\n\n\nNormally the image itself won’t be seen by users but only conversation.\n\nAnd you can withdraw any insights via LLM.\n\n\n\n\n\nThe NVIDIA JETSON Orin AGX provides significant computational capacity for real-time AI applications, making it the ideal platform for running multimodal LLMs. The PoC will utilize the following:\n\nNVIDIA JETSON Orin AGX: A powerful edge AI platform capable of handling advanced computations with up to 200 TOPS of performance.\nMultimodal LLM: Pre-trained LLMs that can interpret visual, textual, and contextual data in real-time, optimized for efficient on-device processing.\nAdvanced Model Compression: Techniques like quantization and pruning ensure that LLMs are optimized for running efficiently on the Orin AGX.\n\nBy keeping data processing local, we eliminate the need for external data transmission, reducing latency while preserving privacy.\n\n\n\nBy the end of this 1.5-month PoC, we expect to demonstrate:\n\nParking Slot Detection: Detecting vacant parking spaces and providing real-time updates.\nPublic Safety Monitoring: Detecting and responding to public safety incidents.\nCitizen Engagement via WebApp: Offering citizens real-time access to the system’s insights by allowing them to query localized information through the app. This democratization of data will unlock unforeseen use cases that expand beyond mobility and safety.\nEnvironmental and Traffic Monitoring: Demonstrating how the Universal Sensor can track and respond to environmental and traffic conditions in real time, supporting various urban planning initiatives.\n\n\n\n\n\nWeek 1 - Initial Setup:\n\nInstall and configure NVIDIA JETSON Orin AGX and integrate multimodal LLMs for the core and extended use cases.\nBegin small-scale testing in a controlled environment.\n\nWeek 2-3 - Use Case Development:\n\nFine-tune parking slot detection, public safety monitoring, and citizen query capabilities.\nDevelop app functionality for citizen interaction with the Universal Sensor.\n\nWeek 4-6 - Deployment and Validation:\n\nDeploy the system in a small urban environment (e.g., a parking lot, public square).\nTest accuracy, latency, and app engagement, collecting feedback from stakeholders and citizens.\nAnalyze results for further scalability.\n\n\n\n\n\nWe will collaborate with local municipalities and NVIDIA to optimize the NVIDIA JETSON Orin AGX platform for smart mobility applications. Citizen engagement will be supported through partnerships with local app developers and parking management services.\n\n\n\nNinjaLABO’s LLM on Camera as a Universal Sensor is a powerful, privacy-preserving tool designed to transform smart city services. By leveraging the advanced capabilities of NVIDIA JETSON Orin AGX and introducing an open citizen engagement app, this PoC will demonstrate the potential for real-time mobility management, public safety enhancement, and localized insights for citizens. The possibilities of new use cases emerging from public queries are vast, making this technology a cornerstone of smart city innovation.\nWe look forward to showcasing how Universal Sensors can transform the way citizens interact with and benefit from their urban environments, fully aligning with Helsinki’s goals of AI-powered mobility."
  },
  {
    "objectID": "blogs/llm_on_camera.html#executive-summary",
    "href": "blogs/llm_on_camera.html#executive-summary",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO proposes a 1.5-month Proof of Concept (PoC) to demonstrate the transformative potential of Local Large Language Models (LLMs) on edge cameras as a Universal Sensor for smart mobility. Utilizing NVIDIA JETSON Orin AGX for high-performance, real-time image and data processing, our solution will empower both public authorities and citizens with localized insights.\nBy ensuring full GDPR compliance through on-device processing, this solution opens new possibilities for smart city services. Citizens, via a public app interface, will be able to ask real-time questions about their immediate environment—from parking availability to public safety insights—unlocking a wide range of unseen use cases."
  },
  {
    "objectID": "blogs/llm_on_camera.html#challenge-context",
    "href": "blogs/llm_on_camera.html#challenge-context",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "In modern cities, there is a growing demand for context-aware, real-time insights for mobility, safety, and general urban well-being. Current solutions may rely heavily on inflexible fixed purpose sensors or security camera with cloud infrastructure, which raises privacy concerns and introduces latency. Our camera as an Universal Sensor approach, leveraging NVIDIA JETSON Orin AGX, processes information locally, providing flexible, actionable insights without compromising privacy.\nThis PoC will not only demonstrate how this system improves mobility management and public safety, but it will also introduce new unseen citizen-driven use cases by allowing anyone to communite with LLM on CAM at real-time via an WebApp, effectively democratizing urban data."
  },
  {
    "objectID": "blogs/llm_on_camera.html#solution-overview",
    "href": "blogs/llm_on_camera.html#solution-overview",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "Our LLM on Camera as a Universal Sensor solution utilizes multimodal LLMs to interpret visual data (i.e. camera /video capture images) and answer questions in real-time. With the NVIDIA JETSON Orin AGX, which provides industry-leading computational power, the solution will demonstrate several impactful use cases:\n\n\n\nVacant Parking Slot Detection: Edge cameras detect available parking spaces and provide real-time updates to users by asking LLM, even with number of vacancy, size of such slot, and anything as long as LLM can understand or interpret.\nPublic Safety Monitoring: Detect disturbances or suspicious activity in public spaces. The system can instantly alert authorities to potential security threats, improving urban safety without exposing personal data by asking LLM with “any fights heppening?”.\nEnvironmental Monitoring: Citizens or local governments can query the system about feeling of air quality, noise levels, or local weather conditions (e.g., slippery sidewalks during winter) by asking LLM what kind of cloths people wear, ensuring timely responses to environmental changes.\nTraffic Flow and Congestion Analysis: The system tracks traffic patterns and provides real-time feedback on road congestion, alerting users to alternate routes or times to travel if needed via LLM.\nPublic Event Reporting: The app can report real-time crowd density and event status during festivals, concerts, or protests, helping citizens avoid overcrowded areas or plan their attendance.\nEmergency Support: In the case of a disaster (e.g., a fire or flood), the system can help first responders by providing any type of localized data on obstacles, congestion, or affected areas.\nLocalized Queries from Citizens: Any citizen can ask the LLM on Camera specific questions about the local environment—e.g., “Are there available seats in the park?” or “Is there a bike rack near this building?”—unlocking real-time insights and enhancing city navigation.\n\nThe flexibility of the Universal Sensor allows it to be expanded into sectors like tourism, disaster response, and public infrastructure management, enabling both real-time insights and enhanced citizen engagement as a part of City infrastructure.\n\n\n\nNormally the image itself won’t be seen by users but only conversation.\n\nAnd you can withdraw any insights via LLM."
  },
  {
    "objectID": "blogs/llm_on_camera.html#technical-approach",
    "href": "blogs/llm_on_camera.html#technical-approach",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "The NVIDIA JETSON Orin AGX provides significant computational capacity for real-time AI applications, making it the ideal platform for running multimodal LLMs. The PoC will utilize the following:\n\nNVIDIA JETSON Orin AGX: A powerful edge AI platform capable of handling advanced computations with up to 200 TOPS of performance.\nMultimodal LLM: Pre-trained LLMs that can interpret visual, textual, and contextual data in real-time, optimized for efficient on-device processing.\nAdvanced Model Compression: Techniques like quantization and pruning ensure that LLMs are optimized for running efficiently on the Orin AGX.\n\nBy keeping data processing local, we eliminate the need for external data transmission, reducing latency while preserving privacy."
  },
  {
    "objectID": "blogs/llm_on_camera.html#expected-outcomes-and-impact",
    "href": "blogs/llm_on_camera.html#expected-outcomes-and-impact",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "By the end of this 1.5-month PoC, we expect to demonstrate:\n\nParking Slot Detection: Detecting vacant parking spaces and providing real-time updates.\nPublic Safety Monitoring: Detecting and responding to public safety incidents.\nCitizen Engagement via WebApp: Offering citizens real-time access to the system’s insights by allowing them to query localized information through the app. This democratization of data will unlock unforeseen use cases that expand beyond mobility and safety.\nEnvironmental and Traffic Monitoring: Demonstrating how the Universal Sensor can track and respond to environmental and traffic conditions in real time, supporting various urban planning initiatives."
  },
  {
    "objectID": "blogs/llm_on_camera.html#project-timeline-1.5-months",
    "href": "blogs/llm_on_camera.html#project-timeline-1.5-months",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "Week 1 - Initial Setup:\n\nInstall and configure NVIDIA JETSON Orin AGX and integrate multimodal LLMs for the core and extended use cases.\nBegin small-scale testing in a controlled environment.\n\nWeek 2-3 - Use Case Development:\n\nFine-tune parking slot detection, public safety monitoring, and citizen query capabilities.\nDevelop app functionality for citizen interaction with the Universal Sensor.\n\nWeek 4-6 - Deployment and Validation:\n\nDeploy the system in a small urban environment (e.g., a parking lot, public square).\nTest accuracy, latency, and app engagement, collecting feedback from stakeholders and citizens.\nAnalyze results for further scalability."
  },
  {
    "objectID": "blogs/llm_on_camera.html#collaboration-and-ecosystem",
    "href": "blogs/llm_on_camera.html#collaboration-and-ecosystem",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "We will collaborate with local municipalities and NVIDIA to optimize the NVIDIA JETSON Orin AGX platform for smart mobility applications. Citizen engagement will be supported through partnerships with local app developers and parking management services."
  },
  {
    "objectID": "blogs/llm_on_camera.html#conclusion",
    "href": "blogs/llm_on_camera.html#conclusion",
    "title": "LLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC",
    "section": "",
    "text": "NinjaLABO’s LLM on Camera as a Universal Sensor is a powerful, privacy-preserving tool designed to transform smart city services. By leveraging the advanced capabilities of NVIDIA JETSON Orin AGX and introducing an open citizen engagement app, this PoC will demonstrate the potential for real-time mobility management, public safety enhancement, and localized insights for citizens. The possibilities of new use cases emerging from public queries are vast, making this technology a cornerstone of smart city innovation.\nWe look forward to showcasing how Universal Sensors can transform the way citizens interact with and benefit from their urban environments, fully aligning with Helsinki’s goals of AI-powered mobility."
  },
  {
    "objectID": "blogs/tinymlaas.html",
    "href": "blogs/tinymlaas.html",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "href": "blogs/tinymlaas.html#what-is-tinyml-ai-compression",
    "title": "TinyML as-a-Service Overview",
    "section": "",
    "text": "TinyML involves deploying machine learning models on small, resource-constrained devices, such as microcontrollers or edge devices. The objective is to enable these devices to perform intelligent tasks without relying on powerful servers or the cloud. AI Compression involves techniques to reduce the size and computational requirements of AI models, making them suitable for deployment on smaller devices without significantly sacrificing performance. Techniques include Quantization (reducing precision), Fusing (combining layers), Factorization (breaking down complex operations), Pruning (removing unimportant neurons), and Knowledge Distillation (transferring knowledge from a large model to a smaller one).\nTinyML isn’t limited to microcontrollers with constrained resources. A core concept of TinyML is the Machine Learning Compiler (ML Compiler), which compiles pre-trained models into small, optimized executables. This approach can be applied to large deep neural network (DNN) models, such as large language models (LLMs), to reduce operational expenses (OPEX) in datacenters.\n\n\n\nTraining Phase: In this phase, the AI model learns from data. This involves feeding the model large datasets and iteratively adjusting its parameters to minimize prediction errors. The training phase requires significant computational power and is usually performed on powerful servers or cloud infrastructure.\nInference Phase: This is the phase where the trained model is used to make predictions or decisions based on new data. Unlike the training phase, inference can occur on much smaller devices, making it practical for real-world applications where quick responses are needed, and continuous connectivity to powerful servers is impractical.\n\nCurrently, our focus is on Post-Training Quantization (PTQ) and other post-training compression techniques, which are crucial for making models efficient enough to run on limited hardware.\n\n\n\nThe provided diagram outlines a streamlined process to make AI models efficient and deployable on small devices.\n\nHere’s a detailed explanation of each step:\n\nUpload:\n\nUsers upload their AI models, datasets, and information about their devices to the TinyMLaaS platform. The platform uses Huggingface storage for managing these uploads, ensuring secure and efficient handling of the data.\n\nRegister:\n\nThe platform registers the user’s device, model, and dataset. This step involves creating records of the hardware specifications, model details, and dataset characteristics, ensuring the platform understands the user’s specific requirements.\n\nRegister Compiler:\n\nThe platform registers a compiler tailored for the machine learning model. This compiler employs various optimization techniques such as:\n\nQuantization: Reducing the precision of the model’s calculations, which decreases the model size and computational load.\nFusing: Combining multiple layers of the model to streamline operations.\nSparsifying: Removing less important parts of the model to further reduce its size.\nPruning: Eliminating redundant or less significant neurons in the model, reducing complexity.\nKnowledge Distillation: Training a smaller model to mimic the behavior of a larger model, transferring its knowledge efficiently.\n\nAdditionally, the platform registers an installer that handles hardware optimization and can support software over-the-air updates (SOTA) for seamless deployment.\n\nExecute:\n\nThe platform executes the compiler and optimizer, generating a custom runtime environment. This process transforms the original AI model into a more compact and efficient version, specifically tuned to run on the user’s hardware.\n\nCompare:\n\nUsers can compare the original and optimized models based on metrics such as accuracy, memory size, and speed. This comparison helps ensure that the optimized model still meets the necessary performance criteria while being more efficient.\n\nDownload:\n\nUsers download the optimized model for installation on their devices. In some cases, if SOTA is supported, the TinyMLaaS platform can directly install the model onto the devices.\n\nInstall:\n\nUsers install the optimized AI model on their device, setting it up to run efficiently. This step is crucial for enabling the device to perform intelligent tasks, leveraging the compressed model for real-time inference.\n\n\n\n\n\n\nStreamlined Process: The process is designed to be straightforward and user-friendly, even for those who are not experts in AI or machine learning.\nFlexibility: The platform supports registering any compiler and installer as Docker images, allowing for flexible and customizable pipelines.\nEfficiency: The service focuses on making AI models smaller and faster without significantly sacrificing accuracy, enabling deployment on devices with limited resources.\nDeployment: Optimized AI models can be deployed on various devices, from industrial robots to satellites, making the technology versatile and widely applicable.\n\nBy using TinyMLaaS, users can leverage advanced AI capabilities on small devices. This is particularly useful for applications that require low latency, privacy, and reduced reliance on constant internet connectivity, without needing deep expertise in AI compression techniques. Additionally, the ML Compiler approach can be applied to large DNN models to reduce operational costs in datacenters, making it a versatile solution across different scales and use cases."
  },
  {
    "objectID": "blogs/dnn_hsi_opt.html",
    "href": "blogs/dnn_hsi_opt.html",
    "title": "Optimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data",
    "section": "",
    "text": "Introduction\nHyperspectral Imaging (HSI) is a powerful technique that captures a wide spectrum of light across dozens to hundreds of narrow wavelength bands. Unlike traditional RGB imaging, which only captures three broad bands of red, green, and blue, HSI provides rich spectral information for each pixel, enabling detailed analysis of the material properties, composition, and other characteristics of the observed objects.\nHowever, this richness comes at a cost: HSI data is massive and complex, requiring sophisticated processing techniques to extract useful information. Deep Neural Networks (DNNs) are a natural choice for processing such data due to their ability to model complex patterns and relationships. But given the high dimensionality and large data volumes inherent to HSI, optimizing DNNs for this task is crucial to ensure efficiency and effectiveness. This article explores the key strategies for optimizing DNN models tailored for HSI data.\n\n\n\n1. Dimensionality Reduction\nHSI datasets often contain hundreds of spectral bands, leading to high-dimensional data that can be computationally expensive to process and prone to overfitting. Dimensionality reduction techniques help mitigate these issues by reducing the number of input features while preserving the essential information.\n\nPrincipal Component Analysis (PCA): PCA is widely used to reduce the dimensionality of HSI data by transforming it into a set of orthogonal components that capture the most variance in the data. This reduces the input size for DNNs, making the models more manageable and less computationally intensive.\nLinear Discriminant Analysis (LDA): LDA is particularly useful in supervised learning tasks, as it maximizes class separability by finding the linear combinations of features that best separate the classes. This helps in focusing the DNN on the most discriminative features.\nNon-negative Matrix Factorization (NMF): NMF decomposes the data into non-negative components, making it particularly suitable for HSI data, which often consists of non-negative values. This method not only reduces dimensionality but also facilitates interpretability, as the resulting components often correspond to physically meaningful spectra.\n\n\n\n2. Custom Network Architectures\nGiven the unique characteristics of HSI data, custom DNN architectures can be designed to exploit both the spectral and spatial dimensions effectively.\n\n3D Convolutional Neural Networks (3D-CNNs): Unlike traditional 2D CNNs used for RGB images, 3D-CNNs process data across three dimensions: height, width, and spectral depth. This allows the model to simultaneously capture spatial patterns and spectral features, making it ideal for HSI data.\nSpectral-Spatial Networks: These networks separately handle spectral and spatial features before combining them. For example, 1D CNNs can be used to process spectral data, while 2D CNNs handle spatial data. This approach ensures that the model effectively captures relevant features from both domains.\nCapsule Networks: Capsule networks preserve the hierarchical relationships between features, which can be particularly useful when dealing with the complex and multidimensional nature of HSI data. They can enhance the model’s ability to understand spatial hierarchies and the orientation of spectral features.\n\n\n\n3. Lightweight Models and Compression Techniques\nDue to the large size and complexity of HSI data, DNN models can become computationally expensive to train and deploy. Techniques to reduce the model size and complexity are essential for making them practical, especially in resource-constrained environments.\n\nKnowledge Distillation: This technique involves training a smaller “student” model to mimic the predictions of a larger “teacher” model. The student model, while being less complex, learns to approximate the teacher’s performance, making it more efficient for processing HSI data.\nModel Compression: Compression techniques such as quantization, pruning, and low-rank approximation can significantly reduce the size of a DNN without drastically affecting its performance. For instance, pruning removes less important neurons or filters, reducing computational cost and memory usage.\nLightweight Architectures: Adapting lightweight models like MobileNet or EfficientNet for HSI data processing can significantly improve efficiency. These architectures are designed to be computationally efficient and can be customized to handle the higher dimensionality of HSI data.\n\n\n\n4. Data Augmentation and Regularization\nHSI datasets are often smaller compared to RGB datasets, which increases the risk of overfitting. Data augmentation and regularization are crucial for improving the generalization capability of DNN models.\n\nData Augmentation: Techniques such as spectral variation, spatial transformations, and noise injection can increase the diversity of the training data, helping the model to generalize better to unseen data. This is particularly important in HSI, where the data can be highly variable depending on the environmental conditions.\nRegularization Techniques: Methods like dropout, L2 regularization, and spectral regularization are important for preventing overfitting. Regularization helps the model to remain robust and generalize well, even when trained on high-dimensional HSI data.\n\n\n\n5. Multi-Scale Learning\nHSI data often contains features at multiple spatial or spectral scales, making multi-scale learning an effective approach.\n\nMulti-Scale CNNs: These networks use filters of different sizes to capture features at various scales, allowing the model to understand both fine-grained details and broader patterns in the HSI data. This is crucial for accurately capturing the complex relationships inherent in HSI data.\nAttention Mechanisms: Attention mechanisms enable the model to focus on the most relevant spectral bands or spatial regions, improving both efficiency and accuracy. By selectively focusing on important features, attention mechanisms help the model to prioritize the most critical information, reducing computational overhead.\n\n\n\nConclusion\nOptimizing DNN models for processing HSI data is essential due to the high dimensionality and complexity of the data. Through dimensionality reduction, custom architectures, model compression, data augmentation, regularization, and multi-scale learning, it is possible to develop efficient and effective models for HSI applications. These optimizations not only make HSI processing more practical but also open up new possibilities for deploying such models in real-world scenarios, including mobile and edge computing environments. As technology continues to advance, the integration of HSI with optimized DNNs will likely play a crucial role in a wide range of fields, from agriculture and environmental monitoring to healthcare and beyond."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html",
    "href": "blogs/edgeimplusestudio.html",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse has rapidly become a prominent player in the field of machine learning for edge devices. Tailored to cater to a wide range of users, from hobbyists to professional embedded engineers, Edge Impulse Studio offers a comprehensive platform that supports the development, training, and deployment of machine learning models directly onto edge devices.\n\n\nEdge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools\n\n\n\n\n\n\n\nEdge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments.\n\n\n\n\n\nEdge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#platform-overview",
    "href": "blogs/edgeimplusestudio.html#platform-overview",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse Studio is a cloud-based platform designed to simplify the modeling and deployment process of machine learning models on edge devices. It’s structured to accommodate a variety of user levels, from beginners exploring ML for the first time to seasoned professionals who require advanced features for enterprise applications. The platform supports multiple subscription tiers: Community, Professional, and Enterprise, each unlocking different levels of functionality and support.\nThe architecture of Edge Impulse is modular and flexible, allowing users to integrate different data sources, customize processing pipelines, and deploy models on various devices. The platform is organized into “impulse blocks” that guide users through the ML workflow, from data acquisition to model deployment. This block-based approach ensures that each step is transparent and manageable, especially for users with lower level of expertise.\n\n\n\nEdge Impulse general architecture & integrations with existing ML tools"
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#key-features",
    "href": "blogs/edgeimplusestudio.html#key-features",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse offers a robust suite of tools for data collection and management, making it easy to gather the data necessary for training models.\n\nMultiple Data Collection Methods: Users can collect data directly from devices, upload existing datasets, or even pull data from cloud storage solutions like Amazon S3.\nReal-Time Data Collection: The platform supports real-time data collection through phones, computers, or connected development boards. This versatility allows users to capture images, audio, and motion data using a web-based interface.\nData Explorer: This tool provides powerful data visualization capabilities, helping users to explore and understand their datasets. Enterprise users can also monitor model performance with new data and automate data processing, enhancing the platform’s scalability and efficiency.\nSynthetic Data Generation: While not fully tested, Edge Impulse offers tools to generate synthetic data, which can be valuable in scenarios where real-world data is scarce.\n\n\n\n\nEdge Impulse simplifies the machine learning process with a clear, step-by-step approach:\n\nImpulse Block Design: The platform’s ML workflow is divided into four block types: data extraction, data processing, training, and output. This modular approach ensures that users can easily follow the process and make adjustments as needed.\nDefault and Custom Processing Blocks: Users can choose from default data processing and training blocks or create custom models using tools like Keras. This flexibility is key for users with specific needs or those looking to experiment with novel approaches.\nComprehensive Visualization and Reporting: The platform provides a range of visualization tools, including feature importance analysis, confusion matrices, and performance profiles, which help users evaluate model accuracy and optimize performance.\n\n\n\n\nImpulse Block Design & Workflow\n\n\n\n\n\nOne of the standout features of Edge Impulse is its strong focus on edge devices:\n\nPerformance Profiling: The platform provides performance profiles for selected devices, detailing key metrics like RAM usage, disk space, and processing speed.\nDSP and Transformation Compilation: Users can compile digital signal processing (DSP) transformations and inference code specifically for their target devices, ensuring that models run efficiently on hardware with limited resources.\nHyperparameter Tuning: The platform offers tools for tuning model parameters and selecting the best-performing configurations for a given device, which is crucial for optimizing model deployment on resource-constrained environments."
  },
  {
    "objectID": "blogs/edgeimplusestudio.html#conclusion",
    "href": "blogs/edgeimplusestudio.html#conclusion",
    "title": "Edge Impulse Review",
    "section": "",
    "text": "Edge Impulse is a versatile, well-designed platform for developing and deploying machine learning models on edge devices. With its extensive feature set, modular workflow, and strong support for real-time data collection, it caters to both beginners and advanced users alike.\nFor more detailed information, visit the Edge Impulse homepage and Edge Impulse documentation."
  },
  {
    "objectID": "tutorials/runwalkthrough.html",
    "href": "tutorials/runwalkthrough.html",
    "title": "Run Walkthrough",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#summary",
    "href": "tutorials/runwalkthrough.html#summary",
    "title": "Run Walkthrough",
    "section": "Summary",
    "text": "Summary\nThe video tutorial is directly obtained from our internal review process of MVP6. It covers the process of creating and reviewing two default runs via project dashboard. Starting with user registration, the current MVP creates a default project and redirects the user to the project dashboard, in which page the main operation takes place. With the project dashboard, the user can specify a model and a dataset to run inference on a selected device. By default, two versions of this inference task are executed (forming two separate runs): a version that uses the original model for inference via Pytorch runtime, and a version that uses a quantized model (8-bit quantization) via tinyruntime (our custom runtime). After both runs are finished, the frontend provides visualizations to compare the results (accuracy), speed, and size of the model for each inference task.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/runwalkthrough.html#features-breakdown",
    "href": "tutorials/runwalkthrough.html#features-breakdown",
    "title": "Run Walkthrough",
    "section": "Features Breakdown",
    "text": "Features Breakdown\n\nRegistration & Project Setup\nThe registration process in MVP6 is simplified, only requiring the user’s email address and a secure password. Email verification is yet to be implemented at this stage. After a successful registration, the user is redirected to the project dashboard with a default name and description, both of which can be modified later on. New projects can be created via sidebar button, which requires a unique name and possibly some description. Different projects can be selected also via the sidebar, which navigates to the project dashboard page of the corresponding selected project.\n\n\nModel & Dataset Selection\nWe are currently using models and datasets uploaded to Hugging Face Hub. The user can choose any publicly accessible models or datasets from Hugging Face. However, the currently recommended ones are our own uploaded models (ninjalabo/resnet18, ninjalabo/resnet34, and ninjalabo/resnet50) and dataset (ninjalabo/imagenette2-320) for consistency in directory structure and accepted file formats.\nThe user can also upload models and datasets to our own UI, which accepts a model file model.pkl and a dataset file in .zip or .tar.gz format. For convenience, the user can download our example model and dataset to easily test this feature without compatability issue.\n\n\nDevice Selection\nThe user can setup and select an edge device in which the inference task(s) shall be executed. The user can register a device with information including its architecture, connectivity details, memory and storage limits, and installation option. Currently, we are supporting devices with arm64 or amd64 architectures, in which the model will be installed in the form of a Docker image in our VM. The model will be optimized to the selected device, and its performance on it is measured and reported (accuracy, execution time, and model size).",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Run Walkthrough"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "tinyRuntime review\n\n\n\n\n\n\nTech\n\n\n\nReviewing the current status of our lightweight runtime\n\n\n\n\n\nOct 26, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation\n\n\n\n\n\n\nTech\n\n\n\nOverview of Knowledge Distillation: Simplifying Deep Learning Models\n\n\n\n\n\nSep 28, 2024\n\n\nLeila Mozaffari\n\n\n\n\n\n\n\n\n\n\n\n\nDemistifying TinyMLaaS\n\n\n\n\n\n\nTech\n\n\nTinyMLaaS\n\n\n\nBrief overview of the technology behind NinjaLABO’s TinyML as-a-Service\n\n\n\n\n\nSep 27, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nLLM on Camera as an Universal Sensor for Smart Mobility - Privacy-Preserving Edge AI PoC\n\n\nBeyond 1984: Smart Cities Powered by Ethical AI and Citizen Empowerment\n\n\n\nUseCase\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nToDo list in Autumn 2024\n\n\n\n\n\n\nWoW\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Insights: Evolving tinyMLaaS with Flexible Pipelines and Distributed Execution\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for Software Engineer - AI Model Compression & DNN Runtime\n\n\n\n\n\n\nHiring\n\n\n\n\n\n\n\n\n\nAug 29, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing a DNN Model for Processing Hyperspectral Imaging (HSI) Data\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Overview of DNN Model Compression Techniques\n\n\n\n\n\n\nTech\n\n\nDNN\n\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Real-Time Face Identification System: A Comprehensive Guide\n\n\n\n\n\n\nTech\n\n\nUseCase\n\n\n\nExplore how to build a real-time face identification system using Ultralytics YOLOv8, a state-of-the-art object detection model known for its speed and accuracy. We’ll compare various tools, including managed services like AWS Rekognition and Azure Face API, to help you choose the best solution for your needs. Whether you’re a developer seeking full control over your models or looking for an easy-to-integrate, scalable solution, this guide will provide you with the knowledge and tools to create a powerful face identification system.\n\n\n\n\n\nAug 18, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying PyTorch Static Quantization\n\n\n\n\n\n\nTech\n\n\n\nA deep dive into how PyTorch performs inference with quantized models.\n\n\n\n\n\nAug 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service with Helsinki University summer course 2023\n\n\n\n\n\n\nHU\n\n\nTinyMLaaS\n\n\n\nThis is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\nAbout\n\n\nTeam\n\n\n\nThe team of experts in software development, research, and design is working to enable businesses to integrate AI & Machine Learning applications on a wide range of devices without relying on expensive networking & cloud computing services thanks to their solution, TinyML as a Service.\n\n\n\n\n\nAug 6, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nAI Compression as-a-Service MVP6 review\n\n\n\n\n\n\nNews\n\n\nMVP\n\n\n\nOur internal review of MVP6 usage\n\n\n\n\n\nAug 6, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Impulse Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nEdge Impulse - A Comprehensive Review of the Leading Edge AI Platform\n\n\n\n\n\nAug 5, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service Overview\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of NinjaLABO’s TinyML as-a-Service / AI compression as-a-Service \n\n\n\n\n\nJul 17, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nIREE review\n\n\n\n\n\n\nTech\n\n\nCompiler\n\n\n\nBrief overview of IREE\n\n\n\n\n\nJul 12, 2024\n\n\nHaruka Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nImagimob Studio Review\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nCompetitor\n\n\n\nBrief overview of Imagimob Studio - A solution for Edge AI applications\n\n\n\n\n\nJul 8, 2024\n\n\nNghi Vo\n\n\n\n\n\n\n\n\n\n\n\n\nAI compression SaaS\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\n\nBrief overview of AI Compression\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML Use Cases\n\n\n\n\n\n\nTech\n\n\nSaaS\n\n\nUseCase\n\n\n\nUse cases for TinyML offered by NinjaLABO\n\n\n\n\n\nJul 2, 2024\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we democratize machine learning on IoT devices?\n\n\n\n\n\n\nEricsson\n\n\n\nMaking tiny machine learning widely available on edge IoT devices could prove to be a major leap in smart sensing across industries. Below, we plot the technical milestones to making that happen as part of our ongoing research into TinyML as-a-service.\n\n\n\n\n\nAug 6, 2020\n\n\nHiroshi Doyu\n\n\n\n\n\n\n\n\n\n\n\n\nTinyML as-a-Service and the challenges of machine learning at the edge\n\n\n\n\n\n\nEricsson\n\n\n\nThe TinyML as-a-Service project at Ericsson Research sets out to address the challenges that today limit the potential of machine learning (ML) paradigms at the edge of the embedded IoT world. In this post, the second post in our TinyML series, we take a closer look at the technical and non-technical challenges on our journey to making that happen. Learn more below.\n\n\n\n\n\nAug 6, 2019\n\n\nHiroshi Doyu\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Get Started",
      "Blog"
    ]
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html",
    "href": "blogs/demistify_tinymlaas.html",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "TinyMLaaS is an end-to-end service that supports the entire lifecycle of a machine learning (ML) model deployment - from dataset selection to running inference on real devices. This system streamlines the process by combining both frontend and backend services into a unified platform known as NinjaLABO Studio. This blog post will guide you through how the system functions, covering essential components like pipelines, artifacts, GitHub automation, Docker integration, and more.\n\n\nOur research results are published via a Landing Page that features documentation, research blogs, and performance reports. The page helps users stay updated with the latest performance benchmarks and model optimizations.\n\nResearch Blogs: Published in formats such as .ipynb, .md, or .qmd using Quarto architecture.\nPerformance Updates: Automated updates on model performance are triggered through GitHub Actions.\nHTML Page Generation: GitHub Actions are also employed to publish performance metrics and blog posts as .html pages for easy access through the landing page.\n\n\n\n\nThe functionalities of TinyMLaaS are delivered through its Studio interface, where users interact with the system. The Studio is constructed from coordinating a backend API developed with FastAPI and frontend interface built with Streamlit.\nThe Studio allows users to manage projects and monitor their deployment pipelines. Some notable features of the Studio include:\n\nProject dashboard: The configurations of deployment pipelines are simplified into a singular dashboard. The user can easily spawns multiple runs with different models, datasets, and compression techniques under a project.\n\nResult comparison and visualization: After multiple pipelines (runs) are complete, the user can compare their inference accuracy, execution time, and model size. The user can also visualize such differences and trade-off between these metrics to select the most suitable compression techniques.\n\nUpload models & datasets: The user can upload their own models and datasets following our accepted format.\n\nExport inference image: The user can export artifacts from their pipeline including the compressed model file and inference image to run on their local device.\n\nEmail notifications: The user is notified about the pipeline status (success / error) via emails.\n\n\n\n\nProject Dashboard UI\n\n\nFor more information about the user experience, please check out the walkthrough tutorial!\n\n\nTinyMLaaS utilizes a system with high-degree integration with Hugging Face and Docker, with the main components being:\n\nUser: Entity representing the user’s requests through Studio.\n\nEndpoint: The API layer that handles requests and manages interactions with backend services.\n\nDatabase: Stores records related to models, datasets, runs, and their statuses.\n\nVolume: Dedicated storage for artifacts generated by Docker containers.\n\nHugging Face: Provides access to pre-trained models and datasets.\n\nDocker Client: Executes tasks inside containers, including compiling models, installing models to a specified device, and running inference.\n\n\n\n\nThe Studio operates via an API layer that coordinates user requests, Docker containers, Hugging Face datasets, and device installations to construct the deployment pipeline of a machine learning model on edge device. The pipeline steps are:\n\nCreate a Run: A new run is created for a project, starting the entire process.\nDownload or Extract Models and Datasets: The model is either fetched from Hugging Face or extracted from a user-provided file.\nCompile Model: The model is compiled inside a Docker container.\nInstall Model on Device: The compiled model is installed on the specified device.\nRun Inference: Inference is executed using the test dataset on the device.\n\n\n\n\n\n\n\n---\ntitle: Pipeline interactions\n---\nsequenceDiagram\n  actor User\n  participant Endpoint\n  participant Database\n  participant Volume\n  participant HuggingFace\n  participant DockerClient\n\n  note over User,Volume: New run\n  User-&gt;&gt;Endpoint: Create a new run\n  Endpoint-&gt;&gt;+Database: add record\n  Endpoint-&gt;&gt;+Volume: create new directory as run volume\n  Endpoint--&gt;&gt;User: 201 CREATED\n  \n  note over User,HuggingFace: Download Model & Dataset from Hugging Face\n  User-&gt;&gt;+Endpoint: Download model & dataset from HF\n  Endpoint--&gt;&gt;+HuggingFace: verify access to HF ID\n  HuggingFace--&gt;&gt;-Endpoint: confirm accessibility\n  Endpoint--&gt;&gt;HuggingFace: request downloading\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  HuggingFace-&gt;&gt;Volume: download\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,Volume: Upload customized Model & Dataset\n  User-&gt;&gt;+Endpoint: Upload model & dataset\n  Endpoint--&gt;&gt;Volume: upload temporary file & request extracting / copying to run directory\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  Volume-&gt;&gt;Volume: extracting .zip / copying model.pkl\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Compile model\n  User-&gt;&gt;+Endpoint: Compile model\n  Endpoint-&gt;&gt;Volume: update compression configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Install model on device\n  User-&gt;&gt;+Endpoint: Register device\n  Endpoint-&gt;&gt;Database: add records of device information\n  Endpoint--&gt;&gt;-User: 201 CREATED\n\n  User-&gt;&gt;+Endpoint: Install model on device\n  Endpoint-&gt;&gt;Volume: update runtime & device configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Run inference\n  User-&gt;&gt;+Endpoint: Run inference on device\n  Endpoint--&gt;&gt;+Volume: extract inference image name\n  Volume--&gt;&gt;-Endpoint: return image name (result of installation)\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n\n\n\n\n\n\n\n\n\n\nTinyMLaaS simplifies the entire machine learning model lifecycle, from research to deployment on edge devices. With seamless integration between Docker and Hugging Face, users can manage models, datasets, and pipelines through an intuitive Studio interface. By automating key tasks like model compilation, deployment, and performance monitoring, TinyMLaaS enables fast, efficient edge AI development, making it a powerful solution for professionals and researchers alike."
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#research-and-performance",
    "href": "blogs/demistify_tinymlaas.html#research-and-performance",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "Our research results are published via a Landing Page that features documentation, research blogs, and performance reports. The page helps users stay updated with the latest performance benchmarks and model optimizations.\n\nResearch Blogs: Published in formats such as .ipynb, .md, or .qmd using Quarto architecture.\nPerformance Updates: Automated updates on model performance are triggered through GitHub Actions.\nHTML Page Generation: GitHub Actions are also employed to publish performance metrics and blog posts as .html pages for easy access through the landing page."
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#user-experience-with-ninjalabo-studio",
    "href": "blogs/demistify_tinymlaas.html#user-experience-with-ninjalabo-studio",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "The functionalities of TinyMLaaS are delivered through its Studio interface, where users interact with the system. The Studio is constructed from coordinating a backend API developed with FastAPI and frontend interface built with Streamlit.\nThe Studio allows users to manage projects and monitor their deployment pipelines. Some notable features of the Studio include:\n\nProject dashboard: The configurations of deployment pipelines are simplified into a singular dashboard. The user can easily spawns multiple runs with different models, datasets, and compression techniques under a project.\n\nResult comparison and visualization: After multiple pipelines (runs) are complete, the user can compare their inference accuracy, execution time, and model size. The user can also visualize such differences and trade-off between these metrics to select the most suitable compression techniques.\n\nUpload models & datasets: The user can upload their own models and datasets following our accepted format.\n\nExport inference image: The user can export artifacts from their pipeline including the compressed model file and inference image to run on their local device.\n\nEmail notifications: The user is notified about the pipeline status (success / error) via emails.\n\n\n\n\nProject Dashboard UI\n\n\nFor more information about the user experience, please check out the walkthrough tutorial!\n\n\nTinyMLaaS utilizes a system with high-degree integration with Hugging Face and Docker, with the main components being:\n\nUser: Entity representing the user’s requests through Studio.\n\nEndpoint: The API layer that handles requests and manages interactions with backend services.\n\nDatabase: Stores records related to models, datasets, runs, and their statuses.\n\nVolume: Dedicated storage for artifacts generated by Docker containers.\n\nHugging Face: Provides access to pre-trained models and datasets.\n\nDocker Client: Executes tasks inside containers, including compiling models, installing models to a specified device, and running inference.\n\n\n\n\nThe Studio operates via an API layer that coordinates user requests, Docker containers, Hugging Face datasets, and device installations to construct the deployment pipeline of a machine learning model on edge device. The pipeline steps are:\n\nCreate a Run: A new run is created for a project, starting the entire process.\nDownload or Extract Models and Datasets: The model is either fetched from Hugging Face or extracted from a user-provided file.\nCompile Model: The model is compiled inside a Docker container.\nInstall Model on Device: The compiled model is installed on the specified device.\nRun Inference: Inference is executed using the test dataset on the device.\n\n\n\n\n\n\n\n---\ntitle: Pipeline interactions\n---\nsequenceDiagram\n  actor User\n  participant Endpoint\n  participant Database\n  participant Volume\n  participant HuggingFace\n  participant DockerClient\n\n  note over User,Volume: New run\n  User-&gt;&gt;Endpoint: Create a new run\n  Endpoint-&gt;&gt;+Database: add record\n  Endpoint-&gt;&gt;+Volume: create new directory as run volume\n  Endpoint--&gt;&gt;User: 201 CREATED\n  \n  note over User,HuggingFace: Download Model & Dataset from Hugging Face\n  User-&gt;&gt;+Endpoint: Download model & dataset from HF\n  Endpoint--&gt;&gt;+HuggingFace: verify access to HF ID\n  HuggingFace--&gt;&gt;-Endpoint: confirm accessibility\n  Endpoint--&gt;&gt;HuggingFace: request downloading\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  HuggingFace-&gt;&gt;Volume: download\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,Volume: Upload customized Model & Dataset\n  User-&gt;&gt;+Endpoint: Upload model & dataset\n  Endpoint--&gt;&gt;Volume: upload temporary file & request extracting / copying to run directory\n  Endpoint-&gt;&gt;Database: add record of resource 'status'=running\n  Volume-&gt;&gt;Volume: extracting .zip / copying model.pkl\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Compile model\n  User-&gt;&gt;+Endpoint: Compile model\n  Endpoint-&gt;&gt;Volume: update compression configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Install model on device\n  User-&gt;&gt;+Endpoint: Register device\n  Endpoint-&gt;&gt;Database: add records of device information\n  Endpoint--&gt;&gt;-User: 201 CREATED\n\n  User-&gt;&gt;+Endpoint: Install model on device\n  Endpoint-&gt;&gt;Volume: update runtime & device configurations\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK\n\n  note over User,DockerClient: Run inference\n  User-&gt;&gt;+Endpoint: Run inference on device\n  Endpoint--&gt;&gt;+Volume: extract inference image name\n  Volume--&gt;&gt;-Endpoint: return image name (result of installation)\n  Endpoint-&gt;Volume: spawn Docker container\n  Volume-&gt;&gt;+DockerClient: with mounted model & dataset\n  Endpoint-&gt;&gt;Database: update resource 'status'=running  \n  DockerClient-&gt;&gt;DockerClient: run & remove container\n  DockerClient-&gt;&gt;-Volume: update results to run directory\n  Endpoint-&gt;&gt;Database: update resource 'status'=generated\n  Endpoint--&gt;&gt;-User: 200 OK"
  },
  {
    "objectID": "blogs/demistify_tinymlaas.html#conclusion",
    "href": "blogs/demistify_tinymlaas.html#conclusion",
    "title": "Demistifying TinyMLaaS",
    "section": "",
    "text": "TinyMLaaS simplifies the entire machine learning model lifecycle, from research to deployment on edge devices. With seamless integration between Docker and Hugging Face, users can manage models, datasets, and pipelines through an intuitive Studio interface. By automating key tasks like model compilation, deployment, and performance monitoring, TinyMLaaS enables fast, efficient edge AI development, making it a powerful solution for professionals and researchers alike."
  },
  {
    "objectID": "blogs/HU_Summer_2023.html",
    "href": "blogs/HU_Summer_2023.html",
    "title": "TinyML as-a-Service with Helsinki University summer course 2023",
    "section": "",
    "text": "TinyMLaaS\n\n\nRead more…"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html",
    "href": "blogs/hiring_2024_fall.html",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Location: Remote (Occasional meetings at A-Grid, Otaniemi)\nCompany: NinjaLABO\nDuration: Full-time or Part-time for 3 months (ASAP to the end of December).\n\n\n\nNinjaLABO is a fresh, finnish startup at the forefront of AI innovation, focusing on AI model compression and deep neural network (DNN) runtime optimization for IoT, particularly for enabling onboard Edge AI in satellite systems.\n\n\n\n\n\nImplement DNN model compression techniques.\nIntegrate the above into our SaaS platform, TinyML as-a-Service.\nPort and optimize DNN runtimes for Edge devices such as Nvidia Jetson and Orange Pi 5+.\n\n\n\n\n\nStrong proficiency in Python and C/C++.\nFamiliarity with PyTorch or TensorFlow.\n\n\n\n\n\nHands-on experience with Nvidia Jetson & Orange Pi 5+.\nExperience in AI model compression and DNN runtime development.\nKnowledge of TensorFlow Lite for Microcontrollers, MLIR, and IREE.\nKnowledge of HyperSpectral Imaging (HSI).\n\n\n\n\n\nSuccessful completion of CS-E4890 - Deep Learning D with good grades.\nSuccessful completion of CS-E4580 - Programming Parallel Computers D with good grades.\n\n\n\n\n\nFull-time preferred, part-time candidates must be available for more than 3 days per week.\nDiscord for daily communication\nDaily Scrum at 8:00am every weekday via Discord\nGitHuB Project as Kanban board\nGitHub Workflow/Actions as CI/CD\n2 weeks Sprint planning / review / retro under SCRUM\n\n\n\n\n\n3,000+€ for PhD students\n2,600-2,800€ for Master’s students\n2,100-2,400€ for Bachelor’s students\n\nContracts facilitated through UKKO.fi.\n\n\n\nInterested? Apply to Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#about-us",
    "href": "blogs/hiring_2024_fall.html#about-us",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "NinjaLABO is a fresh, finnish startup at the forefront of AI innovation, focusing on AI model compression and deep neural network (DNN) runtime optimization for IoT, particularly for enabling onboard Edge AI in satellite systems."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#key-responsibilities",
    "href": "blogs/hiring_2024_fall.html#key-responsibilities",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Implement DNN model compression techniques.\nIntegrate the above into our SaaS platform, TinyML as-a-Service.\nPort and optimize DNN runtimes for Edge devices such as Nvidia Jetson and Orange Pi 5+."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#requirements",
    "href": "blogs/hiring_2024_fall.html#requirements",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Strong proficiency in Python and C/C++.\nFamiliarity with PyTorch or TensorFlow."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#preferred-qualifications",
    "href": "blogs/hiring_2024_fall.html#preferred-qualifications",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Hands-on experience with Nvidia Jetson & Orange Pi 5+.\nExperience in AI model compression and DNN runtime development.\nKnowledge of TensorFlow Lite for Microcontrollers, MLIR, and IREE.\nKnowledge of HyperSpectral Imaging (HSI)."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#preferred-education",
    "href": "blogs/hiring_2024_fall.html#preferred-education",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Successful completion of CS-E4890 - Deep Learning D with good grades.\nSuccessful completion of CS-E4580 - Programming Parallel Computers D with good grades."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#wow",
    "href": "blogs/hiring_2024_fall.html#wow",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Full-time preferred, part-time candidates must be available for more than 3 days per week.\nDiscord for daily communication\nDaily Scrum at 8:00am every weekday via Discord\nGitHuB Project as Kanban board\nGitHub Workflow/Actions as CI/CD\n2 weeks Sprint planning / review / retro under SCRUM"
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#monthly-pay",
    "href": "blogs/hiring_2024_fall.html#monthly-pay",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "3,000+€ for PhD students\n2,600-2,800€ for Master’s students\n2,100-2,400€ for Bachelor’s students\n\nContracts facilitated through UKKO.fi."
  },
  {
    "objectID": "blogs/hiring_2024_fall.html#how-to-apply",
    "href": "blogs/hiring_2024_fall.html#how-to-apply",
    "title": "Looking for Software Engineer - AI Model Compression & DNN Runtime",
    "section": "",
    "text": "Interested? Apply to Hiroshi Doyu &lt;hiroshi.doyu@ninjalabo.ai&gt;"
  },
  {
    "objectID": "blogs/pytorch_staticq.html",
    "href": "blogs/pytorch_staticq.html",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "",
    "text": "In the world of machine learning, optimizing model performance and efficiency is crucial, especially for deploying models on edge devices with limited resources. One powerful technique to achieve this is quantization, which reduces the precision of the numbers used in a model’s computations. PyTorch supports two types of quantization: dynamic and static. Dynamic quantization adjusts the precision of weights at runtime, while static quantization involves converting the model’s weights and activations to lower precision based on calibration data. This article will focus on statically quantized models, breaking down the core concepts and steps involved in PyTorch’s approach to inference with these models.\nNote: This article assumes you are already familiar with quantization, particularly static quantization. If not, I recommend checking out the some materials, e.g., our technology page for an introduction.\nfrom fastai.vision.all import *\n\nimport torch\nfrom torch.ao.quantization import get_default_qconfig_mapping\nimport torch.ao.quantization.quantize_fx as quantize_fx\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\nLet’s start by creating a Quantizer class to quantize a PyTorch model. For an introduction to PyTorch quantization, you can refer to the official documentation. As an example, I will use the Imagenette2-320 dataset and the ResNet18 model. For convenience, I will leverage the Fastai learner to streamline this process.\nclass Quantizer():\n    def __init__(self, backend=\"x86\"):\n        self.qconfig = get_default_qconfig_mapping(backend)\n        torch.backends.quantized.engine = backend\n\n    def quantize(self, model, calibration_dls):\n        x, _ = calibration_dls.valid.one_batch()\n        model_prepared = prepare_fx(model.eval(), self.qconfig, x)\n        with torch.no_grad():\n            _ = [model_prepared(xb.to('cpu')) for xb, _ in calibration_dls.valid]\n\n        return model_prepared, convert_fx(model_prepared)\npath = untar_data(URLs.IMAGENETTE_320, data=Path.cwd()/'data')\ndls = ImageDataLoaders.from_folder(path, valid='val', item_tfms=Resize(224),\n                                   batch_tfms=Normalize.from_stats(*imagenet_stats))\nlearn = vision_learner(dls, resnet18)\nmodel_prepared, qmodel = Quantizer(\"qnnpack\").quantize(learn.model, learn.dls)\nIn static quantization, the scaling factors and zero points for weights and activations are determined after model calibration but before inference. In this context, we are using per-tensor quantization, which means that there is a single scaling factor and zero point applied uniformly across all elements in each tensor of a layer. This approach is straightforward and computationally efficient, as it simplifies the quantization process by treating the entire tensor as a whole.\nIn the above cell, model_prepared instance represents the model after it has recorded the range of activations across a validation dataset. This model contains the necessary information about the model structure and activation ranges, from which the scaling factors and zero points are calculated. Below is an example of the quantization parameters for some activations. The HistogramObserver is used to record the activation ranges. The first output shows the quantized parameters of the first activation, which is the model input, while the second output shows the quantization parameters of the second activation, which is the output of the first Conv2d + ReLU layer. In PyTorch, to avoid redundant quantization and dequantization processes between layers, batch normalization is folded into the preceding layer (batch normalization folding), and the ReLU layer is fused with the layer it follows.\n# Example activation quantization parameters\nfor i in range(3):\n    attr = getattr(model_prepared, f\"activation_post_process_{i}\")\n    scale, zero_p = attr.calculate_qparams()\n    print(\"{}\\nScaling Factor: {}\\nZero Point: {}\\n\".format(attr, scale.item(), zero_p.item()))\n\nHistogramObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\nScaling Factor: 0.018649335950613022\nZero Point: 114\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\n\nHistogramObserver(min_val=0.0, max_val=7.000605583190918)\nScaling Factor: 0.011327190324664116\nZero Point: 0\nqmodel instance represents the quantized model. It contains quantized weights, along with their associated scaling factor and zero point, as well as the scaling factor and zero point for activations. Additionally, it includes some non-quantized parameters, which I will explain later.\nqmodel\n\nGraphModule(\n  (0): Module(\n    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008901300840079784, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024013830348849297, zero_point=149, padding=(1, 1))\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007031331304460764, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031252723187208176, zero_point=156, padding=(1, 1))\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.007301042787730694, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019116230309009552, zero_point=124, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.01664934679865837, zero_point=135)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008282394148409367, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.02566305175423622, zero_point=137, padding=(1, 1))\n      )\n    )\n    (6): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010484358295798302, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02675902470946312, zero_point=90, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.008271278813481331, zero_point=162)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00832998938858509, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.027811763808131218, zero_point=142, padding=(1, 1))\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.006999513134360313, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.023119885474443436, zero_point=140, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.02033478580415249, zero_point=128)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.006345659960061312, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.12105856835842133, zero_point=88, padding=(1, 1))\n      )\n    )\n  )\n  (1): Module(\n    (0): Module(\n      (mp): AdaptiveMaxPool2d(output_size=1)\n      (ap): AdaptiveAvgPool2d(output_size=1)\n    )\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): QuantizedDropout(p=0.25, inplace=False)\n    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.08005672693252563, zero_point=0, qscheme=torch.per_tensor_affine)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): QuantizedDropout(p=0.5, inplace=False)\n    (8): QuantizedLinear(in_features=512, out_features=10, scale=0.10456003248691559, zero_point=150, qscheme=torch.per_tensor_affine)\n  )\n)\nLet’s investigate the first layer of qmodel, i.e., quantized Conv2d + ReLU layer.\nlayer = qmodel._modules['0']._modules['0']\nprint(layer)\nprint(\"Weight Scale: {}, Weight Zero Point: {}\".format(layer.weight().q_scale(),\n                                                       layer.weight().q_zero_point()))\nprint(\"Output Scaling Factor: {}, Output Zero Point: {}\\n\".format(layer.scale, \n                                                                  layer.zero_point))\n\nprint(\"Example weights:\", layer.weight()[0, 0, 0])\nprint(\"In integer representation:\", layer.weight()[0, 0, 0].int_repr())\n\nQuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.011327190324664116, zero_point=0, padding=(3, 3))\nWeight Scale: 0.0030892190989106894, Weight Zero Point: 0\nOutput Scaling Factor: 0.011327190324664116, Output Zero Point: 0\n\nExample weights: tensor([-0.0031,  0.0000,  0.0000,  0.0185,  0.0124,  0.0031, -0.0031],\n       size=(7,), dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n       scale=0.0030892190989106894, zero_point=0)\nIn integer representation: tensor([-1,  0,  0,  6,  4,  1, -1], dtype=torch.int8)\nAs shown above, the quantized layer contains two scaling factors and zero points: one for the weights and another for the output activation. You may have noticed that the output scaling factor and zero point are the same as those displayed in the second cell above, as they represent the same activation.\nWhat about biases, which I haven’t discussed yet? In PyTorch, bias quantization depends on the backend you use. For example, if you specify the x86 backend, biases are not quantized and are instead used as floating-point values. On the other hand, the QNNPACK backend quantizes biases. However, biases are not quantized during the initial quantization stage; they are quantized at inference time. Thus, even the inference uses quantized biases, PyTorch does not display the quantized biases at before inference. The formula for bias quantization in PyTorch is: \\[\nb_q = round(b / (si * sw))\n\\] , where \\(b_q\\) is quantized bias, \\(b\\) is bias before quantization, \\(si\\) is input activation scale and \\(sw\\) is weight scale. For more details, you can refer to this discussion.\nIn addition, the model may include other non-quantized parameters, such as parameters in batch normalization layers that are not fused. This is likely because quantizing the activations in these layers would not provide significant benefits."
  },
  {
    "objectID": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "href": "blogs/pytorch_staticq.html#what-happens-during-inference",
    "title": "Demystifying PyTorch Static Quantization",
    "section": "What happens during inference?",
    "text": "What happens during inference?\nThis section demonstrates how calculations are performed in the quantized model during inference. To illustrate this, I calculate the output of the first convolutional layer and validate it against the actual result.\n\nlayer_input = None\nlayer_output = None\n\ndef hook_fn(module, input, output):\n    global layer_output, layer_input\n    layer_input = input\n    layer_output = output\n\nimg = torch.rand([1, 3, 224, 224])\nhook = qmodel._modules['0']._modules['0'].register_forward_hook(hook_fn)\noutput = qmodel(img)\nhook.remove()\nprint(\"Example input:\", layer_input[0][0,0,0,:10].int_repr())\nprint(\"Example output:\", layer_output[0,0,0,:10].int_repr())\n\nExample input: tensor([163, 119, 155, 138, 126, 164, 115, 132, 115, 166], dtype=torch.uint8)\nExample output: tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.uint8)\n\n\n\nimport numpy as np\n\ndef quantize(x, qparams, itype):\n    xtype = torch.iinfo(itype)\n    return torch.clamp(torch.round(x / qparams[0]) + qparams[1], min=xtype.min, max=xtype.max)\n\ndef dequantize(x, qparams):\n    return (x - qparams[1]) * qparams[0]\n\ndef im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    N, C, H, W = input_data.shape\n    out_h = (H + 2 * pad - filter_h) // stride + 1\n    out_w = (W + 2 * pad - filter_w) // stride + 1\n\n    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride * out_h\n        for x in range(filter_w):\n            x_max = x + stride * out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n    return torch.tensor(col)\n\n# first use im2col, which is efficient way to perform Conv2d operation\ninp = im2col(img, 7, 7, 2, 3).float()\n# quantize input values using input scale and zero point\ninp = quantize(inp, [layer_input[0].q_scale(), layer_input[0].q_zero_point()], torch.uint8)\n# get quantized weights, weight scale and quantize biases\nw = qmodel._modules['0']._modules['0'].weight().int_repr().reshape(64, -1).float()\nsw = qmodel._modules['0']._modules['0'].weight().q_scale()\nb = quantize(qmodel._modules['0']._modules['0'].bias(),\n             [layer_input[0].q_scale() * sw, 0], torch.int32)\nb = b.reshape(1,64,1,1).detach()\n# calculate matmul in Conv2d and add biases\nout = (w @ (inp.T - layer_input[0].q_zero_point())).view(1,64,112,112) + b\n# dequantize, perform ReLU and quantize based on output scale and zero point\nout = out * sw * layer_input[0].q_scale()\nout = torch.relu(out)\nout = quantize(out, [layer_output.q_scale(), layer_output.q_zero_point()], torch.uint8)\n\n\ntorch.allclose(out, layer_output.int_repr().float())\nprint(\"Output: \", out[0, 0, 0, :10])\n\nOutput:  tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n\nOur calculation matches the actual result, which is a good sign. Although some operations in PyTorch’s implementation might be performed in a different order, the overall process is likely very similar."
  },
  {
    "objectID": "blogs/imagimobstudio.html",
    "href": "blogs/imagimobstudio.html",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is an end-to-end platform designed for developing Edge AI and Machine Learning applications. The platform covers the machine learning workflow from data collection to model deployment in embedded devices to support both experts and non-experts in building production grade models.\n\n\n\n\nImagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C.\n\n\n\n\n\n\nPredictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars.\n\n\n\n\nImagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/imagimobstudio.html#key-features",
    "href": "blogs/imagimobstudio.html#key-features",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio offers dynamic visualisations throughout the machine learning workflow, including the data distribution and model performance. Moreover, Imagimob Studio employs an intuitive interface to support its functionalities, an example of which is the session view with overlapping tracks for displaying the original audio track alongside labels, predicted labels, and processed tracks. This design choice facilitates the data annotation process, as well as providing an overview of model performance on a specific data file.\nIn a recent development, Graph UX is introduced which enables a different approach to visualise the machine learning workflow. This approach aligns with the representation of neural networks, in which the original data is processed layer-by-layer by passing through nodes of a connected network. Graph UX adopts such a concept and adapts it to the machine learning workflow. Each process in the overall flow, including data collection and annotation, preprocessing, and inference, becomes a node drawn in a canvas with defined inputs and outputs. This design provides a comprehensive view of these processes for better management and understanding of the workflow.\n\n\n\nMain Canvas for a Model Evaluation Graph UX project\n\n\n\n\n\n\nIntegration with Sensors: Easily connect and collect data from various sensors and hardware platforms, including those running Python.\n\nAnnotation Tools: Efficiently label and manage data with drag-and-drop capabilities, auto-annotation scripts, and visualisation tools to verify data consistency.\n\nDataset Management: Create, shuffle, and manage training, validation, and test sets.\n\nError Detection: Automatically detect and correct data inconsistencies to avoid costly mistakes.\n\n\n\n\nData management & annotation for an audio file with Session view\n\n\n\n\n\n\nAutoML: Automatically generate high-performance models tailored to your data.\n\nParallel Training: Train multiple models simultaneously in the cloud for faster results.\n\nCustom Models: Import and modify models from TensorFlow if needed.\n\nReal-Time Evaluation: Visualise model predictions on the same timeline as your data, allowing for thorough performance analysis before deployment.\n\nOne-Click Deployment: Convert models into optimised C code with a simple API for deployment on any platform supporting C."
  },
  {
    "objectID": "blogs/imagimobstudio.html#use-cases",
    "href": "blogs/imagimobstudio.html#use-cases",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Predictive Maintenance: Detects machine anomalies in real-time.\n\nAudio Applications: Classifies sound events and recognises sound environments.\n\nGesture Recognition: Detects hand gestures using sensors.\n\nSignal Classification: Identifies repeatable patterns from any sensor data.\n\nFall Detection: Utilises IMUs or accelerometers for accurate fall detection.\n\nMaterial Detection: Performs real-time material detection with low-power radars."
  },
  {
    "objectID": "blogs/imagimobstudio.html#conclusion",
    "href": "blogs/imagimobstudio.html#conclusion",
    "title": "Imagimob Studio Review",
    "section": "",
    "text": "Imagimob Studio is a powerful, user-friendly solution for developing edge AI applications. Its comprehensive feature set, intuitive interface, and robust support make it an ideal choice for both novice and experienced developers aiming to deploy machine learning models on edge devices.\nFor more detailed information, visit Imagimob Studio homepage and Imagimob Studio documentation."
  },
  {
    "objectID": "blogs/howto_enable_jupyter_remotely.html",
    "href": "blogs/howto_enable_jupyter_remotely.html",
    "title": "How to set up Jupyter notebook on a remote VM",
    "section": "",
    "text": "ssh root@gpu-instance-ip\njupyter notebook --no-browser --port=8888 --allow-root\nssh -N -L localhost:7777:localhost:8888 root@gpu-instance-ip\nhttps://www.scaleway.com/en/docs/tutorials/setup-jupyter-notebook/"
  },
  {
    "objectID": "blogs/tinyruntime.html",
    "href": "blogs/tinyruntime.html",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Performance comparison with PyTorch. For more details, visit https://ninjalabo.ai/performance.html.\n\n\nIn the evolving landscape of machine learning, efficient model deployment is paramount, especially in resource-limited environments. That’s where tinyRuntime comes in—a lightweight runtime designed to facilitate the fast inference of compressed ML models. Current tinyRuntime performance compared to PyTorch can be checked on our performance page.\n\n\nWritten in C, tinyRuntime is engineered for optimal performance on arm64 and x86 architectures. It addresses the growing need for a runtime that operates efficiently with minimal dependencies, making it ideal for applications where memory and computational resources are constrained.\n\n\n\n\nFast: tinyRuntime is optimized for speed, enabling quick model inference that can meet the demands of real-time applications.\nLightweight: With significantly lower memory requirements compared to traditional frameworks like PyTorch, tinyRuntime ensures that even devices with limited resources can effectively run ML models.\nScalable: Its design allows for scalability across various applications and use cases, making it adaptable to different industry needs.\n\n\n\n\nWhile tinyRuntime offers significant advantages, it does come with its own set of challenges:\n\nMaintenance Effort: The custom implementation of tinyRuntime necessitates ongoing effort to maintain and update the codebase, ensuring it remains efficient and relevant.\nHardware Support: Providing broad support for diverse hardware platforms and various ML architectures can be challenging, necessitating continuous optimization.\n\n\n\n\nMoving forward, our focus will be on:\n\nContinued Optimization: We will work on enhancing performance and expanding support for different hardware and ML architectures, ensuring tinyRuntime remains competitive and efficient.\nExploring Alternatives: We are considering exploring other runtimes like IREE, which already supports a variety of hardware. This could provide insights and solutions that may benefit tinyRuntime’s development.\n\n\n\n\ntinyRuntime is paving the way for efficient ML model inference in resource-constrained environments. With its fast performance and lightweight design, it can be an option for developers and organizations looking to deploy machine learning solutions effectively. We invite you to follow our journey as we continue to enhance tinyRuntime and explore new possibilities in the world of machine learning."
  },
  {
    "objectID": "blogs/tinyruntime.html#what-is-tinyruntime",
    "href": "blogs/tinyruntime.html#what-is-tinyruntime",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Written in C, tinyRuntime is engineered for optimal performance on arm64 and x86 architectures. It addresses the growing need for a runtime that operates efficiently with minimal dependencies, making it ideal for applications where memory and computational resources are constrained."
  },
  {
    "objectID": "blogs/tinyruntime.html#key-benefits-of-tinyruntime",
    "href": "blogs/tinyruntime.html#key-benefits-of-tinyruntime",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Fast: tinyRuntime is optimized for speed, enabling quick model inference that can meet the demands of real-time applications.\nLightweight: With significantly lower memory requirements compared to traditional frameworks like PyTorch, tinyRuntime ensures that even devices with limited resources can effectively run ML models.\nScalable: Its design allows for scalability across various applications and use cases, making it adaptable to different industry needs."
  },
  {
    "objectID": "blogs/tinyruntime.html#challenges-ahead",
    "href": "blogs/tinyruntime.html#challenges-ahead",
    "title": "tinyRuntime review",
    "section": "",
    "text": "While tinyRuntime offers significant advantages, it does come with its own set of challenges:\n\nMaintenance Effort: The custom implementation of tinyRuntime necessitates ongoing effort to maintain and update the codebase, ensuring it remains efficient and relevant.\nHardware Support: Providing broad support for diverse hardware platforms and various ML architectures can be challenging, necessitating continuous optimization."
  },
  {
    "objectID": "blogs/tinyruntime.html#next-steps",
    "href": "blogs/tinyruntime.html#next-steps",
    "title": "tinyRuntime review",
    "section": "",
    "text": "Moving forward, our focus will be on:\n\nContinued Optimization: We will work on enhancing performance and expanding support for different hardware and ML architectures, ensuring tinyRuntime remains competitive and efficient.\nExploring Alternatives: We are considering exploring other runtimes like IREE, which already supports a variety of hardware. This could provide insights and solutions that may benefit tinyRuntime’s development."
  },
  {
    "objectID": "blogs/tinyruntime.html#conclusion",
    "href": "blogs/tinyruntime.html#conclusion",
    "title": "tinyRuntime review",
    "section": "",
    "text": "tinyRuntime is paving the way for efficient ML model inference in resource-constrained environments. With its fast performance and lightweight design, it can be an option for developers and organizations looking to deploy machine learning solutions effectively. We invite you to follow our journey as we continue to enhance tinyRuntime and explore new possibilities in the world of machine learning."
  },
  {
    "objectID": "blogs/aicompressionsaas.html",
    "href": "blogs/aicompressionsaas.html",
    "title": "AI compression SaaS",
    "section": "",
    "text": "It has been nearly five years since I first wrote about TinyML and TinyML as-a-Service (TinyMLaaS) on the Ericsson blog.\n\nIn the rapidly evolving landscape of artificial intelligence (AI), efficiency and resource optimization are paramount. AI Compression as-a-Service (ACaaS) and TinyML as-a-Service (TinyMLaaS) emerge as transformative solutions that address the growing demand for deploying AI on resource-constrained devices. These services offer scalable, cost-effective, and high-performance AI capabilities, enabling a new wave of innovation in edge computing.\n\n\nAI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance.\n\n\n\n\nResource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security.\n\n\n\n\nTinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications.\n\n\n\n\n\n\nSmart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring.\n\n\n\n\nAI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "href": "blogs/aicompressionsaas.html#the-need-for-ai-compression",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI models, especially deep learning networks, have become increasingly complex and resource-intensive. Traditional deployment of these models on edge devices, such as smartphones, IoT sensors, and embedded systems, poses significant challenges due to limited computational power, memory, and energy resources. AI compression techniques, including model pruning, quantization, and knowledge distillation, aim to reduce the size and computational requirements of AI models without significantly compromising their performance."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "href": "blogs/aicompressionsaas.html#benefits-of-ai-compression-as-a-service",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Resource Optimization: ACaaS allows businesses to deploy AI models on edge devices with constrained resources, ensuring efficient utilization of hardware capabilities.\nScalability: By leveraging cloud-based AI compression services, organizations can scale their AI deployments seamlessly, catering to diverse applications and devices.\nCost-Effectiveness: Reduced model sizes and lower computational demands translate to cost savings in terms of hardware investment and energy consumption.\nFaster Inference: Compressed models enable faster inference times, critical for real-time applications such as autonomous vehicles, surveillance, and industrial automation.\nEnhanced Security: Processing data locally on edge devices minimizes data transfer to the cloud, enhancing data privacy and security."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "href": "blogs/aicompressionsaas.html#tinyml-as-a-service-empowering-the-internet-of-things",
    "title": "AI compression SaaS",
    "section": "",
    "text": "TinyMLaaS extends the concept of AI compression to the domain of microcontrollers and other ultra-low-power devices. By providing pre-trained, compressed AI models and tools for deploying them on TinyML platforms, this service empowers developers to create intelligent applications for the Internet of Things (IoT).\n\n\n\nPre-trained Models: Access to a library of pre-trained, optimized models for various applications, from anomaly detection to image recognition.\nDeployment Tools: Comprehensive toolchains for converting, optimizing, and deploying models on TinyML hardware, simplifying the development process.\nCustom Solutions: Tailored AI solutions for specific use cases, ensuring optimal performance and efficiency.\nSeamless Integration: Easy integration with existing IoT ecosystems, enabling rapid deployment and scaling of intelligent applications."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "href": "blogs/aicompressionsaas.html#applications-and-use-cases",
    "title": "AI compression SaaS",
    "section": "",
    "text": "Smart Home Devices: Enhancing the capabilities of home automation systems with intelligent voice assistants, security cameras, and energy management solutions.\nIndustrial IoT: Enabling predictive maintenance, quality control, and automation in manufacturing and logistics.\nHealthcare: Providing real-time monitoring and diagnostics through wearable devices and smart medical equipment.\nAgriculture: Facilitating precision farming with AI-powered sensors for soil health, weather conditions, and crop monitoring."
  },
  {
    "objectID": "blogs/aicompressionsaas.html#conclusion",
    "href": "blogs/aicompressionsaas.html#conclusion",
    "title": "AI compression SaaS",
    "section": "",
    "text": "AI Compression as-a-Service and TinyML as-a-Service represent the next frontier in AI deployment, bridging the gap between advanced AI capabilities and resource-constrained edge devices. By offering scalable, efficient, and cost-effective solutions, these services empower a wide range of industries to harness the power of AI, driving innovation and transforming the way we interact with technology.\nAs the demand for edge computing continues to grow, ACaaS and TinyMLaaS will play a crucial role in shaping the future of AI, making intelligent applications more accessible and ubiquitous than ever before."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html",
    "href": "blogs/Knowledge_Distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "In the world of AI and deep learning, bigger models usually mean better performance. These models, often referred to as deep neural networks (DNNs), have taken artificial intelligence to new heights by achieving remarkable success in computer vision, natural language processing (NLP), and other domains. However, their sheer size and complexity pose significant challenges when it comes to deploying them on devices with limited resources, such as mobile phones and embedded systems. Enter knowledge distillation, an efficient solution to compress large models into smaller, faster, and more resource-friendly versions without compromising accuracy.\n\n\n\nAt its core, knowledge distillation (KD) is a technique used to transfer the knowledge from a large model (referred to as the teacher model) into a smaller model (the student model). The primary objective is to reduce the size of the deep neural network while maintaining a similar level of performance. This method has become increasingly popular in the AI community, as it offers a way to bring complex models to devices with limited computational power. The concept of KD was popularized by Hinton et al. in 2015. The idea is that instead of training a small model from scratch, it can be trained to mimic the outputs of a large model, allowing the student model to learn more efficiently and effectively. By doing so, the student model not only compresses the size of the neural network but also retains much of the functionality of the larger teacher model.\n\n\n\n\nThe process of knowledge distillation generally involves three components:\n\nThe Teacher Model: This is the pre-trained, large neural network that has high accuracy but is computationally expensive.\nThe Student Model: This smaller model aims to replicate the performance of the teacher model while being more efficient in terms of memory and speed.\nDistillation Process: The student model learns by mimicking the teacher’s outputs. The goal is to make the student model as accurate as possible, often by using a special loss function that helps it learn from the teacher’s “soft predictions” (or probability outputs) rather than the hard, ground-truth labels.\n\nIn a typical KD scenario, the teacher model is first trained on a dataset. Then, during the distillation phase, the student model is trained not only using the true labels of the dataset but also by trying to match the teacher’s outputs. These outputs often include subtle patterns and relationships in the data that the teacher has learned, which help guide the student model towards better generalization and accuracy.\n\n\n\nThe knowledge that the student model learns from the teacher can come in various forms. These include:\n\nResponse-Based Knowledge: This refers to the outputs or predictions made by the teacher model. The student model learns by mimicking the probability distribution of the teacher’s predictions, often referred to as “soft labels.”\nFeature-Based Knowledge: Instead of just learning from the final outputs, the student model can also learn from the intermediate representations, or feature maps, generated by the teacher. This helps the student model capture more fine-grained information about the data.\nRelation-Based Knowledge: In some cases, the relationships between different samples or features are used to guide the student model. This could involve learning the similarities and differences between pairs of data points or understanding how various features interact.\n\nEach of these methods provides unique benefits, and depending on the use case, one might be more suitable than the others.\n\n\n\n\n\nThere are several strategies to perform knowledge distillation, each offering different advantages:\n\nOffline Distillation: This is the most common method, where the teacher model is pre-trained and remains fixed during the distillation process. The student model learns from this static teacher, making the process simple and easy to implement.\nOnline Distillation: In this scheme, both the teacher and the student models are trained simultaneously. This is useful when a high-performance teacher model is not available from the start, allowing the two models to evolve together during training.\nSelf-Distillation: In this variant, the teacher and the student are essentially the same model. Different parts of the model (for example, deeper layers) act as the teacher to guide earlier layers (the student). This strategy has been shown to improve performance even within a single network.\n\n\n\n\n\nKnowledge distillation (KD) has revolutionized the process of model compression. This process enables lightweight models to perform complex tasks while retaining most of the teacher’s capabilities. Over time, numerous specialized distillation algorithms have been developed, each designed to address specific challenges or make distillation more effective. A number of prominent KD algorithms are explored in this section, along with their unique approaches and applications.\n\n\nAdversarial KD combines the principles of Generative Adversarial Networks (GANs) with distillation to refine how student models mimic teachers. Here, a generator-discriminator dynamic is introduced where the student model acts as a generator trying to fool a discriminator into thinking its outputs are from the teacher.\n\n\n\nSynthetic Data Generation: A GAN generates synthetic data that aids the student in understanding complex data distributions. This data either supplements or entirely replaces the training dataset.\nDiscriminative Supervision: The student model is trained against a discriminator that distinguishes between teacher and student outputs, ensuring the student closely mimics the teacher.\n\nThis method is particularly effective in improving KD performance in scenarios where labeled data is sparse, as the adversarial mechanism helps the student model learn complex data patterns efficiently .\n\n\n\n\n\nIn Multi-Teacher KD, a student model learns from multiple teachers rather than a single one, each offering distinct knowledge or expertise. This approach is useful when the teachers are trained on different datasets or specialize in different aspects of a task.\n\n\n\nAveraged Response: The simplest implementation involves averaging the outputs from multiple teachers, allowing the student to benefit from diverse viewpoints.\nFeature and Logit Transfer: Some implementations utilize both logits and intermediate feature maps from various teachers to provide the student with a comprehensive understanding of the task.\n\nThis method is beneficial in situations like object detection, where combining multiple expert models can improve the student’s performance across diverse categories.\n\n\n\n\n\nCross-Modal KD enables knowledge transfer between models trained on different modalities (e.g., vision, text, or audio). For example, a teacher model trained on RGB images can transfer its knowledge to a student model working with depth images.\n\n\n\nPaired Modality Learning: Often, paired data (e.g., RGB and depth images) is used, where knowledge learned by the teacher in one modality guides the student’s learning in another modality.\nHallucination Streams: In some cases, a hallucination stream is generated for a missing modality, such as generating depth features from RGB inputs to guide the student.\n\nThis technique is ideal for scenarios where data from one modality is scarce, such as human pose estimation using radio frequency signals paired with RGB video inputs.\n\n\n\n\n\nGraph-Based KD models knowledge as a graph, where vertices represent data instances or features, and edges capture relationships between them. This allows the student model to learn not only from individual outputs but also from the relationships between data points.\n\n\n\nIntra-Data Relations: The student learns from graphs that represent relationships between data points, such as the similarity between features or mutual relations.\nGraph Construction: Graphs can be built from logits, feature maps, or even a combination of the two, allowing for a rich transfer of relational knowledge.\n\nThis method excels in applications requiring an understanding of complex data structures, such as social network analysis or recommender systems, where the relationships between data points are as important as the points themselves.\n\n\n\n\n\nAttention-Based KD leverages attention mechanisms within the teacher model to highlight important features, which are then transferred to the student. Attention maps, which indicate which parts of the input are most important, guide the student in focusing on critical aspects of the data.\n\n\n\nAttention Maps: The student model is trained to replicate the attention maps of the teacher, learning to focus on the same areas of the input data.\nConfidence Assignment: Some methods also use attention mechanisms to assign different confidence levels to various parts of the data, ensuring the student model prioritizes the right features.\n\nThis approach is especially useful in image classification and object detection, where attention maps can guide the student in focusing on specific regions of an image.\n\n\n\n\n\nIn Data-Free KD, the student model is trained without access to the original training data. This is often necessary in situations where the data is sensitive, such as in healthcare or legal domains.\n\n\n\nSynthetic Data Generation: Techniques like GANs or layer activations are used to generate synthetic data that mimics the original dataset, which is then used to train the student model.\nUnlabeled Knowledge Transfer: In some cases, the student learns by replicating the teacher’s outputs without any real data.\n\nThis method is crucial when data privacy is a concern, such as in medical AI applications, where access to patient data may be restricted.\n\n\n\n\n\nQuantized KD deals with converting teacher models into low-precision student models (such as 8-bit or 4-bit representations) without losing significant accuracy. This helps in deploying models on resource-constrained devices.\n\n\n\nPrecision Reduction: The weights and activations of the student model are reduced in precision, making it more efficient for inference on edge devices.\nLoss Minimization: Techniques are employed to minimize the loss in performance due to quantization.\n\nThis method is widely used in deploying AI models on mobile devices and embedded systems, where computational and memory resources are limited.\n\n\n\n\n\nLifelong KD aims to continually update the student model as new tasks or data are introduced, allowing it to learn from the teacher while preserving its knowledge of previous tasks. This is a crucial aspect of lifelong learning in AI.\n\n\n\nTask Preservation: The student model is designed to retain knowledge from previous tasks while learning new ones, reducing the risk of catastrophic forgetting.\nContinuous Learning: New knowledge is distilled incrementally, making the student adaptive to evolving data.\n\nLifelong KD is important in applications like autonomous driving, where the model needs to adapt to new environments without forgetting previously learned knowledge.\n\n\n\n\n\nNeural Architecture Search (NAS)-Based KD integrates NAS into the distillation process, automating the search for the optimal student architecture under the guidance of the teacher model. This approach aims to find the best possible student model structure through data-driven optimization.\n\n\n\nAutomated Architecture Search: NAS methods search for an efficient student model architecture that best matches the teacher’s performance.\nReinforcement Learning: Some NAS-based KD methods use reinforcement learning to dynamically search for and prune unnecessary layers in the student model .\n\nThis method is highly useful in optimizing model performance across a variety of applications, including speech recognition and natural language processing.\n\n\n\n\n\n\nKnowledge distillation has found applications across various domains of artificial intelligence:\n\nVisual Recognition: Distilled models are widely used in tasks such as image classification, object detection, and human pose estimation. Smaller models are crucial for deployment on devices with limited computational power, such as smartphones or embedded cameras.\nNatural Language Processing (NLP): The success of models like BERT and GPT has revolutionized NLP, but their massive size makes them impractical for real-time applications. Distilled versions of these models have been created to retain their impressive performance while being small enough for deployment in resource-constrained environments.\nSpeech Recognition: Similar to other AI tasks, large models can achieve great accuracy in speech recognition, but distilled models enable their use in real-time applications like virtual assistants, where speed and memory are critical.\n\n\n\n\n\nWhile knowledge distillation is a promising approach to compressing deep learning models, it still faces several challenges:\n\nModel Capacity Gap: If the student model is too small compared to the teacher, it may struggle to capture all the necessary knowledge, leading to a performance drop. Researchers are exploring ways to bridge this gap, such as introducing intermediate “assistant” models to facilitate the transfer of knowledge.\nAdversarial Distillation: Incorporating adversarial learning into distillation, where a student model learns to fool a discriminator into thinking its outputs come from the teacher, is an emerging area that could improve the effectiveness of KD in complex tasks.\nCross-Modal Distillation: Transferring knowledge between different modalities (e.g., from images to text or sound) is another exciting frontier that could open up new applications, especially in multi-modal AI systems.\n\n\n\n\n\nThe evolving landscape of knowledge distillation has given rise to a diverse set of algorithms, each designed to tackle specific challenges. From adversarial techniques and multi-teacher models to cross-modal transfers and lifelong learning, these algorithms make it possible to compress complex deep learning models without compromising on accuracy. As AI continues to move towards more resource-efficient solutions, these distillation methods will be crucial in enabling powerful models to run on everyday devices.\nKnowledge distillation is a powerful tool for reducing the size of deep neural networks while maintaining their accuracy. As AI continues to advance, this technique will play a crucial role in making sophisticated models accessible on everyday devices. With ongoing research addressing current challenges, knowledge distillation is poised to become a foundational technique in the future of AI and machine learning."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#what-is-knowledge-distillation",
    "href": "blogs/Knowledge_Distillation.html#what-is-knowledge-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "At its core, knowledge distillation (KD) is a technique used to transfer the knowledge from a large model (referred to as the teacher model) into a smaller model (the student model). The primary objective is to reduce the size of the deep neural network while maintaining a similar level of performance. This method has become increasingly popular in the AI community, as it offers a way to bring complex models to devices with limited computational power. The concept of KD was popularized by Hinton et al. in 2015. The idea is that instead of training a small model from scratch, it can be trained to mimic the outputs of a large model, allowing the student model to learn more efficiently and effectively. By doing so, the student model not only compresses the size of the neural network but also retains much of the functionality of the larger teacher model."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#how-does-knowledge-distillation-work",
    "href": "blogs/Knowledge_Distillation.html#how-does-knowledge-distillation-work",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "The process of knowledge distillation generally involves three components:\n\nThe Teacher Model: This is the pre-trained, large neural network that has high accuracy but is computationally expensive.\nThe Student Model: This smaller model aims to replicate the performance of the teacher model while being more efficient in terms of memory and speed.\nDistillation Process: The student model learns by mimicking the teacher’s outputs. The goal is to make the student model as accurate as possible, often by using a special loss function that helps it learn from the teacher’s “soft predictions” (or probability outputs) rather than the hard, ground-truth labels.\n\nIn a typical KD scenario, the teacher model is first trained on a dataset. Then, during the distillation phase, the student model is trained not only using the true labels of the dataset but also by trying to match the teacher’s outputs. These outputs often include subtle patterns and relationships in the data that the teacher has learned, which help guide the student model towards better generalization and accuracy.\n\n\n\nThe knowledge that the student model learns from the teacher can come in various forms. These include:\n\nResponse-Based Knowledge: This refers to the outputs or predictions made by the teacher model. The student model learns by mimicking the probability distribution of the teacher’s predictions, often referred to as “soft labels.”\nFeature-Based Knowledge: Instead of just learning from the final outputs, the student model can also learn from the intermediate representations, or feature maps, generated by the teacher. This helps the student model capture more fine-grained information about the data.\nRelation-Based Knowledge: In some cases, the relationships between different samples or features are used to guide the student model. This could involve learning the similarities and differences between pairs of data points or understanding how various features interact.\n\nEach of these methods provides unique benefits, and depending on the use case, one might be more suitable than the others."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#distillation-schemes-offline-online-and-self-distillation",
    "href": "blogs/Knowledge_Distillation.html#distillation-schemes-offline-online-and-self-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "There are several strategies to perform knowledge distillation, each offering different advantages:\n\nOffline Distillation: This is the most common method, where the teacher model is pre-trained and remains fixed during the distillation process. The student model learns from this static teacher, making the process simple and easy to implement.\nOnline Distillation: In this scheme, both the teacher and the student models are trained simultaneously. This is useful when a high-performance teacher model is not available from the start, allowing the two models to evolve together during training.\nSelf-Distillation: In this variant, the teacher and the student are essentially the same model. Different parts of the model (for example, deeper layers) act as the teacher to guide earlier layers (the student). This strategy has been shown to improve performance even within a single network."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#knowledge-distillation-algorithms",
    "href": "blogs/Knowledge_Distillation.html#knowledge-distillation-algorithms",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge distillation (KD) has revolutionized the process of model compression. This process enables lightweight models to perform complex tasks while retaining most of the teacher’s capabilities. Over time, numerous specialized distillation algorithms have been developed, each designed to address specific challenges or make distillation more effective. A number of prominent KD algorithms are explored in this section, along with their unique approaches and applications.\n\n\nAdversarial KD combines the principles of Generative Adversarial Networks (GANs) with distillation to refine how student models mimic teachers. Here, a generator-discriminator dynamic is introduced where the student model acts as a generator trying to fool a discriminator into thinking its outputs are from the teacher.\n\n\n\nSynthetic Data Generation: A GAN generates synthetic data that aids the student in understanding complex data distributions. This data either supplements or entirely replaces the training dataset.\nDiscriminative Supervision: The student model is trained against a discriminator that distinguishes between teacher and student outputs, ensuring the student closely mimics the teacher.\n\nThis method is particularly effective in improving KD performance in scenarios where labeled data is sparse, as the adversarial mechanism helps the student model learn complex data patterns efficiently .\n\n\n\n\n\nIn Multi-Teacher KD, a student model learns from multiple teachers rather than a single one, each offering distinct knowledge or expertise. This approach is useful when the teachers are trained on different datasets or specialize in different aspects of a task.\n\n\n\nAveraged Response: The simplest implementation involves averaging the outputs from multiple teachers, allowing the student to benefit from diverse viewpoints.\nFeature and Logit Transfer: Some implementations utilize both logits and intermediate feature maps from various teachers to provide the student with a comprehensive understanding of the task.\n\nThis method is beneficial in situations like object detection, where combining multiple expert models can improve the student’s performance across diverse categories.\n\n\n\n\n\nCross-Modal KD enables knowledge transfer between models trained on different modalities (e.g., vision, text, or audio). For example, a teacher model trained on RGB images can transfer its knowledge to a student model working with depth images.\n\n\n\nPaired Modality Learning: Often, paired data (e.g., RGB and depth images) is used, where knowledge learned by the teacher in one modality guides the student’s learning in another modality.\nHallucination Streams: In some cases, a hallucination stream is generated for a missing modality, such as generating depth features from RGB inputs to guide the student.\n\nThis technique is ideal for scenarios where data from one modality is scarce, such as human pose estimation using radio frequency signals paired with RGB video inputs.\n\n\n\n\n\nGraph-Based KD models knowledge as a graph, where vertices represent data instances or features, and edges capture relationships between them. This allows the student model to learn not only from individual outputs but also from the relationships between data points.\n\n\n\nIntra-Data Relations: The student learns from graphs that represent relationships between data points, such as the similarity between features or mutual relations.\nGraph Construction: Graphs can be built from logits, feature maps, or even a combination of the two, allowing for a rich transfer of relational knowledge.\n\nThis method excels in applications requiring an understanding of complex data structures, such as social network analysis or recommender systems, where the relationships between data points are as important as the points themselves.\n\n\n\n\n\nAttention-Based KD leverages attention mechanisms within the teacher model to highlight important features, which are then transferred to the student. Attention maps, which indicate which parts of the input are most important, guide the student in focusing on critical aspects of the data.\n\n\n\nAttention Maps: The student model is trained to replicate the attention maps of the teacher, learning to focus on the same areas of the input data.\nConfidence Assignment: Some methods also use attention mechanisms to assign different confidence levels to various parts of the data, ensuring the student model prioritizes the right features.\n\nThis approach is especially useful in image classification and object detection, where attention maps can guide the student in focusing on specific regions of an image.\n\n\n\n\n\nIn Data-Free KD, the student model is trained without access to the original training data. This is often necessary in situations where the data is sensitive, such as in healthcare or legal domains.\n\n\n\nSynthetic Data Generation: Techniques like GANs or layer activations are used to generate synthetic data that mimics the original dataset, which is then used to train the student model.\nUnlabeled Knowledge Transfer: In some cases, the student learns by replicating the teacher’s outputs without any real data.\n\nThis method is crucial when data privacy is a concern, such as in medical AI applications, where access to patient data may be restricted.\n\n\n\n\n\nQuantized KD deals with converting teacher models into low-precision student models (such as 8-bit or 4-bit representations) without losing significant accuracy. This helps in deploying models on resource-constrained devices.\n\n\n\nPrecision Reduction: The weights and activations of the student model are reduced in precision, making it more efficient for inference on edge devices.\nLoss Minimization: Techniques are employed to minimize the loss in performance due to quantization.\n\nThis method is widely used in deploying AI models on mobile devices and embedded systems, where computational and memory resources are limited.\n\n\n\n\n\nLifelong KD aims to continually update the student model as new tasks or data are introduced, allowing it to learn from the teacher while preserving its knowledge of previous tasks. This is a crucial aspect of lifelong learning in AI.\n\n\n\nTask Preservation: The student model is designed to retain knowledge from previous tasks while learning new ones, reducing the risk of catastrophic forgetting.\nContinuous Learning: New knowledge is distilled incrementally, making the student adaptive to evolving data.\n\nLifelong KD is important in applications like autonomous driving, where the model needs to adapt to new environments without forgetting previously learned knowledge.\n\n\n\n\n\nNeural Architecture Search (NAS)-Based KD integrates NAS into the distillation process, automating the search for the optimal student architecture under the guidance of the teacher model. This approach aims to find the best possible student model structure through data-driven optimization.\n\n\n\nAutomated Architecture Search: NAS methods search for an efficient student model architecture that best matches the teacher’s performance.\nReinforcement Learning: Some NAS-based KD methods use reinforcement learning to dynamically search for and prune unnecessary layers in the student model .\n\nThis method is highly useful in optimizing model performance across a variety of applications, including speech recognition and natural language processing."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#applications-of-knowledge-distillation",
    "href": "blogs/Knowledge_Distillation.html#applications-of-knowledge-distillation",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge distillation has found applications across various domains of artificial intelligence:\n\nVisual Recognition: Distilled models are widely used in tasks such as image classification, object detection, and human pose estimation. Smaller models are crucial for deployment on devices with limited computational power, such as smartphones or embedded cameras.\nNatural Language Processing (NLP): The success of models like BERT and GPT has revolutionized NLP, but their massive size makes them impractical for real-time applications. Distilled versions of these models have been created to retain their impressive performance while being small enough for deployment in resource-constrained environments.\nSpeech Recognition: Similar to other AI tasks, large models can achieve great accuracy in speech recognition, but distilled models enable their use in real-time applications like virtual assistants, where speed and memory are critical."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#challenges-and-future-directions",
    "href": "blogs/Knowledge_Distillation.html#challenges-and-future-directions",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "While knowledge distillation is a promising approach to compressing deep learning models, it still faces several challenges:\n\nModel Capacity Gap: If the student model is too small compared to the teacher, it may struggle to capture all the necessary knowledge, leading to a performance drop. Researchers are exploring ways to bridge this gap, such as introducing intermediate “assistant” models to facilitate the transfer of knowledge.\nAdversarial Distillation: Incorporating adversarial learning into distillation, where a student model learns to fool a discriminator into thinking its outputs come from the teacher, is an emerging area that could improve the effectiveness of KD in complex tasks.\nCross-Modal Distillation: Transferring knowledge between different modalities (e.g., from images to text or sound) is another exciting frontier that could open up new applications, especially in multi-modal AI systems."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#conclusion",
    "href": "blogs/Knowledge_Distillation.html#conclusion",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "The evolving landscape of knowledge distillation has given rise to a diverse set of algorithms, each designed to tackle specific challenges. From adversarial techniques and multi-teacher models to cross-modal transfers and lifelong learning, these algorithms make it possible to compress complex deep learning models without compromising on accuracy. As AI continues to move towards more resource-efficient solutions, these distillation methods will be crucial in enabling powerful models to run on everyday devices.\nKnowledge distillation is a powerful tool for reducing the size of deep neural networks while maintaining their accuracy. As AI continues to advance, this technique will play a crucial role in making sophisticated models accessible on everyday devices. With ongoing research addressing current challenges, knowledge distillation is poised to become a foundational technique in the future of AI and machine learning."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#the-need-for-faster-ai",
    "href": "blogs/Knowledge_Distillation.html#the-need-for-faster-ai",
    "title": "Knowledge Distillation",
    "section": "The Need for Faster AI",
    "text": "The Need for Faster AI\nAI models, especially deep learning models, are known for their ability to handle large-scale data and solve complex tasks. However, this power comes at a cost—many AI models are large, computationally intensive, and require significant memory and energy. As AI becomes increasingly integrated into real-time applications like autonomous driving, healthcare, mobile applications, and edge computing, deploying massive models on devices with limited resources becomes a significant challenge.\nFor example: - Mobile applications need AI models that can run quickly and efficiently on smartphones with limited memory and processing power. - Edge computing devices, like IoT sensors and cameras, require lightweight AI models that deliver real-time results without relying on powerful cloud servers. - Autonomous vehicles depend on fast, real-time decision-making AI models that must run efficiently without draining the battery or overloading the hardware.\nTo meet these growing demands, FasterAI offers a comprehensive solution that makes AI models more accessible, efficient, and ready for deployment across various platforms."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#how-fasterai-works",
    "href": "blogs/Knowledge_Distillation.html#how-fasterai-works",
    "title": "Knowledge Distillation",
    "section": "How FasterAI Works",
    "text": "How FasterAI Works\nFasterAI streamlines the development and deployment of AI models using several advanced techniques. Here’s how it works:\n\n1. Model Compression\nModel compression is a critical technique for reducing the size of deep learning models while retaining their accuracy. FasterAI uses compression techniques such as pruning, quantization, and weight sharing to shrink the model’s size and make it faster.\n\nPruning: This process removes unnecessary weights and neurons from a neural network, reducing the model’s complexity without affecting performance.\nQuantization: By converting high-precision models (e.g., 32-bit floating point) into lower precision (e.g., 8-bit or 4-bit models), FasterAI drastically reduces the memory footprint while keeping the model’s accuracy intact.\nWeight Sharing: FasterAI identifies and merges similar weights in the model, allowing it to run more efficiently.\n\nWith these techniques, FasterAI reduces both the storage and computational costs of deploying large models on smaller devices.\n\n\n2. Knowledge Distillation\nKnowledge distillation is a process where a large, complex AI model (the teacher) transfers its knowledge to a smaller model (the student). The student model is trained to mimic the behavior of the teacher, delivering similar performance while being much smaller and faster.\nFasterAI uses knowledge distillation to: - Shrink models: Student models are much smaller in size compared to teacher models but retain nearly the same level of performance. - Improve efficiency: Distilled models are optimized for faster inference, making them ideal for real-time applications. - Ensure accuracy: Despite the reduction in size, FasterAI ensures that the smaller models maintain high accuracy, allowing them to be used in mission-critical applications.\n\n\n3. Neural Architecture Search (NAS)\nNeural Architecture Search (NAS) is a technique that automates the process of designing AI models. Instead of manually choosing the architecture of a neural network, FasterAI uses NAS to explore different architectures and find the most efficient model for a specific task.\n\nAutomated Search: NAS automatically searches for the optimal model structure, balancing speed and accuracy.\nCustomized Models: FasterAI tailors models to specific hardware requirements, ensuring that AI applications can run smoothly across different platforms—whether it’s a cloud server or an embedded device.\nTask-Specific Optimization: With NAS, FasterAI generates models that are highly optimized for particular tasks, ensuring maximum performance and minimal resource usage.\n\nThis approach enables FasterAI to create models that are both high-performing and efficient, with minimal human intervention."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#why-choose-fasterai",
    "href": "blogs/Knowledge_Distillation.html#why-choose-fasterai",
    "title": "Knowledge Distillation",
    "section": "Why Choose FasterAI?",
    "text": "Why Choose FasterAI?\nFasterAI is not just another AI framework—it’s a comprehensive solution designed to tackle the growing demand for faster, more efficient AI models. Here’s why FasterAI stands out:\n\n1. Real-Time AI\nFasterAI is built to support real-time applications that require immediate processing, such as autonomous vehicles, facial recognition, and augmented reality. By optimizing models for speed, FasterAI ensures low-latency performance, allowing AI models to run in real time without lag.\n\n\n2. Cross-Platform Deployment\nWith FasterAI, AI models can be easily deployed across various platforms—from cloud servers and desktops to mobile devices and edge hardware. Whether you’re working on a mobile app or an IoT project, FasterAI ensures that your models run smoothly, regardless of the hardware limitations.\n\n\n3. Scalability\nAs businesses scale, so do their AI needs. FasterAI provides a scalable solution by making it easy to deploy models on a wide range of devices. From small-scale projects to large enterprise applications, FasterAI enables seamless scaling without the need for complex retraining or re-engineering.\n\n\n4. Energy Efficiency\nIn scenarios where power consumption is a major concern (e.g., edge computing, battery-operated devices), FasterAI optimizes models to use less energy while maintaining high performance. This is especially important for applications like wearable technology or smart home devices.\n\n\n5. Enhanced User Experience\nFasterAI enables AI models to deliver results faster, enhancing user experiences in applications that rely on AI for real-time feedback. Whether it’s a voice assistant responding instantly or an app that processes images in a fraction of a second, FasterAI ensures your AI enhances, not hinders, user experience."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#applications-of-fasterai",
    "href": "blogs/Knowledge_Distillation.html#applications-of-fasterai",
    "title": "Knowledge Distillation",
    "section": "Applications of FasterAI",
    "text": "Applications of FasterAI\nFasterAI can be applied across various industries, making AI faster and more accessible in real-world scenarios:\n\nHealthcare: FasterAI compresses complex diagnostic models, enabling them to run on portable devices in hospitals, improving patient care without needing bulky hardware.\nAutonomous Driving: In autonomous vehicles, FasterAI ensures that AI models can make quick, real-time decisions while conserving battery life and reducing hardware demands.\nRetail: FasterAI powers AI-driven recommendation systems and customer service bots in real-time, enhancing the shopping experience for customers and driving more sales.\nSmart Devices: From smartwatches to smart speakers, FasterAI makes AI possible on devices with limited computing power, allowing real-time AI-driven features like voice recognition and activity tracking.\nFinance: In banking and financial services, FasterAI optimizes AI models used for fraud detection and algorithmic trading, ensuring they run efficiently with minimal delay."
  },
  {
    "objectID": "blogs/Knowledge_Distillation.html#conclusion-1",
    "href": "blogs/Knowledge_Distillation.html#conclusion-1",
    "title": "Knowledge Distillation",
    "section": "Conclusion",
    "text": "Conclusion\nIn an era where AI is becoming indispensable in our daily lives, FasterAI is the solution that bridges the gap between cutting-edge AI capabilities and the practical need for speed, efficiency, and scalability. By leveraging advanced techniques like model compression, knowledge distillation, and neural architecture search, FasterAI delivers fast, efficient AI models that are easy to deploy across a wide range of devices.\nWhether you’re developing mobile apps, working on IoT solutions, or deploying large-scale AI systems, FasterAI offers the tools to make AI work smarter, faster, and more efficiently—paving the way for the future of AI-powered innovation."
  },
  {
    "objectID": "blogs/about.html",
    "href": "blogs/about.html",
    "title": "About",
    "section": "",
    "text": "What’s NinjaLABO is and how it was started!\n\n\n\nNinjaLABO’s origin",
    "crumbs": [
      "Get Started",
      "About"
    ]
  },
  {
    "objectID": "blogs/todo_2024_autumn.html",
    "href": "blogs/todo_2024_autumn.html",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "This document provides an overview of NinjaLABO’s tasks and objectives for the autumn of 2024. The following sections outline key areas of focus, ranging from way of working (WoW) to model development and hardware deployment strategies.\n\n\n\nToDo 2024 Autumn Overview\n\n\n\n\nOur Way of Working (WoW) aims to ensure efficient and collaborative work among team members. This includes SCRUM practices, GitHub workflows, and workshops for knowledge sharing.\n\n\nSCRUM is our core agile framework, promoting daily check-ins, iterative sprint cycles, and continuous improvement.\n\n\n\n\n\nHeld on Discord in the Voice Channel.\nA strict 15-minute standup where everyone declares the following:\n\nWhat they completed (Done).\nWhat they will work on next (ToDo).\nAny blockers or issues (Issue).\n\n\n\n\n\nIf a topic takes longer than 5 minutes, continue the discussion in backlog item comments or schedule another meeting.\n\n\n\n\n\n\nEach sprint follows a clear structure, from planning to review, ensuring that we meet our project goals while continuously improving.\n\n\n\nBacklog grooming and sprint planning usually take place on Monday morning at the beginning of the sprint.\nThe team selects high-priority tasks from the backlog.\n\n\n\n\n\nThe review occurs on Friday afternoon at the end of the sprint.\nWe assess completed backlog items and discuss progress with stakeholders.\n\n\n\n\n\nAfter the review, we hold a retrospective to discuss what worked well and what could be improved in our WoW.\nThe retro typically happens right after the review on Friday afternoon.\n\n\n\n\n\n\n\nGitHub is our primary tool for project management, code reviews, and CI/CD.\n\n\n\n\n\nWe use the KANBAN board to visualize and track our backlog items, ensuring we stay on top of tasks and priorities.\n\n\n\n\n\n\n\n\nWe follow the Acceptance Test-Driven Development (ATDD) approach, which ensures that tests are written based on the system’s behavior before the implementation. \n\n\n\n\n\n\nUsing Nbdev, we streamline writing, testing, documenting, and distributing software packages directly from Jupyter Notebooks.\n\n\n\n\n\nTo facilitate knowledge sharing, we conduct workshops, encouraging developers to present and follow along using Jupyter notebooks.\n\n\n\n\nThe following techniques will be our focus for optimizing model performance:\n\n\nIn-depth profiling is necessary to guide our compression strategies.\n\n\n\nReducing the number of bits needed to represent weights and activations with Post-training Quantization.\n\n\n\n\n\nPruning involves removing less critical parameters from the model.\n\n\n\n\n\nTraining a smaller model (student) to mimic a larger model (teacher) through Knowledge Distillation.\n\n\n\n\n\nReducing model complexity by approximating weight matrices with Low-Rank Factorization.\n\n\n\n\n\n\nExploring hardware platforms and their capabilities.\n\n\n\nInvestigate the potential of using IREE for machine learning workloads.\n\n\n\n\n\nContinue optimizing and leveraging CUDA for GPU-accelerated tasks.\n\n\n\n\n\n\n\nBenchmark and compare model performance with NVIDIA’s TensorRT on Jetson devices.\n\n\n\n\n\n\n\n\nRunning Large Language Models (LLM) on the Orange Pi 5 Plus, evaluating its performance and scalability.\n\n\n\n\n\n\nOur tinyMLaaS focuses on providing scalable machine learning services, particularly for resource-constrained devices.\n\n\n\nA pipeline for flexible model transformations, such as compression, dockerization, and packaging models for various environments.\n\n\n\n\nLeveraging FastHTML for quicker model transformations and deployment.\n\n\n\n\n\n\nDistributed model execution across nodes, utilizing technologies like FaaS (Function as a Service) and Docker Swarm.\n\n\n\n\n\nOur custom runtime for executing deep learning models on various hardware platforms.\n\n\n\nFocus on improving GPU acceleration within our runtime.\n\n\n\n\n\nCPU-based inference and training optimizations.\n\n\n\n\n\n\nEvaluating inference performance with the ResNet50 model.\n\n\n\n\nUsing Convolutional Neural Networks (CNN) to process image data.\n\n\n\n\n\n\nBenchmarking with the Imagenette dataset.\n\n\n\n\n\nSupporting inference of Transformers, including Large Language Models (LLMs) and Vision Transformers.\n\n\n\n\n\n\nOptimizing training pipelines on both CPU and CUDA-based platforms.\n\n\n\n\n\n\nModel development and collaboration projects for 2024.\n\n\n\n\n\nDeveloping models for hyperspectral imaging using 3D Convolutional Neural Networks, particularly for CubeSat applications.\n\n\n\n\n\n\n\n\nFocusing on compression techniques for LLMs, making them more efficient for deployment.\n\n\n\n\n\nExploring physics simulation models as part of the ESA tender."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#wow",
    "href": "blogs/todo_2024_autumn.html#wow",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our Way of Working (WoW) aims to ensure efficient and collaborative work among team members. This includes SCRUM practices, GitHub workflows, and workshops for knowledge sharing.\n\n\nSCRUM is our core agile framework, promoting daily check-ins, iterative sprint cycles, and continuous improvement.\n\n\n\n\n\nHeld on Discord in the Voice Channel.\nA strict 15-minute standup where everyone declares the following:\n\nWhat they completed (Done).\nWhat they will work on next (ToDo).\nAny blockers or issues (Issue).\n\n\n\n\n\nIf a topic takes longer than 5 minutes, continue the discussion in backlog item comments or schedule another meeting.\n\n\n\n\n\n\nEach sprint follows a clear structure, from planning to review, ensuring that we meet our project goals while continuously improving.\n\n\n\nBacklog grooming and sprint planning usually take place on Monday morning at the beginning of the sprint.\nThe team selects high-priority tasks from the backlog.\n\n\n\n\n\nThe review occurs on Friday afternoon at the end of the sprint.\nWe assess completed backlog items and discuss progress with stakeholders.\n\n\n\n\n\nAfter the review, we hold a retrospective to discuss what worked well and what could be improved in our WoW.\nThe retro typically happens right after the review on Friday afternoon.\n\n\n\n\n\n\n\nGitHub is our primary tool for project management, code reviews, and CI/CD.\n\n\n\n\n\nWe use the KANBAN board to visualize and track our backlog items, ensuring we stay on top of tasks and priorities.\n\n\n\n\n\n\n\n\nWe follow the Acceptance Test-Driven Development (ATDD) approach, which ensures that tests are written based on the system’s behavior before the implementation. \n\n\n\n\n\n\nUsing Nbdev, we streamline writing, testing, documenting, and distributing software packages directly from Jupyter Notebooks.\n\n\n\n\n\nTo facilitate knowledge sharing, we conduct workshops, encouraging developers to present and follow along using Jupyter notebooks."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#compression",
    "href": "blogs/todo_2024_autumn.html#compression",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "The following techniques will be our focus for optimizing model performance:\n\n\nIn-depth profiling is necessary to guide our compression strategies.\n\n\n\nReducing the number of bits needed to represent weights and activations with Post-training Quantization.\n\n\n\n\n\nPruning involves removing less critical parameters from the model.\n\n\n\n\n\nTraining a smaller model (student) to mimic a larger model (teacher) through Knowledge Distillation.\n\n\n\n\n\nReducing model complexity by approximating weight matrices with Low-Rank Factorization."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#hw",
    "href": "blogs/todo_2024_autumn.html#hw",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Exploring hardware platforms and their capabilities.\n\n\n\nInvestigate the potential of using IREE for machine learning workloads.\n\n\n\n\n\nContinue optimizing and leveraging CUDA for GPU-accelerated tasks.\n\n\n\n\n\n\n\nBenchmark and compare model performance with NVIDIA’s TensorRT on Jetson devices.\n\n\n\n\n\n\n\n\nRunning Large Language Models (LLM) on the Orange Pi 5 Plus, evaluating its performance and scalability."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#tinymlaas",
    "href": "blogs/todo_2024_autumn.html#tinymlaas",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our tinyMLaaS focuses on providing scalable machine learning services, particularly for resource-constrained devices.\n\n\n\nA pipeline for flexible model transformations, such as compression, dockerization, and packaging models for various environments.\n\n\n\n\nLeveraging FastHTML for quicker model transformations and deployment.\n\n\n\n\n\n\nDistributed model execution across nodes, utilizing technologies like FaaS (Function as a Service) and Docker Swarm."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#tinyruntime",
    "href": "blogs/todo_2024_autumn.html#tinyruntime",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Our custom runtime for executing deep learning models on various hardware platforms.\n\n\n\nFocus on improving GPU acceleration within our runtime.\n\n\n\n\n\nCPU-based inference and training optimizations.\n\n\n\n\n\n\nEvaluating inference performance with the ResNet50 model.\n\n\n\n\nUsing Convolutional Neural Networks (CNN) to process image data.\n\n\n\n\n\n\nBenchmarking with the Imagenette dataset.\n\n\n\n\n\nSupporting inference of Transformers, including Large Language Models (LLMs) and Vision Transformers.\n\n\n\n\n\n\nOptimizing training pipelines on both CPU and CUDA-based platforms."
  },
  {
    "objectID": "blogs/todo_2024_autumn.html#model",
    "href": "blogs/todo_2024_autumn.html#model",
    "title": "ToDo list in Autumn 2024",
    "section": "",
    "text": "Model development and collaboration projects for 2024.\n\n\n\n\n\nDeveloping models for hyperspectral imaging using 3D Convolutional Neural Networks, particularly for CubeSat applications.\n\n\n\n\n\n\n\n\nFocusing on compression techniques for LLMs, making them more efficient for deployment.\n\n\n\n\n\nExploring physics simulation models as part of the ESA tender."
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "AI Compression as-a-Service MVP6 review",
    "crumbs": [
      "Get Started"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html",
    "href": "docs/tinymlaas/TinyML-MCU_README.html",
    "title": "TinyML-MCU",
    "section": "",
    "text": "Code for TinyMLaaS MCU devices.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#dependencies",
    "title": "TinyML-MCU",
    "section": "Dependencies",
    "text": "Dependencies\nThe bridge has a dependency on usbutils. On Debian-based systems, it can be installed with\napt install usbutils",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#running",
    "title": "TinyML-MCU",
    "section": "Running",
    "text": "Running\n\nLocally\nInstall the python dependecies\npip install -r requirements.txt\nThere are two different ways to run the bridge locally.\nFirst, to run a development server, run\nflask --app main.py --debug run\nFor production, this is not ideal. To use waitress as production server\nwaitress-serve main:app\n\n\nDocker\nThe docker version of the bridge requires nestybox/sysbox.\nThe provided docker-compose file looks like this\nversion: '3'\n\nservices:\n  bridge:\n    build:\n      context: .\n    # If you have devices connected to the relay, add them here\n    # devices:\n      # - \"/dev/ttyACM0:/dev/ttyACM0\"\n    volumes:\n      - \"/dev/bus/usb:/dev/bus/usb\"\n      - \"/dev/serial:/dev/serial\"\n    runtime: sysbox-runc\n    ports:\n      - 5000:8080\nAny microcontrollers you want to control with the bridge need to be added to the devices at this point. They also need to be connected to the computer.\nIf you want to add more microcontrollers to the bridge later, you can add them manually by adding them to the docker-compose file and restarting the container.\nThere is also a provided script to check for added Arduinos. The script docker-container-restarted automatically detects, when new arduinos are added and restart the container with the new devices. It can be run by making it executable and running it\nchmod u+x docker-container-restarter.py\n./docker-container-restarter.py\nThere might be problems with serial port permissions. As a solution, give permissions to the port for all users with\nchmod 777 /path/to/port\nNote that this will most likely require root priviledges.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#serving-the-application-to-internet",
    "title": "TinyML-MCU",
    "section": "Serving the application to internet",
    "text": "Serving the application to internet\n\nWebserver\nIf you already have a webserver setup, such as Apache2 or Nginx, with which you can serve the application to the internet, add a new proxy for the bridge according to the style of the webserver.\n\n\nPort forwarding\nAnother way is to add a new port forwarding setting to the host machine to port 5000 in the router.\n\n\nNgrok\nHowever, these might not be usable by everyone. With Ngrok, you can serve the bridge to the internet without having access to the local router.\n\nInstall ngrok\nServe the application with\n\nngrok http 5000",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "href": "docs/tinymlaas/TinyML-MCU_README.html#microcontrollers",
    "title": "TinyML-MCU",
    "section": "Microcontrollers",
    "text": "Microcontrollers\nThere are seperate instructions for microcontrollers.\nArduino",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyML-MCU"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html",
    "href": "docs/tinymlaas/Architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "This page contains general information about the architechture and how each component is related to each other.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#overview",
    "href": "docs/tinymlaas/Architecture.html#overview",
    "title": "Architecture",
    "section": "Overview",
    "text": "Overview\nTinyMLaaS consist of several components: - backend - streamlit frontend - cli - MCU components\nThe different components are stored in their own repositories which can be found here.\nThe backend is the core component which contains all the API endpoints. By calling them you can execute all the tasks necessary for the workflow. The backend is responsible with communicating with the machine learing components, storing data in the database and installing & managing MCU devices.\nTensorflow machine learning components live in the main repository and need to be fetched for the backend seperately.\nMCU repository contains the bridge for communicating with the devices and the code needed for devices.\nWe have implemented two different interfaces for the TinyMLaaS: CLI and website GUI using streamlit. Since you can make API calls directly to backend it’s extremely simple to build your own frontends in the future.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#block-diagram",
    "href": "docs/tinymlaas/Architecture.html#block-diagram",
    "title": "Architecture",
    "section": "Block Diagram",
    "text": "Block Diagram\n\n\n\nBlock Diagram\n\n\nThe backend is the main component that deals with calling the tensorflow functions and communicating with the MCU devices. Tensorflow is currently the supported UI but you can also make API calls directly or use the CLI. In the future the tensorflow components can be containarized as their own service.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#key-components",
    "href": "docs/tinymlaas/Architecture.html#key-components",
    "title": "Architecture",
    "section": "Key Components",
    "text": "Key Components\n\nML model training\nData Storage and loading (database)\nML model quantization and optimization\nML model compilation for MCUs",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Architecture.html#database-diagram",
    "href": "docs/tinymlaas/Architecture.html#database-diagram",
    "title": "Architecture",
    "section": "Database diagram",
    "text": "Database diagram\n\n\n\nDatabase Diagram",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Architecture"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html",
    "href": "docs/tinymlaas/Background.html",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "href": "docs/tinymlaas/Background.html#federated-learning-in-tinyml",
    "title": "Background information and some literature sources",
    "section": "",
    "text": "TinyML combines ML, mathematical optimization and tiny IoT embedded systems. TinyML is an effective method to analyze real-world data continuously, without the resource overhead of traditional ML hardware. TinyFedTL is the first open-sources implementation of federated learning (FL) in IoT containing microcontroller unit (MCU) and small CPU based devices. TinyFedTL uses transfer learning (TL) to demonstrate effective privacy-centric FL on devices with a small memory footprint (less than 1 MB). Researchers and engineers may open up data in various fields to gain insights for improving life quality or user experience without sacrificing privacy.\nWhile recent progress in ML frameworks has made it possible to perform inference with models using cheap, tiny devices, the training of the models is typically done separately on powerful computers. This provides the training process abundant CPU and memory resources to process large stored datasets. However, it is possible to train the machine learning model directly on the microcontroller and extend the training process with federated learning. On-device training has been illustrated with keyword spotting task. Experiments were conducted with real devices to characterize the learning behaviour and resource consumption for various hyperparameters and federated learning configurations. The observation was, that when training locally with fewer data, more frequent federated learning rounds reduced the training loss faster, but with the cost of higher bandwidth usage and longer training time. The results indicated that depending of the application, the trade-off between the requirements and the resource usage of the system needs to be determined.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#edge-impulse",
    "href": "docs/tinymlaas/Background.html#edge-impulse",
    "title": "Background information and some literature sources",
    "section": "Edge Impulse",
    "text": "Edge Impulse\nEdge Impulse is the leading development platform for ML on edge devices. It is free for developers. With Edge Impulse one can: - build advanced embedded ML apps - build custom datasets rapidly - collect sensor, audio or camera data from devices, files or cloud integrations - use automatic labeling tools such as object detection and audio segmentation - use Edge Impulse cloud infrastructure to set up and run reusable scripted operations that transform the input data on large sets of data in parallel - integrate deployment pipelines, CI/CD tools and custom data sources with open APIs - develop models and algortihms - with prebuilt DSP (Digital Signal Processors) and ML blocks - hardware decisions can be made on device performance and Flash/RAM on every step - DSP feature extraction algorithms can be customized - custom machine learning models with Keras APIs - use visualized insights on datasets, model performance and memory to fine-tune the production model - optimize models and algorithms - EON TUNER for finding balance between DSP configurations and model architecture, budgeted against memory and latency constraints - EON Compiler for lighter and faster neural networks with equal accuracy - have full visibility across the whole ML pipeline - complete access to data attributes, DSP algorithms, model hyperparameters throughout whole development lifecycle - test model performance accurately - virtual cloud hardware simulation framework to get performance and accuracy metrics before deploying on any physical device - model performance can be evaluated with live classification - devices, automated ML pipeline testing, integration with the testing framework - deploy easily on any edge target - optimize source code by generating optimized embedded libraries and applications for any edge device - build ready-to-go binaries with selected development boards supported by Edge Impulse with special firmware - without OS or hardware dependencies and compile to nearly anything - make digital twin by re-deploying cloud-hosted ML projects to any hardware target on the fly - benefit from access and integrations to the leading hardware partner ecosystem from MCUs to MPUs and GPUs including acceleration - Arduino - Himax - OpenMV - Nvidia - Nordic semiconductor - Raspberry Pi - Silicon Labs - Sony - ST - Syntiant - Texas Instruments",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "href": "docs/tinymlaas/Background.html#arduino-and-wiring",
    "title": "Background information and some literature sources",
    "section": "Arduino and Wiring",
    "text": "Arduino and Wiring\nArduino is an open-source electronics platform intended for anyone to make interactive projects. It is based on easy-to-use hardware and software. Arduino boards can - read inputs: light on a sensor, finger on a button, twitter message - turn inputs into outputs: activate motor, turn on a LED, publish something online - be instructed what to do by sending a set of instructions to the microcontroller on the board - using Arduino programming language (based on Wiring) and the Arduino Software (IDE) (based on Processing, an open source integrated development environment (IDE) like the Arduino IDE)\nBecause of simple and accessible user experience Arduino has been used in thousands of projects and applications by a worldwide community of makers, students, hobbyists, artists, programmers and professionals to produce a vast amount of accessible knowledge. The product range includes products for IoT applications, wearable, 3D printing and embedded environments.\nThe Arduino software - is easy-to-use for beginners but flexible enough for advanced users - runs on Mac, Windows and Linux - is used by teachers and students to build low cost scientific instruments to prove chemistry and physics principles and to get started with programming and robotics - is used by designers and architects to build interactive prototypes - is used by musicians and artists to build installations ansd to experiment with new musical instruments - is used by makers to build projects - can be used by anyone by following detailed instructions of a kit or sharing ideas online\nArduino offers some pros over other microcontrollers and microcontroller platforms available: - inexpensive compared to other microcontrolle platforms; the least expensive version of the Arduino module can be assembled by hand, pre-assembled Arduino modules are also affordable - cross-platform, the Arduino software (IDE) runs on Windows, Macintosh OSX, Linux (most microcontroller systems are limited to Windows) - simple, clear programming environment - easy-to-use for beginners but flexible enough for advanced users - because it’s based on Processing programming environment, students learning to program in that environment will be familiar with how the Arduino IDE works - open source and extensible software, published as open source tools - the language is based on AVR-C programming and can be expanded through C++ libraries - AVR-C code can be added directly into the Arduino programs - open source and extensible hardware, with the plans of the Arduino boards published under a Creative Commons licence, so own versions of the module with extensions and improvements can be built by anyone\nCode can be developed in the Arduino Cloud to build smart IoT projects. - smart devices can be connected within minutes - wide range of compatible devices, the Arduino Cloud provides the necessary code - nice dashbords can be created with mix and match customizable widgets to visualize real time or historical data or to control the device - projects can be controlled from anywhere in the world from any device, for example Alexa. - projects and libraries are always synced and up to date - all projects are cloud based and accessible from any device - data is always ecrypted and always belongs to the user (you) - is open and customizable, has flexible APIs to integrate and customize Cloud - all connected Arduino boards have a built-in crypto chip that makes them incredibly secure - sketches and project data are stored in AES 256-bit encrypted datastores - account security is protected with single use authentication codes - open and transparent data privacy terms and your data always belongs to you - has a wide range of resources - tutorials - APIs - documentation\nArduino’s history is interesting. The Arduino project was based on the developing platform Wiring created by Hernando Barragán as a Master’s thesis project at the Interaction Design Institute Ivrea (IDII) in Ivrea, Italy in 2003.\nHernando has been developing Wiring ever since and now Wiring is an open source electronics prototyping platform composed of programming language, an integrated development environment (IDE) , and a single-board microcontroller. More about Wiring can be found here.\nWiring offers new boards to customers in their webshop, but Wiring supports other boards directly, so Wiring IDE can be used on other boards as well (and more will be added). Thus Arduino board can be used with the Wiring IDE. Wiring can be downloaded for Linux, MacOS X and Windows.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#mbed",
    "href": "docs/tinymlaas/Background.html#mbed",
    "title": "Background information and some literature sources",
    "section": "Mbed",
    "text": "Mbed\nMbed is an open-source platform and operating system for internet-connected devices that are based on 32-bit ARM Cortex-M microcontrollers. These devices are known as IoT devices. Mbed is collaboratively developed by Arm and its tech partners.\nMbed is free and open source IoT operating system with connectivity, security, storage, device management and ML. Mbed offers free development tools, thousands of code examples and support for hundreds of microcontrollers development boards, as described here. - Mbed has its own Mbed OS with - well-defined API to develop C++ applications - free tools and thousands of code examples, libraries and drivers for common components - built-in security stack - core components such as storage and connectivity options - Mbed Enabled hardware has many options - Compiler and IDE - Keil Studio Cloud - modern, zero-installation Mbed development environment in the browser - code high-lighting, WebUSB flash and debug and version control - Mbed Studio - an IDE for application and library development - single environment with everything to create, compile and debug Mbed programs - Mbed CLI - command line interface allows to integrate Mbed functionality into preferred editor or enhance automation setup - Security - Arm Mbed TLS provides comprehensive SSL/TLS solution - easy to include cryptographic and SSL/TLS capabilities in the software and embedded products - as an SSL library Arm Mbed TLS provides an intuitive API, readable source code and a minimal and highly configurable code footprint",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#ifttt",
    "href": "docs/tinymlaas/Background.html#ifttt",
    "title": "Background information and some literature sources",
    "section": "IFTTT",
    "text": "IFTTT\nIFTTT is a private company that runs online digital automation platforms and offers them as a service. IFTTT is short for If This Then That. IFTTT integrates apps, devices and services quick and easy. IFTTT makes tech incompatibility easy to tackle. Automating process is simple, the user chooses - trigger - action(s) - name for the applet and finish\nIFTTT has over 700 services ready (more added weekly) to be automated. Price range of the services is from 0€/forever to 5€/month. IFTTT provides a simple way to create for example a smart home: - make a user account and log in (can be done with Google or Facebook) - trigger: give Google Assistant a voice command “Hey Google, I need coffee” - action(s): coffee machine is turned on and when the coffee is ready, the coffee machine turns off - name the applet: “make coffee” and finish the applet",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sony-mesh",
    "href": "docs/tinymlaas/Background.html#sony-mesh",
    "title": "Background information and some literature sources",
    "section": "Sony MESH",
    "text": "Sony MESH\nSony MESH is a Sony corporate startup that sells a range of colored blocks with different sensors and wireless connection to the IoT. It’s digital DIY platform to connect everyday objects into IoT and create your own projects. - MESH blocks are wireless - Visual Coding App called Canvas simplifies programming and wiring with drag-and-drop functions - project can be connected to web services and popular smart gadgets like WeMo and Google Assistant voice activation - hardware projects can be expanded without expertise - IoT block in the project allows additions of smart features such as motion-sensitivity, remote control, orientation monitoring, voice commands, notifications, text messaging and more - projects can be connected to the internet instantly - project can be transformed into an IoT device, such as - Twitter alarm system - a voice-activated, data-logging, remote-controlled car - allows customization of smart gadgets - MESH is compatible with over 350 smart gadgets, home automation devices and web services on IFTTT - each IoT block has built-in IFTTT integration, so that it’s simple to add custom features on a smart gadget - MESH Motion and MESH Temperature & Humidity used together allow addition of motion-activated, multi-room temperature monitoring to a smart device like Nest thermostat - allows to build own smart gadget - MESH GPIO is a simple interface for development boards like Arduino and Raspberry Pi or actuators like a DC motor - MESH GPIO integrates any smart devices or web services on IFTTT, incl. - Amazon Alexa for Echo - Google Assistant - Google Sheets - LIFX - Nest - Phillips Hue - Twitter - WeMo - over 350 more - MESH blocks use Bluetooth - MESH blocks are rechargeable, durable and compact",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Background.html#sources",
    "href": "docs/tinymlaas/Background.html#sources",
    "title": "Background information and some literature sources",
    "section": "Sources:",
    "text": "Sources:\nOn-Device Training of Machine Learning Models on Microcontrollers with Federated Learning\nTinyFedTL: Federated Transfer Learning on Ubiquitous Tiny IoT Devices\nEdge Impulse\nBeginner’s Guide to DSP\nKeras APIs\nArduino\nArduino Getting started guide\nArduino Tutorials on Arduino Project Hub\nMbed\nMbed (product)\nIFTTT\nMESH blocks\nWiring\nWiring webshop",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Background information and some literature sources"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html",
    "href": "docs/tinymlaas/Next_steps.html",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "href": "docs/tinymlaas/Next_steps.html#work-in-progress-isolate-tensorflow-from-backend",
    "title": "Suggestions for further development",
    "section": "",
    "text": "The TinyMLaaS and TinyML-backend repositories both contain a branch called dockerize_tensorflow. These branches contain unfinished development for a feature where the backend does not contain any machine learning libraries but only works as a relay for training and compiling machine learning models. The idea is, that the backend spins up a docker container that contains the machine learning library (currently Tensorflow) and uses it to train an compile models. The model files are then extracted from this Tensorflow container to the filesystem of the container where the backend is running.\nCompiling a tensorflow model for MCU works if there is a model in the tensorflow_models folder and database. Training a new model does not yet work. Some of the backend code is commented out and this feature is very much work in progress.\nThe feature works with Docker in Docker (dind) using the nestybox/sysbox runtime environment which allows a container to run docker without privileged mode / giving access to local docker sockets. This is a bit complicated, but more secure than the alternatives. The same dind principle is also used for running the relay/bridge that is used to install and observe the MCU devices (see https://github.com/TinyMLaas/TinyML-MCU).\nThe dockerize_tensorflow branch contains a Dockerfile that sets up the Tensorflow docker image. In order to develop this feature, you need to work on both repositories TinyMLaaS and TinyML-backend simultaneously. The docker-compile in TinyMLaaS dockerize_tensorflow branch uses backend_no_tf.Dockerfile to pull the dockerize_tensorflow branch from the TinyML-backend repo.\nOur FastAPI backend communicates with the docker image and containers using Docker API and more specifically the Docker SDK for Python. The feature is implemented in the tf_docker/compile.py backend module.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#database",
    "href": "docs/tinymlaas/Next_steps.html#database",
    "title": "Suggestions for further development",
    "section": "Database",
    "text": "Database\n\nProduction ready database\nThe backend uses SQLite. However, as SQlite is so lightweight, there are drawbacks, that affect the usage of the software. First of all, SQLite is not meant for storing big files. Because of this, all datasets, models and compiled models are stored outside the database in directories, and the database contains the path to the files. This is not ideal, as the backend can get messy with all the directories and if permanent storage is required outside the docker container, all of these volumes need to be mounted to the docker container.\nA SQLite database is also a single file, meaning that accidentally deleting the database or misplacing it is more common.\nBecause of these drawbacks, we would suggest changing the used database from SQLite to something more robust and production ready, such as MariaDB or PostgreSQL. Larger files can be stored in these databases and it is easy to mount one database rather than multiple different locations for all different saving locations.\n\n\nMove saving of models and datasets to database\nAs mentioned in the previous section, all datasets and models are stored outside the database. With the change of the database, it would be better to save all these files in the database.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#datasets",
    "href": "docs/tinymlaas/Next_steps.html#datasets",
    "title": "Suggestions for further development",
    "section": "Datasets",
    "text": "Datasets\n\nDownloading/uploading datasets (as zip)\nCurrently, you can only upload and append to datasets. It’s not possible to view or download it anyway. The application should have way to download the entire dataset to your machine (propably as zip so it works on both Windows and Linux).\nAt the moment you can only send bulk of pictures of to the app to create a dataset. It’s propably good idea to allow sending zips and creating datasets from them. You propably need to make sure that - it’s a valid dataset with working images - the saved dataset keeps same folder structure as the send zip - the user doesn’t send malicious data\n\n\nEditing, deleting and managing datasets\nCurrently, only adding and appending datasets is supported. The user should propably be able to manage the datasets better. First you should be able to remove unwanted datasets. Second, the user should be able to make folder structure for the datasets and edit them for labeling purposes see this tutorial for example how the data is structured. Being able to simply download the datasets,editing it locally, removing it from the app and readding it allows for the user to edit it. After that, if you want to enhance the user experience one idea is to add better editing options to the app itself: Being able remove photos, being able to add photos, creating new folders, removing folders.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#cli",
    "href": "docs/tinymlaas/Next_steps.html#cli",
    "title": "Suggestions for further development",
    "section": "CLI",
    "text": "CLI\nThe CLI is mostly autogenerated with this OpenAPI Generator. The current generator works as far as the existing API endpoints stay the same regarding input and output. Whenever new HTTP methods are added or modified a new generation has to be done. This can be done by using the json-file provided by the FastAPI backend. See detailed instructions here.\n\nAdd missing functionality + fixes to existing\nOnly some of the functionality is available with the CLI. See examples from the already implemented functionality from services and tiny_mlaas.py. To help with implementing more functionality, see examples for using the client from the docs. For example for bridges here.\nAdding and listing existing devices isn’t working properly because newly added devices don’t have a designated bridge. It’s worth considering if adding a device should be possible without a bridge. This fix would be implemented in backend. Alternative fix is to modify the validation done by the generated client.\nTraining via the CLI trains a model, but the output contains a picture that can’t be displayed in CLI. A possible fix is to modify the training function so that displaying an example of a prediction is optional.\nMany of the functions have hardcoded parameters. This helps with development and testing, but should be fixed in the future. See example of adding parameters: https://github.com/TinyMLaas/TinyML-CLI/blob/main/services/models.py . Here dataset_id and description are required parameters. Epoch and the rest of the hard coded variables can be handled in a similar fashion.\n\n\nAutogenerate end-to-end CLI from OpenAPI YAML\nThe current version of the CLI is autogenerated with the exception of services and tiny_mlaas.py.\nEnd-to-end autogeneration could be done after publishing services and tiny-mlaas.py as a Python package and having it as a dependency. See instructions: https://typer.tiangolo.com/tutorial/package/.\nA preliminary idea for implementing the end-to-end generation is following:\n\nPublish a package that contains the forementioned files\nOpenAPI yaml-file is needed for generating the CLI tool. Get it by browsing to backend_url/openapi.json. Convert the json to yaml with for example: https://editor.swagger.io/\n\nThe repository for autogeneration would consist of the yaml -file from step 2 and requirements.txt -file with the package published in step 1 (in addition to current requirements). With these prerequisites the steps for end-to-end generation could be:\n\nClone repo\nInstall the generator tool with: npm install @openapitools/openapi-generator-cli -g\nGenerate the client with npx @openapitools/openapi-generator-cli generate -i file.yaml -g python -o output_path\n\nfile.yaml is the yaml-file in the repo\noutput path should probably be the root directory of the cloned CLI-repository\n\nInstall the requirements: pip install -r requirements.txt\nFinish the installation with: python3 setup.py install\n\nFor usage instructions see: https://github.com/TinyMLaas/TinyML-CLI#usage\nMisc: if requirements.txt is overwritten by the generator use a different name, interface_requirements.txt etc (remember to install these in step 4). Using the CLI package might differ from using it locally, see Typer documentation: https://typer.tiangolo.com/tutorial/package/",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Next_steps.html#bridge",
    "href": "docs/tinymlaas/Next_steps.html#bridge",
    "title": "Suggestions for further development",
    "section": "Bridge",
    "text": "Bridge\n\nSupport for Raspberry PI\nInstallation and observation for Arduino nano 33 BLE has been imported fully to this version of the version. However, the Raspberry PI support has not been tested at all and most likely will not work out of the box. Adding support/making sure installation to Raspberries work is a required feature, that unfortunately does not exist yet.\n\n\nBridge port\nA bridge can be saved to the backend as an IP address and as an URL. There is right now no validation to make sure that the given IP address or URL is a valid address. Also, if IP address is used, it automatically asumes that the bridge is hosted on port 5000. This should be changed so that the bridge can be hosted on any port.\n\n\nError handling\nThere is little to none error handling on the bridge. This means that, even if operations fail, it will still say the operation was successfull. The next ones are known errors that do not have error handling:\n\nWhen the compiled arduino sketch is uploaded successfully, but it can not be started for some reason, most likely because there isn’t enough memory for model on the device.\nObservation doesn’t have permission to the device.\n\nThe installation process has a chance to fail when running inside a docker container and the reason is unknown.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Suggestions for further development"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html",
    "href": "docs/tinymlaas/Demonstration.html",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "",
    "text": "This document will demonstrate the steps in TinyMLaaS app. Some pages have dependencies on other pages, so go through pages from top to bottom is recommended at the start.\nIn order to run TinyMLaaS end-to-end following components need to be run: - The Frontend - The Backend - The Relay\nThese can all easily be started with the help of docker using the docker-compose-with-bridge.yml file in the Main repository. The application can be started with",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#device",
    "href": "docs/tinymlaas/Demonstration.html#device",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Device",
    "text": "Device\nIn order to install a model to a embedded device, the briging device of the wanted device and the device need to be selected.\nThe first thing that should be done on the Device page is to either add a new bridge or selecting an existing bridge. Let’s add the bridge that was started by docker compose. To do that, add the name of the bridge docker container with the port 8080 as the bridges address. The bridge will also not use a HTTPS connection in this case.\n\n\n\nAdd new bridge\n\n\nAfter adding the bridge, select the wanted bridge by clicking the Select bridge button next to the wanted bridge\n\n\n\nSelect the bridge\n\n\nSelecting a device to which to install the trained machine learning model later on is required. If the wanted device has not been registered already, register it either manually or by selecting it from the list of devices connected to the bridge. Lets add a device connected to the bridge by pressing the Register this device button next to that device\n\n\n\nRegister the new device\n\n\nAdd the missing information on the form and click add\n\n\n\nDevice form\n\n\nThe added device will automatically be selected as the active device.\n\n\n\nSelected device",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#data",
    "href": "docs/tinymlaas/Demonstration.html#data",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Data",
    "text": "Data\nIn order to train a model, a dataset with which to train the model needs to be selected.\n\n\n\nDataset selection complete\n\n\nUser can add images from local storage to selected dataset.\nIf the existing datasets are not enough, a new dataset can be added to the software.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#model",
    "href": "docs/tinymlaas/Demonstration.html#model",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Model",
    "text": "Model\nThis page shows already trained models as well as allows training of a new model.\nLet’s train a new model. For this, we first need to decide the parameters with which to train the model with. This time we chose to train the model with 27 epochs and with a batch size of 56. The image size is 96x96, as this model is trained for an Arduino, which takes pictures of this size.\n\n\n\nTrain a new model\n\n\nAfter the training is done, the software will show an image of the statistics of the training process as well as a test image with a prediction that the newly trained model gave for that picture.\n\n\n\nAfter training",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#compiling",
    "href": "docs/tinymlaas/Demonstration.html#compiling",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Compiling",
    "text": "Compiling\nThe page is responsible for ML compilation. It will turn the selected ML model and turn it into a tflite model as well as generate a C-array of it. The C array is the tflite model turned into bytes stored in a C array, which is required for embedded devices, which do not have a filesystem.\nAfter the compiling is done, the newly compiled model will be selected as the active model.\n\n\n\nCompilation done",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#installing",
    "href": "docs/tinymlaas/Demonstration.html#installing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Installing",
    "text": "Installing\nNow that a model has been compiled, it can be installed on the device that was selected on the Device page. The page shows a single button, install. When this is pressed, the software will install the selected compiled model to the selected device on the selected bridge.\nBe sure that the software has access to the device. If you are not sure, the next command will give all users permissions to read, write and execute to the machine\nchmod 777 /path/to/port\nThis time, the device is connected to /dev/ttyACM0, so it was given permissions.\nNow, install the model to the device.\n\n\n\nInstall successfully complete",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/Demonstration.html#observing",
    "href": "docs/tinymlaas/Demonstration.html#observing",
    "title": "Demonstratation of TinyMLaaS WebApp",
    "section": "Observing",
    "text": "Observing\nOn the observing page, user can see real-time predictions from device when the start button has been activated.\n\n\n\nReal-time predictions as device output",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "Demonstratation of TinyMLaaS WebApp"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html",
    "href": "docs/tinymlaas/TinyMLaaS_README.html",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "This is the main repository of Tiny Machine Learning as a Service project for Software Engineering Project course at University of Helsinki, summer 2023.\n\n\n\nGitHub Actions\n\n\n\n\nThe GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.\n\n\n\n\nBackend\nFrontend\nCLI\nMCU components\n\n\n\n\n\nWay of Working\nProduct backlog\nWorking hours\nDatabase schema\n\n\n\n\nUse Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#overview",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "The GitHub pages describe the overview of the project and the functionality of the machine learning modules: training, compiling, installing and observing.",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#repositories",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Backend\nFrontend\nCLI\nMCU components",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#documentation",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Way of Working\nProduct backlog\nWorking hours\nDatabase schema",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  },
  {
    "objectID": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "href": "docs/tinymlaas/TinyMLaaS_README.html#running-the-project",
    "title": "TinyMLaaS-main",
    "section": "",
    "text": "Use Docker to build and run the whole project.\n\nClone this repository\nRun\n\ndocker compose up -d\nThis will set up both the backend and frontend, and a network between the two\nIf the bridge is also needed on the same machine, use the docker-compose-with-bridge.yml file. This will build and run the frontend, backend and bridge and create a network between all three components.\n\nClone this repository\nRun\n\ndocker compose up -f docker-compose-with-brdige.yml -d\nNote that requires the Sysbox runtime to be installed and running, as the bridge uses this module.\n\n\nSee instructions in respective repositories for frontend, backend and the birdge / relay service for MCUs.\n\nBackend\nFrontend\nMCUs\nCLI",
    "crumbs": [
      "Get Started",
      "Documentation",
      "TinyMLaaS - summer 2023",
      "TinyMLaaS-main"
    ]
  }
]