{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e1a479",
   "metadata": {},
   "source": [
    "# Model Compression \n",
    "## Singular Value Decomposition (SVD) - Resnet50\n",
    "\n",
    "SVD (a mathematical technique) helps us approximate complex weight matrices with simpler ones, to compress the model's convolutional layers.\n",
    "Goal: Reduce the model's size and speed up computations without significantly sacrificing its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f9cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe60a67",
   "metadata": {},
   "source": [
    "## Download and Load the Dataset\n",
    "Using the Imagenette2-320 dataset, which is a smaller version of the ImageNet dataset. It has images that belong to just 10 classes (e.g., different breeds of dogs, cats, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88379c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\imagenette2-320.tgz\n",
      "Extracting ./data\\imagenette2-320.tgz to ./data\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "# Download Imagenette2-320\n",
    "url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\n",
    "root = \"./data\"\n",
    "download_and_extract_archive(url, download_root=root)\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = os.path.join(root, \"imagenette2-320\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5523450",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: Resize, Convert to Tensor, Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalize as per ImageNet pre-training\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d998e9",
   "metadata": {},
   "source": [
    "## Load the Pre-trained ResNet-50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04398a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Move model to device (CPU or GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03b98",
   "metadata": {},
   "source": [
    "## Apply SVD for Model Compression\n",
    "\n",
    "This technique reduces the size of the model by approximating its convolutional layers (layers that detect image features like edges, shapes, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385dddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_compress_conv_layer(conv_layer, rank):\n",
    "    # Get the weight tensor of the convolutional layer\n",
    "    weight = conv_layer.weight.data\n",
    "    out_channels, in_channels, h, w = weight.shape\n",
    "    \n",
    "    # Reshape the weight tensor to a 2D matrix of shape (out_channels, in_channels * h * w)\n",
    "    weight_reshaped = weight.view(out_channels, -1)\n",
    "\n",
    "    # Apply SVD to the weight matrix\n",
    "    U, S, V = torch.svd(weight_reshaped)\n",
    "\n",
    "    # Keep only the top `rank` singular values/vectors\n",
    "    U_reduced = U[:, :rank]\n",
    "    S_reduced = S[:rank]\n",
    "    V_reduced = V[:, :rank]\n",
    "\n",
    "    # Construct the compressed weight matrix\n",
    "    compressed_weight = torch.mm(U_reduced, torch.diag(S_reduced))\n",
    "    compressed_weight = torch.mm(compressed_weight, V_reduced.t())\n",
    "\n",
    "    # Reshape back to the original convolutional weight shape\n",
    "    compressed_weight = compressed_weight.view(out_channels, in_channels, h, w)\n",
    "\n",
    "    # Replace the original weights with the compressed weights\n",
    "    conv_layer.weight.data = compressed_weight\n",
    "\n",
    "    return conv_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78697b",
   "metadata": {},
   "source": [
    "## Apply SVD to Each Convolutional Layer in ResNet-50\n",
    "We apply SVD to every convolutional layer in the model. By keeping only the most important components (e.g., 20 components), we make each layer smaller and therefore reduce the entire model's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49821238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing layer: conv1\n",
      "Compressing layer: layer1.0.conv1\n",
      "Compressing layer: layer1.0.conv2\n",
      "Compressing layer: layer1.0.conv3\n",
      "Compressing layer: layer1.0.downsample.0\n",
      "Compressing layer: layer1.1.conv1\n",
      "Compressing layer: layer1.1.conv2\n",
      "Compressing layer: layer1.1.conv3\n",
      "Compressing layer: layer1.2.conv1\n",
      "Compressing layer: layer1.2.conv2\n",
      "Compressing layer: layer1.2.conv3\n",
      "Compressing layer: layer2.0.conv1\n",
      "Compressing layer: layer2.0.conv2\n",
      "Compressing layer: layer2.0.conv3\n",
      "Compressing layer: layer2.0.downsample.0\n",
      "Compressing layer: layer2.1.conv1\n",
      "Compressing layer: layer2.1.conv2\n",
      "Compressing layer: layer2.1.conv3\n",
      "Compressing layer: layer2.2.conv1\n",
      "Compressing layer: layer2.2.conv2\n",
      "Compressing layer: layer2.2.conv3\n",
      "Compressing layer: layer2.3.conv1\n",
      "Compressing layer: layer2.3.conv2\n",
      "Compressing layer: layer2.3.conv3\n",
      "Compressing layer: layer3.0.conv1\n",
      "Compressing layer: layer3.0.conv2\n",
      "Compressing layer: layer3.0.conv3\n",
      "Compressing layer: layer3.0.downsample.0\n",
      "Compressing layer: layer3.1.conv1\n",
      "Compressing layer: layer3.1.conv2\n",
      "Compressing layer: layer3.1.conv3\n",
      "Compressing layer: layer3.2.conv1\n",
      "Compressing layer: layer3.2.conv2\n",
      "Compressing layer: layer3.2.conv3\n",
      "Compressing layer: layer3.3.conv1\n",
      "Compressing layer: layer3.3.conv2\n",
      "Compressing layer: layer3.3.conv3\n",
      "Compressing layer: layer3.4.conv1\n",
      "Compressing layer: layer3.4.conv2\n",
      "Compressing layer: layer3.4.conv3\n",
      "Compressing layer: layer3.5.conv1\n",
      "Compressing layer: layer3.5.conv2\n",
      "Compressing layer: layer3.5.conv3\n",
      "Compressing layer: layer4.0.conv1\n",
      "Compressing layer: layer4.0.conv2\n",
      "Compressing layer: layer4.0.conv3\n",
      "Compressing layer: layer4.0.downsample.0\n",
      "Compressing layer: layer4.1.conv1\n",
      "Compressing layer: layer4.1.conv2\n",
      "Compressing layer: layer4.1.conv3\n",
      "Compressing layer: layer4.2.conv1\n",
      "Compressing layer: layer4.2.conv2\n",
      "Compressing layer: layer4.2.conv3\n"
     ]
    }
   ],
   "source": [
    "def compress_resnet50(model, rank):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            print(f\"Compressing layer: {name}\")\n",
    "            compressed_layer = svd_compress_conv_layer(module, rank=rank)\n",
    "            setattr(model, name, compressed_layer)\n",
    "\n",
    "# Compress the model with a reduced rank (e.g., keep top 20 components)\n",
    "compress_resnet50(model, rank=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71291f",
   "metadata": {},
   "source": [
    "## Fine-Tune the Compressed Model\n",
    "\n",
    "After compression, the model needs to be retrained slightly to adjust the weights.\n",
    "\n",
    "Compression changes the weights significantly, which may affect the model's accuracy. Fine-tuning helps adjust these weights and bring back the accuracy closer to its original value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0684a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.4015\n",
      "Epoch [2/3], Loss: 0.8525\n",
      "Epoch [3/3], Loss: 0.6684\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e2bea",
   "metadata": {},
   "source": [
    "## Evaluate the Compressed Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2754b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy after Compression: 72.20%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Validation Accuracy after Compression: {100 * correct / total:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
