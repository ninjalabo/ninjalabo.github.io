<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Leila Mozaffari">
<meta name="dcterms.date" content="2024-10-07">
<meta name="description" content="Overview of Low-rank approximation – Part4">

<title>Low-rank approximation 4/4 – NinjaLABO</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-710567d9ce3aafacac9b6493d3599c7a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1KVB48YND6"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-1KVB48YND6', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Low-rank approximation 4/4 – NinjaLABO">
<meta name="twitter:description" content="Overview of Low-rank approximation – Part4">
<meta name="twitter:image" content="https://ninjalabo.github.io/blogs/images/logo.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/logo.jpg" alt="NinjaLABO log." class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../getstarted.html" aria-current="page"> 
<span class="menu-text">Get Started</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../performance.html">
 <span class="dropdown-text">Performance 🚀</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../tutorials/runwalkthrough.html">
 <span class="dropdown-text">Tutorials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../getstarted.html">
 <span class="dropdown-text">Documentations</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-company" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Company</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-company">    
        <li>
    <a class="dropdown-item" href="../blogs/about.html">
 <span class="dropdown-text">About</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../blog.html#category=News">
 <span class="dropdown-text">News🌟</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../pricing.html"> 
<span class="menu-text">Pricing</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blogs🌟</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../getstarted.html"> 
<span class="menu-text"><a href="https://studio.ninjalabo.ai/" style="background-color: rgb(185, 185, 1); padding: 10px 20px; color: white; border: none; border-radius: 0.5rem; text-decoration: None; margin-right: 10px;">Try Free</a></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://twitter.com/ninjalabo" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter"></i></a>
    <a href="https://www.linkedin.com/company/ninjalabo/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/ninjalabo/ninjalabo.github.io">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/ninjalabo/ninjalabo.github.io/issues">
            Report a Bug
            </a>
          </li>
      </ul>
    </div>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Low-rank approximation 4/4</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../blogs/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../getstarted.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Get Started</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/runwalkthrough.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Run Walkthrough</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Documentation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">TinyMLaaS - summer 2023</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Architecture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Background information and some literature sources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Demonstration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Demonstratation of TinyMLaaS WebApp</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Next_steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Suggestions for further development</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Software-Project-Summer-Kick-off.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project starting point</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Technologies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Technological choices</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/TinyML-backend_README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TinyML-backend</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/TinyML-frontend_README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TinyML-frontend</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/TinyML-MCU_arduino_README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Arduino sketch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/TinyML-MCU_README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TinyML-MCU</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/TinyMLaaS_README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TinyMLaaS-main</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../docs/tinymlaas/Use_cases.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Use case scenarios</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../blog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Performance</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#low-rank-approximation-of-attention-mechanisms" id="toc-low-rank-approximation-of-attention-mechanisms" class="nav-link active" data-scroll-target="#low-rank-approximation-of-attention-mechanisms">Low-Rank Approximation of Attention Mechanisms:</a>
  <ul class="collapse">
  <li><a href="#what-are-kernel-methods" id="toc-what-are-kernel-methods" class="nav-link" data-scroll-target="#what-are-kernel-methods">What are Kernel Methods?</a></li>
  <li><a href="#connecting-kernel-methods-and-attention-mechanisms" id="toc-connecting-kernel-methods-and-attention-mechanisms" class="nav-link" data-scroll-target="#connecting-kernel-methods-and-attention-mechanisms">Connecting Kernel Methods and Attention Mechanisms</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#what-is-a-kernel-function" id="toc-what-is-a-kernel-function" class="nav-link" data-scroll-target="#what-is-a-kernel-function">What is a Kernel Function?</a></li>
  <li><a href="#mathematical-definition" id="toc-mathematical-definition" class="nav-link" data-scroll-target="#mathematical-definition">Mathematical Definition</a></li>
  <li><a href="#properties-of-kernel-functions" id="toc-properties-of-kernel-functions" class="nav-link" data-scroll-target="#properties-of-kernel-functions">Properties of Kernel Functions</a></li>
  <li><a href="#what-are-random-features" id="toc-what-are-random-features" class="nav-link" data-scroll-target="#what-are-random-features">What are Random Features?</a></li>
  <li><a href="#why-use-random-features" id="toc-why-use-random-features" class="nav-link" data-scroll-target="#why-use-random-features">Why Use Random Features?</a></li>
  <li><a href="#how-do-random-features-work" id="toc-how-do-random-features-work" class="nav-link" data-scroll-target="#how-do-random-features-work">How Do Random Features Work?</a></li>
  <li><a href="#advantages-of-random-features" id="toc-advantages-of-random-features" class="nav-link" data-scroll-target="#advantages-of-random-features">Advantages of Random Features</a></li>
  <li><a href="#disadvantages-of-random-features" id="toc-disadvantages-of-random-features" class="nav-link" data-scroll-target="#disadvantages-of-random-features">Disadvantages of Random Features</a></li>
  <li><a href="#approximation-of-kernel-methods-using-random-features" id="toc-approximation-of-kernel-methods-using-random-features" class="nav-link" data-scroll-target="#approximation-of-kernel-methods-using-random-features">Approximation of Kernel Methods Using Random Features</a></li>
  <li><a href="#how-does-random-feature-approximation-work" id="toc-how-does-random-feature-approximation-work" class="nav-link" data-scroll-target="#how-does-random-feature-approximation-work">How Does Random Feature Approximation Work?</a></li>
  <li><a href="#advantages-of-using-random-features" id="toc-advantages-of-using-random-features" class="nav-link" data-scroll-target="#advantages-of-using-random-features">Advantages of Using Random Features</a></li>
  <li><a href="#disadvantages" id="toc-disadvantages" class="nav-link" data-scroll-target="#disadvantages">Disadvantages</a></li>
  <li><a href="#approximation-using-random-features" id="toc-approximation-using-random-features" class="nav-link" data-scroll-target="#approximation-using-random-features">Approximation Using Random Features</a></li>
  <li><a href="#advantages-of-random-feature-approximation-of-attention" id="toc-advantages-of-random-feature-approximation-of-attention" class="nav-link" data-scroll-target="#advantages-of-random-feature-approximation-of-attention">Advantages of Random Feature Approximation of Attention</a></li>
  <li><a href="#disadvantages-1" id="toc-disadvantages-1" class="nav-link" data-scroll-target="#disadvantages-1">Disadvantages</a></li>
  <li><a href="#positive-orthogonal-random-features-p-orfs" id="toc-positive-orthogonal-random-features-p-orfs" class="nav-link" data-scroll-target="#positive-orthogonal-random-features-p-orfs">1. Positive Orthogonal Random Features (P-ORFs)</a></li>
  <li><a href="#performers" id="toc-performers" class="nav-link" data-scroll-target="#performers">2. Performers</a></li>
  <li><a href="#advantages-of-performers-and-p-orfs" id="toc-advantages-of-performers-and-p-orfs" class="nav-link" data-scroll-target="#advantages-of-performers-and-p-orfs">Advantages of Performers and P-ORFs</a></li>
  <li><a href="#disadvantages-2" id="toc-disadvantages-2" class="nav-link" data-scroll-target="#disadvantages-2">Disadvantages</a></li>
  </ul></li>
  <li><a href="#nyström-approximation" id="toc-nyström-approximation" class="nav-link" data-scroll-target="#nyström-approximation">Nyström approximation</a>
  <ul class="collapse">
  <li><a href="#kernel-matrix" id="toc-kernel-matrix" class="nav-link" data-scroll-target="#kernel-matrix">1. Kernel Matrix</a></li>
  <li><a href="#nyström-approximation-steps" id="toc-nyström-approximation-steps" class="nav-link" data-scroll-target="#nyström-approximation-steps">2. Nyström Approximation Steps</a></li>
  <li><a href="#advantages-of-the-nyström-approximation" id="toc-advantages-of-the-nyström-approximation" class="nav-link" data-scroll-target="#advantages-of-the-nyström-approximation">3. Advantages of the Nyström Approximation</a></li>
  <li><a href="#disadvantages-of-the-nyström-approximation" id="toc-disadvantages-of-the-nyström-approximation" class="nav-link" data-scroll-target="#disadvantages-of-the-nyström-approximation">4. Disadvantages of the Nyström Approximation</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/ninjalabo/ninjalabo.github.io/edit/main/blogs/Low_Rank_Approximation-Part4.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ninjalabo/ninjalabo.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Low-rank approximation 4/4</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Tech</div>
  </div>
  </div>

<div>
  <div class="description">
    Overview of Low-rank approximation – Part4
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Leila Mozaffari <a href="mailto:leila.mozaffari@ninjalabo.ai" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="low-rank-approximation-of-attention-mechanisms" class="level1">
<h1>Low-Rank Approximation of Attention Mechanisms:</h1>
<section id="what-is-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="what-is-attention-mechanism">What is Attention Mechanism?</h4>
<p>The <strong>attention mechanism</strong> is a concept from machine learning, particularly in natural language processing and computer vision. It allows models (like transformers) to focus on different parts of the input data when making predictions or generating outputs. For example, when translating a sentence, attention helps the model focus on the most relevant words in the source sentence at each step of the translation.</p>
<p>The standard attention mechanism, while powerful, can be computationally expensive. This is because it involves creating a large attention matrix for every input, where the size of this matrix grows with the number of input tokens. For a sequence of length <span class="math inline">\(N\)</span>, the attention matrix is of size <span class="math inline">\(N \times N\)</span>. This can lead to high memory usage and slow processing times, especially with long sequences.</p>
</section>
<section id="key-concepts-of-attention-mechanisms" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts-of-attention-mechanisms">Key Concepts of Attention Mechanisms</h4>
<ol type="1">
<li><p><strong>Focus on Relevant Information</strong>:</p>
<ul>
<li>Imagine reading a book; you might focus on certain sentences or words that are more important to understand the context. Attention mechanisms do the same for models.</li>
<li>For example, when translating a sentence, the model uses attention to focus on the most relevant words from the source sentence while generating each word of the translation.</li>
</ul></li>
<li><p><strong>Calculation</strong>:</p>
<ul>
<li>In simple terms, attention computes a weighted sum of the input features, where the weights determine how much focus to give to each feature.</li>
<li>This is often done using three key components:
<ul>
<li><strong>Query</strong>: Represents what we are currently focusing on.</li>
<li><strong>Key</strong>: Represents the information we have.</li>
<li><strong>Value</strong>: The actual information corresponding to the keys.</li>
</ul></li>
<li>The attention score is computed by taking the dot product of the query with all the keys, followed by applying a softmax function to obtain weights. These weights are then used to compute a weighted sum of the values.</li>
</ul>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(Q\)</span>: Query matrix</li>
<li><span class="math inline">\(K\)</span>: Key matrix</li>
<li><span class="math inline">\(V\)</span>: Value matrix</li>
<li><span class="math inline">\(d_k\)</span>: Dimension of the keys, used for scaling.</li>
</ul></li>
<li><p><strong>Applications</strong>:</p>
<ul>
<li>Attention mechanisms are widely used in models like Transformers, which have revolutionized NLP tasks such as translation, summarization, and question answering.</li>
</ul></li>
</ol>
</section>
<section id="how-low-rank-approximation-works-in-attention-mechanisms" class="level4">
<h4 class="anchored" data-anchor-id="how-low-rank-approximation-works-in-attention-mechanisms">How Low-Rank Approximation Works in Attention Mechanisms</h4>
<ol type="1">
<li><p><strong>Matrix Decomposition</strong>:</p>
<ul>
<li>Instead of computing the full attention matrix, we can decompose it into two smaller matrices. This means we approximate the large matrix with a product of two smaller matrices, which captures most of the information but is much cheaper to compute.</li>
</ul>
<p>For example, given a matrix <span class="math inline">\(A\)</span>, we can approximate it as: <span class="math display">\[
A \approx U \times V
\]</span> where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are smaller matrices.</p></li>
<li><p><strong>Efficiency</strong>:</p>
<ul>
<li>By reducing the size of the matrices, the computations needed to process the data become significantly faster and require less memory.</li>
<li>This allows models to handle longer sequences without running into performance issues.</li>
</ul></li>
<li><p><strong>Preserving Information</strong>:</p>
<ul>
<li>The key idea is to keep the most important features of the attention mechanism. By using low-rank approximation, we focus on the dominant patterns in the data, ensuring that the model still performs well.</li>
</ul></li>
</ol>
</section>
<section id="benefits-of-using-low-rank-approximation-in-attention" class="level4">
<h4 class="anchored" data-anchor-id="benefits-of-using-low-rank-approximation-in-attention">Benefits of Using Low-Rank Approximation in Attention</h4>
<ul>
<li><strong>Speed</strong>: Faster training and inference times, as calculations involve smaller matrices.</li>
<li><strong>Memory Efficiency</strong>: Reduced memory usage, allowing models to work with longer sequences or larger batch sizes.</li>
<li><strong>Maintain Performance</strong>: Often retains good accuracy even with reduced computational costs.</li>
</ul>
<p>Sure! Let’s break down <strong>kernel methods</strong> and <strong>attention mechanisms</strong> in a simple and understandable way.</p>
</section>
<section id="what-are-kernel-methods" class="level3">
<h3 class="anchored" data-anchor-id="what-are-kernel-methods">What are Kernel Methods?</h3>
<p><strong>Kernel methods</strong> are a class of algorithms used in machine learning that can operate in high-dimensional spaces without explicitly transforming the data into those dimensions. They are particularly useful in <strong>support vector machines (SVM)</strong> and other algorithms that rely on measuring the similarity between data points.</p>
<section id="key-concepts-of-kernel-methods" class="level4">
<h4 class="anchored" data-anchor-id="key-concepts-of-kernel-methods">Key Concepts of Kernel Methods</h4>
<ol type="1">
<li><strong>Feature Space</strong>:
<ul>
<li>Many machine learning algorithms work better in high-dimensional spaces. However, directly transforming data into high dimensions can be computationally expensive.</li>
<li>Kernel methods enable us to operate in this high-dimensional space using a trick called the <strong>kernel trick</strong>.</li>
</ul></li>
<li><strong>Kernel Trick</strong>:
<ul>
<li><p>Instead of transforming the input data <span class="math inline">\(X\)</span> into a higher-dimensional feature space explicitly, kernel methods use a <strong>kernel function</strong> <span class="math inline">\(K\)</span> that computes the inner product of the data points in this high-dimensional space.</p></li>
<li><p>For example, a common kernel is the <strong>Gaussian (RBF) kernel</strong>, defined as:</p>
<p><span class="math display">\[
K(x_i, x_j) = \exp\left(-\frac{||x_i - x_j||^2}{2\sigma^2}\right)
\]</span></p></li>
<li><p>This function calculates the similarity between two data points <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> without needing to compute their coordinates in the higher-dimensional space.</p></li>
</ul></li>
<li><strong>Applications</strong>:
<ul>
<li>Kernel methods are used for classification, regression, and clustering tasks. They are particularly effective for problems where the relationship between features is non-linear.</li>
</ul></li>
</ol>
</section>
</section>
<section id="connecting-kernel-methods-and-attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="connecting-kernel-methods-and-attention-mechanisms">Connecting Kernel Methods and Attention Mechanisms</h3>
<p>While kernel methods and attention mechanisms serve different purposes, there are some interesting connections between them:</p>
<ol type="1">
<li><strong>Similarity Measurement</strong>:
<ul>
<li>Both approaches involve measuring the similarity between data points. Kernel methods explicitly compute similarities through kernel functions, while attention mechanisms compute weighted similarities dynamically based on the query, keys, and values.</li>
</ul></li>
<li><strong>High-Dimensional Spaces</strong>:
<ul>
<li>Kernel methods implicitly work in high-dimensional feature spaces, whereas attention mechanisms effectively operate in a dynamic context, allowing the model to learn what features are most relevant based on the task at hand.</li>
</ul></li>
<li><strong>Flexible Representations</strong>:
<ul>
<li>Attention mechanisms can be viewed as a flexible way to represent relationships in data, similar to how kernel methods represent data in high-dimensional spaces. Both methods enhance model performance by focusing on relevant aspects of the data.</li>
</ul></li>
</ol>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li><strong>Kernel methods</strong> provide a way to work with high-dimensional data using similarity measurements without explicitly transforming the data, making them efficient and powerful for various machine learning tasks.</li>
<li><strong>Attention mechanisms</strong> allow models to dynamically focus on important parts of the input data, improving their ability to handle tasks in natural language processing and beyond.</li>
</ul>
<p>Defining the <strong>kernel function</strong> is an essential aspect of understanding kernel methods in machine learning. Let’s break it down into clear components, explaining what a kernel function is, its properties, and some common examples.</p>
</section>
<section id="what-is-a-kernel-function" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-kernel-function">What is a Kernel Function?</h3>
<p>A <strong>kernel function</strong> is a mathematical function that computes the similarity between two data points in a potentially high-dimensional feature space without explicitly mapping the data into that space. This allows algorithms to operate efficiently even in complex spaces.</p>
<p>The kernel function can be thought of as a measure of similarity between two input vectors,<span class="math inline">\(x_i\)</span> and<span class="math inline">\(x_j\)</span>.</p>
</section>
<section id="mathematical-definition" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-definition">Mathematical Definition</h3>
<p>Formally, a kernel function<span class="math inline">\(K\)</span> takes two input vectors<span class="math inline">\(x_i\)</span> and<span class="math inline">\(x_j\)</span> and produces a scalar value that represents their similarity:</p>
<p><span class="math display">\[
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
\]</span></p>
<p>Where:</p>
<p>-<span class="math inline">\(\phi\)</span> is the mapping function that transforms the input vectors into a higher-dimensional space.</p>
<p>-<span class="math inline">\(\langle \cdot, \cdot \rangle\)</span> denotes the inner product in that space.</p>
<p>The beauty of kernel functions is that you don’t need to know<span class="math inline">\(\phi\)</span> explicitly. Instead, you can directly compute<span class="math inline">\(K(x_i, x_j)\)</span> using the kernel function, which simplifies computations significantly.</p>
</section>
<section id="properties-of-kernel-functions" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-kernel-functions">Properties of Kernel Functions</h3>
<ol type="1">
<li><p><strong>Symmetry</strong>: <span class="math display">\[
K(x_i, x_j) = K(x_j, x_i)
\]</span> The similarity between<span class="math inline">\(x_i\)</span> and<span class="math inline">\(x_j\)</span> is the same regardless of the order of the inputs.</p></li>
<li><p><strong>Positive Semi-Definiteness</strong>: For any set of points<span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> and any coefficients<span class="math inline">\(\alpha_1, \alpha_2, \ldots, \alpha_n\)</span>, the following holds: <span class="math display">\[
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j K(x_i, x_j) \geq 0
\]</span> This property ensures that the kernel represents a valid inner product in some feature space.</p></li>
</ol>
<p><strong>Random features</strong> are a technique used to approximate kernel methods in machine learning, particularly useful when dealing with large datasets and high-dimensional feature spaces. They allow models to leverage the advantages of kernel methods while improving computational efficiency. Let’s break this down step by step.</p>
</section>
<section id="what-are-random-features" class="level3">
<h3 class="anchored" data-anchor-id="what-are-random-features">What are Random Features?</h3>
<p>Random features involve using a randomized approach to create new features from the original input data. Instead of computing the kernel function directly, random features approximate the kernel mapping, enabling the application of linear models in an implicit high-dimensional space.</p>
</section>
<section id="why-use-random-features" class="level3">
<h3 class="anchored" data-anchor-id="why-use-random-features">Why Use Random Features?</h3>
<ol type="1">
<li><strong>Scalability</strong>: Directly using kernel methods can be computationally expensive, especially for large datasets. Random features provide a way to scale kernel methods to large datasets efficiently.</li>
<li><strong>Speed</strong>: By transforming the data into a lower-dimensional space using random features, models can be trained and evaluated much faster.</li>
<li><strong>Flexibility</strong>: Random features can approximate various types of kernels, making them versatile for different applications.</li>
</ol>
</section>
<section id="how-do-random-features-work" class="level3">
<h3 class="anchored" data-anchor-id="how-do-random-features-work">How Do Random Features Work?</h3>
<section id="kernel-approximation" class="level4">
<h4 class="anchored" data-anchor-id="kernel-approximation">1. Kernel Approximation</h4>
<p>The idea behind random features is based on the <strong>kernel trick</strong>. The kernel function<span class="math inline">\(K(x_i, x_j)\)</span> can be approximated using random projections. The key concept is that you can express the kernel function as an inner product in a new feature space.</p>
<p>For example, consider a kernel function<span class="math inline">\(K(x_i, x_j)\)</span> that corresponds to some mapping<span class="math inline">\(\phi(x)\)</span>:</p>
<p><span class="math display">\[
K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
\]</span></p>
<p>The goal is to find a random mapping<span class="math inline">\(\psi\)</span> such that:</p>
<p><span class="math display">\[
\psi(x) \approx \phi(x)
\]</span></p>
</section>
<section id="random-feature-generation" class="level4">
<h4 class="anchored" data-anchor-id="random-feature-generation">2. Random Feature Generation</h4>
<p>To create random features, you typically follow these steps:</p>
<ol type="1">
<li><p><strong>Random Projections</strong>:</p>
<ul>
<li>Use a random matrix to project the original data into a higher-dimensional space.</li>
<li>For example, if you want to approximate the Gaussian kernel, you can generate random vectors from a Gaussian distribution.</li>
</ul></li>
<li><p><strong>Feature Mapping</strong>:</p>
<ul>
<li>Map the input data<span class="math inline">\(x\)</span> using random projections. For the Gaussian kernel, you could compute the following:</li>
</ul>
<p><span class="math display">\[
\phi(x) = \sqrt{\frac{2}{D}} \left[\cos(w_1^T x + b_1), \cos(w_2^T x + b_2), \ldots, \cos(w_D^T x + b_D)\right]
\]</span></p>
<p>Where: -<span class="math inline">\(w_d\)</span> is a random vector (typically sampled from a Gaussian distribution). -<span class="math inline">\(b_d\)</span> is a random bias (often uniformly sampled from<span class="math inline">\([0, 2\pi]\)</span>). -<span class="math inline">\(D\)</span> is the number of random features.</p></li>
<li><p><strong>Approximate Kernel</strong>:</p>
<ul>
<li>The kernel function can be approximated using these random features, allowing for efficient computations.</li>
</ul></li>
</ol>
</section>
</section>
<section id="advantages-of-random-features" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-random-features">Advantages of Random Features</h3>
<ul>
<li><strong>Reduced Complexity</strong>: They reduce the computational burden associated with calculating kernel matrices directly.</li>
<li><strong>Linear Models</strong>: They enable the use of linear models to capture complex relationships in the data through the randomized feature space.</li>
<li><strong>Flexible</strong>: They can be adapted to various kernel functions, making them versatile for different applications.</li>
</ul>
</section>
<section id="disadvantages-of-random-features" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-of-random-features">Disadvantages of Random Features</h3>
<ul>
<li><strong>Approximation</strong>: Since random features are an approximation, the quality of the approximation may vary depending on the number of features<span class="math inline">\(D\)</span> used.</li>
<li><strong>Stochastic Nature</strong>: The randomness in generating features can lead to variability in model performance. It’s important to tune the number of random features and consider methods like averaging to improve results.</li>
</ul>
</section>
<section id="approximation-of-kernel-methods-using-random-features" class="level3">
<h3 class="anchored" data-anchor-id="approximation-of-kernel-methods-using-random-features">Approximation of Kernel Methods Using Random Features</h3>
<p>The approximation of kernel methods using random features is a powerful technique that allows us to leverage the benefits of kernel methods while improving computational efficiency. This approach is particularly useful in large-scale machine learning tasks where direct computation of kernel functions can be expensive.</p>
</section>
<section id="how-does-random-feature-approximation-work" class="level3">
<h3 class="anchored" data-anchor-id="how-does-random-feature-approximation-work">How Does Random Feature Approximation Work?</h3>
<p>The random feature approximation can be broken down into several steps:</p>
<section id="kernel-function-representation" class="level4">
<h4 class="anchored" data-anchor-id="kernel-function-representation">1. Kernel Function Representation</h4>
<p>Assume we have a kernel function<span class="math inline">\(K(x_i, x_j)\)</span> that we want to approximate. For instance, for the <strong>Gaussian kernel</strong>, the kernel function is defined as:</p>
<p><span class="math display">\[
K(x_i, x_j) = \exp\left(-\frac{||x_i - x_j||^2}{2\sigma^2}\right)
\]</span></p>
<p>This kernel function can be interpreted as an inner product in an infinite-dimensional space.</p>
</section>
<section id="random-feature-mapping" class="level4">
<h4 class="anchored" data-anchor-id="random-feature-mapping">2. Random Feature Mapping</h4>
<p>To approximate the kernel function, we can use random features based on the idea of <strong>Fourier feature mapping</strong>. The basic idea is to approximate the kernel function as follows:</p>
<p><span class="math display">\[
K(x_i, x_j) \approx \phi(x_i)^T \phi(x_j)
\]</span></p>
<p>Where<span class="math inline">\(\phi(x)\)</span> is a mapping that transforms the input data into a new feature space. For the Gaussian kernel, we can derive a finite-dimensional approximation using random features:</p>
<ol type="1">
<li><p><strong>Generate Random Weights</strong>: Create random weights<span class="math inline">\(w_d\)</span> sampled from a Gaussian distribution (e.g.,<span class="math inline">\(\mathcal{N}(0, 1)\)</span>).</p></li>
<li><p><strong>Generate Random Biases</strong>: Create random biases<span class="math inline">\(b_d\)</span> uniformly sampled from<span class="math inline">\([0, 2\pi]\)</span>.</p></li>
<li><p><strong>Construct the Feature Mapping</strong>: The feature mapping<span class="math inline">\(\phi(x)\)</span> can be defined as:</p>
<p><span class="math display">\[
\phi(x) = \sqrt{\frac{2}{D}} \left[ \cos(w_1^T x + b_1), \cos(w_2^T x + b_2), \ldots, \cos(w_D^T x + b_D) \right]
\]</span></p>
<p>Here,<span class="math inline">\(D\)</span> is the number of random features we want to generate.</p></li>
</ol>
</section>
<section id="approximate-the-kernel" class="level4">
<h4 class="anchored" data-anchor-id="approximate-the-kernel">3. Approximate the Kernel</h4>
<p>Using the random features, we can now approximate the kernel function as follows:</p>
<p><span class="math display">\[
K(x_i, x_j) \approx \frac{1}{D} \sum_{d=1}^{D} \cos(w_d^T x_i + b_d) \cos(w_d^T x_j + b_d)
\]</span></p>
<p>This expression gives us an efficient way to compute an approximation of the kernel using the generated random features.</p>
</section>
</section>
<section id="advantages-of-using-random-features" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-using-random-features">Advantages of Using Random Features</h3>
<ul>
<li><strong>Efficiency</strong>: Significantly reduces the computational complexity associated with kernel methods, making them feasible for large datasets.</li>
<li><strong>Scalability</strong>: Random features allow models to scale better, as you can adjust the number of features<span class="math inline">\(D\)</span> based on computational resources.</li>
<li><strong>Flexibility</strong>: They can approximate various kernel functions, making them versatile for different tasks.</li>
</ul>
</section>
<section id="disadvantages" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages">Disadvantages</h3>
<ul>
<li><strong>Approximation Quality</strong>: The quality of the approximation can depend on the number of random features<span class="math inline">\(D\)</span>. A larger<span class="math inline">\(D\)</span> generally leads to better approximations but at the cost of increased computation.</li>
<li><strong>Variability</strong>: The stochastic nature of random feature generation can introduce variability in model performance. Multiple runs may yield different results, so averaging over several trials can be beneficial.</li>
</ul>
<p>Approximating attention mechanisms using random features is a promising approach to make attention computations more efficient, especially in scenarios where the input sequences are long. Traditional attention mechanisms, such as those used in the Transformer model, can become computationally expensive due to their quadratic complexity with respect to the sequence length. Using random features can help mitigate this issue by approximating the attention scores while maintaining the essential characteristics of the attention mechanism.</p>
</section>
<section id="approximation-using-random-features" class="level3">
<h3 class="anchored" data-anchor-id="approximation-using-random-features">Approximation Using Random Features</h3>
<p>To approximate the attention mechanism using random features, we can follow a similar idea as approximating kernel methods. The key is to approximate the attention scores in a way that reduces computational complexity.</p>
<section id="steps-for-random-feature-approximation-of-attention" class="level4">
<h4 class="anchored" data-anchor-id="steps-for-random-feature-approximation-of-attention">Steps for Random Feature Approximation of Attention</h4>
<ol type="1">
<li><p><strong>Random Projection</strong>:</p>
<ul>
<li>Instead of computing the full attention matrix using dot products between<span class="math inline">\(Q\)</span> and<span class="math inline">\(K\)</span>, we can use random projections to reduce the dimensionality and computational cost.</li>
<li>The main idea is to project the query and key representations into a lower-dimensional space using random feature mappings.</li>
</ul></li>
<li><p><strong>Feature Mapping</strong>:</p>
<ul>
<li>We can use random Fourier features or similar mappings to transform<span class="math inline">\(Q\)</span> and<span class="math inline">\(K\)</span>.</li>
<li>For instance, we can define random feature mappings<span class="math inline">\(\phi(Q)\)</span> and<span class="math inline">\(\phi(K)\)</span> for the query and key matrices, respectively.</li>
</ul>
<p>The mapping could look like this:</p>
<p><span class="math display">\[
\phi(x) = \sqrt{\frac{2}{D}} \left[\cos(w_1^T x + b_1), \cos(w_2^T x + b_2), \ldots, \cos(w_D^T x + b_D)\right]
\]</span></p>
<p>Where<span class="math inline">\(D\)</span> is the number of random features,<span class="math inline">\(w_d\)</span> is a random weight vector, and<span class="math inline">\(b_d\)</span> is a random bias.</p></li>
<li><p><strong>Approximate Attention Scores</strong>:</p>
<ul>
<li>Instead of directly computing<span class="math inline">\(QK^T\)</span>, we can compute<span class="math inline">\(\phi(Q)\)</span> and<span class="math inline">\(\phi(K)\)</span> and then approximate the attention as follows:</li>
</ul>
<p><span class="math display">\[
\text{Attention}(Q, K, V) \approx \text{softmax}\left(\frac{\phi(Q) \phi(K)^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>This reduces the dimensionality of the operations involved, allowing for faster computations.</p></li>
<li><p><strong>Reduce Complexity</strong>:</p>
<ul>
<li>By using random features, we can reduce the complexity of the attention computation from<span class="math inline">\(O(n^2)\)</span> to<span class="math inline">\(O(nD)\)</span>, where<span class="math inline">\(D\)</span> is the number of random features. This is particularly useful for long sequences.</li>
</ul></li>
</ol>
</section>
</section>
<section id="advantages-of-random-feature-approximation-of-attention" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-random-feature-approximation-of-attention">Advantages of Random Feature Approximation of Attention</h3>
<ul>
<li><strong>Improved Efficiency</strong>: The computational cost of attention mechanisms is significantly reduced, making it feasible to apply attention to longer sequences.</li>
<li><strong>Scalability</strong>: This approach allows models to handle larger datasets and longer sequences effectively.</li>
<li><strong>Flexibility</strong>: Random feature approximation can be adapted to various attention mechanisms, making it a versatile approach for different architectures.</li>
</ul>
</section>
<section id="disadvantages-1" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-1">Disadvantages</h3>
<ul>
<li><strong>Approximation Quality</strong>: The quality of the attention approximation depends on the number of random features used. A larger number of features generally leads to better approximations but increases computation.</li>
<li><strong>Variability</strong>: Since random features are generated randomly, the model performance may vary across runs. This can introduce instability, and techniques such as averaging results over multiple runs may be needed.</li>
</ul>
<p><strong>Positive Orthogonal Random Features (P-ORFs)</strong> and <strong>Performers</strong> are advanced techniques that enhance the efficiency and scalability of attention mechanisms in deep learning models, particularly in Transformer architectures. Below is an overview of both concepts, their motivations, and their applications.</p>
</section>
<section id="positive-orthogonal-random-features-p-orfs" class="level3">
<h3 class="anchored" data-anchor-id="positive-orthogonal-random-features-p-orfs">1. Positive Orthogonal Random Features (P-ORFs)</h3>
<p>Positive Orthogonal Random Features (P-ORFs) are a way to approximate kernel functions and attention mechanisms efficiently. The idea is to construct random features that maintain certain mathematical properties, such as orthogonality and positivity, to better capture the relationships in the data while keeping computational costs low.</p>
<section id="properties-of-p-orfs" class="level4">
<h4 class="anchored" data-anchor-id="properties-of-p-orfs">Properties of P-ORFs</h4>
<ol type="1">
<li><strong>Orthogonality</strong>:
<ul>
<li>The random features are designed to be orthogonal, which helps in maintaining the diversity of the features and reduces redundancy.</li>
<li>This property allows the model to learn a richer representation of the input data, making it more expressive.</li>
</ul></li>
<li><strong>Positivity</strong>:
<ul>
<li>The features are positive, meaning they do not take on negative values. This is particularly useful in contexts where negative values may not make sense, such as certain types of similarity measures.</li>
</ul></li>
<li><strong>Dimensionality Reduction</strong>:
<ul>
<li>P-ORFs approximate the original high-dimensional space by projecting data into a lower-dimensional space, thus reducing the complexity of operations involved in models like attention mechanisms.</li>
</ul></li>
</ol>
</section>
<section id="mathematical-representation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-representation">Mathematical Representation</h4>
<p>The P-ORF mapping can be represented as follows:</p>
<p><span class="math display">\[
\phi(x) = \sqrt{\frac{2}{D}} \left[ \cos(w_1^T x + b_1), \cos(w_2^T x + b_2), \ldots, \cos(w_D^T x + b_D) \right]
\]</span></p>
<p>Where: -<span class="math inline">\(w_d\)</span> are random weight vectors sampled from a Gaussian distribution. -<span class="math inline">\(b_d\)</span> are random biases uniformly sampled from<span class="math inline">\([0, 2\pi]\)</span>. -<span class="math inline">\(D\)</span> is the number of random features.</p>
</section>
</section>
<section id="performers" class="level3">
<h3 class="anchored" data-anchor-id="performers">2. Performers</h3>
<p><strong>Performers</strong> are a type of Transformer architecture that leverage positive orthogonal random features for approximating attention mechanisms. The core idea is to replace the traditional attention mechanism with a more efficient computation that uses P-ORFs.</p>
<section id="key-features-of-performers" class="level4">
<h4 class="anchored" data-anchor-id="key-features-of-performers">Key Features of Performers</h4>
<ol type="1">
<li><strong>Efficient Attention Approximation</strong>:
<ul>
<li>Performers approximate the scaled dot-product attention mechanism by using P-ORFs to create a low-rank approximation of the attention scores. This reduces the computational complexity of the attention mechanism, especially for long sequences.</li>
</ul></li>
<li><strong>Linear Time Complexity</strong>:
<ul>
<li>The original attention mechanism has a time complexity of<span class="math inline">\(O(n^2)\)</span>, where<span class="math inline">\(n\)</span> is the sequence length. Performers reduce this complexity to<span class="math inline">\(O(n \cdot D)\)</span>, where<span class="math inline">\(D\)</span> is the number of random features.</li>
<li>This makes it feasible to apply attention to longer sequences and larger datasets.</li>
</ul></li>
<li><strong>Stability and Robustness</strong>:
<ul>
<li>The use of positive orthogonal features helps maintain numerical stability and robustness in training deep models.</li>
<li>The orthogonality property also ensures that the representations learned by the model are diverse and informative.</li>
</ul></li>
</ol>
</section>
<section id="attention-mechanism-in-performers" class="level4">
<h4 class="anchored" data-anchor-id="attention-mechanism-in-performers">Attention Mechanism in Performers</h4>
<p>The attention mechanism in Performers can be described as follows:</p>
<ol type="1">
<li><strong>Random Feature Mapping</strong>:
<ul>
<li>Queries<span class="math inline">\(Q\)</span> and keys<span class="math inline">\(K\)</span> are mapped into a lower-dimensional space using P-ORFs.</li>
</ul></li>
<li><strong>Approximation of Attention</strong>:
<ul>
<li>The attention scores are approximated using the mapped queries and keys. The softmax operation is applied to the approximated scores to obtain the final attention distribution.</li>
</ul></li>
<li><strong>Output Calculation</strong>:
<ul>
<li>The output is computed by multiplying the attention distribution with the values<span class="math inline">\(V\)</span> as in standard attention mechanisms.</li>
</ul></li>
</ol>
</section>
</section>
<section id="advantages-of-performers-and-p-orfs" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-performers-and-p-orfs">Advantages of Performers and P-ORFs</h3>
<ul>
<li><strong>Scalability</strong>: Both techniques allow for scalable attention mechanisms that can handle long sequences efficiently.</li>
<li><strong>Reduced Computational Burden</strong>: By approximating the attention scores with random features, Performers significantly reduce the computational overhead, enabling faster training and inference.</li>
<li><strong>Expressive Power</strong>: The use of positive orthogonal features ensures that the learned representations are rich and meaningful, improving model performance.</li>
</ul>
</section>
<section id="disadvantages-2" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-2">Disadvantages</h3>
<ul>
<li><strong>Approximation Quality</strong>: The quality of the approximation may depend on the number of random features<span class="math inline">\(D\)</span>. A smaller number of features may lead to poorer approximations, while a larger number increases computational requirements.</li>
<li><strong>Stochastic Variability</strong>: The randomness in generating features can lead to variability in performance across different runs. Techniques such as averaging results over multiple runs or using ensemble methods may be needed to stabilize performance.</li>
</ul>
</section>
</section>
<section id="nyström-approximation" class="level1">
<h1>Nyström approximation</h1>
<p>The <strong>Nyström approximation</strong> is a powerful technique used in machine learning and statistics to approximate kernel methods, particularly in scenarios where dealing with large datasets is computationally expensive. This approximation method is particularly beneficial for approximating the kernel matrix, making it feasible to use algorithms that are otherwise intractable due to high computational costs.</p>
<p>The Nyström method is named after the Swedish mathematician <strong>Gunnar Nyström</strong>, who introduced the idea. It provides a way to efficiently approximate a positive semi-definite kernel matrix using a subset of the data points. The primary goal is to reduce the computational complexity involved in kernel methods, especially when working with large datasets.</p>
<section id="kernel-matrix" class="level3">
<h3 class="anchored" data-anchor-id="kernel-matrix">1. Kernel Matrix</h3>
<p>In many machine learning tasks, especially in kernel-based methods (like Support Vector Machines or Gaussian Processes), we need to compute a <strong>kernel matrix</strong><span class="math inline">\(K\)</span>. This matrix is constructed from the pairwise evaluations of a kernel function<span class="math inline">\(k(x_i, x_j)\)</span> for data points<span class="math inline">\(x_i\)</span> and<span class="math inline">\(x_j\)</span>.</p>
<p><span class="math display">\[
K = \begin{bmatrix}
k(x_1, x_1) &amp; k(x_1, x_2) &amp; \ldots &amp; k(x_1, x_n) \\
k(x_2, x_1) &amp; k(x_2, x_2) &amp; \ldots &amp; k(x_2, x_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k(x_n, x_1) &amp; k(x_n, x_2) &amp; \ldots &amp; k(x_n, x_n) \\
\end{bmatrix}
\]</span></p>
<p>Where<span class="math inline">\(n\)</span> is the number of data points.</p>
<p>The computational complexity of constructing this matrix is<span class="math inline">\(O(n^2)\)</span>, which can be prohibitive for large datasets.</p>
</section>
<section id="nyström-approximation-steps" class="level3">
<h3 class="anchored" data-anchor-id="nyström-approximation-steps">2. Nyström Approximation Steps</h3>
<p>The Nyström method approximates the kernel matrix using a smaller set of points, typically referred to as “landmark” points. The steps involved in the Nyström approximation are as follows:</p>
<section id="step-1-select-landmark-points" class="level4">
<h4 class="anchored" data-anchor-id="step-1-select-landmark-points">Step 1: Select Landmark Points</h4>
<p>Randomly select a subset of<span class="math inline">\(m\)</span> data points from the original dataset, where<span class="math inline">\(m &lt; n\)</span>. Let’s denote these selected points as<span class="math inline">\(X_m\)</span>.</p>
</section>
<section id="step-2-compute-the-kernel-matrix-for-landmark-points" class="level4">
<h4 class="anchored" data-anchor-id="step-2-compute-the-kernel-matrix-for-landmark-points">Step 2: Compute the Kernel Matrix for Landmark Points</h4>
<p>Compute the kernel matrix<span class="math inline">\(K_{mm}\)</span> for the selected landmark points:</p>
<p><span class="math display">\[
K_{mm} = \begin{bmatrix}
k(x_{i_1}, x_{i_1}) &amp; k(x_{i_1}, x_{i_2}) &amp; \ldots &amp; k(x_{i_1}, x_{i_m}) \\
k(x_{i_2}, x_{i_1}) &amp; k(x_{i_2}, x_{i_2}) &amp; \ldots &amp; k(x_{i_2}, x_{i_m}) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k(x_{i_m}, x_{i_1}) &amp; k(x_{i_m}, x_{i_2}) &amp; \ldots &amp; k(x_{i_m}, x_{i_m}) \\
\end{bmatrix}
\]</span></p>
</section>
<section id="step-3-compute-the-kernel-matrix-between-landmark-and-all-points" class="level4">
<h4 class="anchored" data-anchor-id="step-3-compute-the-kernel-matrix-between-landmark-and-all-points">Step 3: Compute the Kernel Matrix Between Landmark and All Points</h4>
<p>Compute the kernel matrix<span class="math inline">\(K_{mn}\)</span> between the landmark points and all other points in the dataset:</p>
<p><span class="math display">\[
K_{mn} = \begin{bmatrix}
k(x_{i_1}, x_1) &amp; k(x_{i_1}, x_2) &amp; \ldots &amp; k(x_{i_1}, x_n) \\
k(x_{i_2}, x_1) &amp; k(x_{i_2}, x_2) &amp; \ldots &amp; k(x_{i_2}, x_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k(x_{i_m}, x_1) &amp; k(x_{i_m}, x_2) &amp; \ldots &amp; k(x_{i_m}, x_n) \\
\end{bmatrix}
\]</span></p>
</section>
<section id="step-4-construct-the-approximate-kernel-matrix" class="level4">
<h4 class="anchored" data-anchor-id="step-4-construct-the-approximate-kernel-matrix">Step 4: Construct the Approximate Kernel Matrix</h4>
<p>Using<span class="math inline">\(K_{mm}\)</span> and<span class="math inline">\(K_{mn}\)</span>, the Nyström approximation of the original kernel matrix<span class="math inline">\(K\)</span> is given by:</p>
<p><span class="math display">\[
K \approx K_{mn} K_{mm}^{+} K_{mn}^T
\]</span></p>
<p>Where<span class="math inline">\(K_{mm}^{+}\)</span> denotes the pseudoinverse of<span class="math inline">\(K_{mm}\)</span>. This step is crucial because it allows for the interpolation of the full kernel matrix based on the landmark points.</p>
</section>
</section>
<section id="advantages-of-the-nyström-approximation" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-the-nyström-approximation">3. Advantages of the Nyström Approximation</h3>
<ul>
<li><strong>Scalability</strong>: The Nyström method reduces the computational burden associated with calculating the full kernel matrix, making it feasible to work with large datasets.</li>
<li><strong>Flexibility</strong>: It can be applied to various kernel functions, allowing for adaptability in different contexts.</li>
<li><strong>Efficiency</strong>: By selecting only a subset of points, the Nyström method can provide good approximations with significantly reduced computation time.</li>
</ul>
</section>
<section id="disadvantages-of-the-nyström-approximation" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-of-the-nyström-approximation">4. Disadvantages of the Nyström Approximation</h3>
<ul>
<li><strong>Approximation Quality</strong>: The quality of the approximation depends on the choice of landmark points. Poor choices may lead to inaccuracies.</li>
<li><strong>Randomness</strong>: If the landmark points are chosen randomly, the results may vary between runs. To improve stability, multiple sets of landmark points can be evaluated.</li>
<li><strong>Inherent Bias</strong>: Since the approximation is based on a subset of the data, there may be inherent biases in the approximation.</li>
</ul>
</section>
<section id="conclusion-1" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-1">Conclusion</h3>
<p>The Nyström approximation is a valuable technique for efficiently approximating kernel matrices, significantly reducing computational costs in large-scale machine learning tasks. By leveraging a subset of the data points, it provides a scalable solution to the challenges posed by traditional kernel methods, making it a crucial tool in modern machine learning.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>https://arxiv.org/html/2401.10341v1</p>
<p>https://arxiv.org/pdf/2401.10341v1</p>
<p>https://gaussian37.github.io/dl-concept-quantization/</p>
<p>https://en.wikipedia.org/wiki/Singular_value_decomposition</p>
<p>https://arxiv.org/abs/2102.03902</p>
<p>https://www.youtube.com/watch?v=vSczTbgc8Rc</p>
<p>https://arxiv.org/abs/2408.16289</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ninjalabo\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2023-2024, NinjaLABO</p>
</div>   
    <div class="nav-footer-center">
<p><strong>Get support</strong></p>
<ul>
<li>For pricing and sales inquiries <a href="mailto:sales@ninjalabo.ai" class="email">sales@ninjalabo.ai</a></li>
<li>Ask tech questions <a href="mailto:help@ninjalabo.ai" class="email">help@ninjalabo.ai</a></li>
</ul>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ninjalabo/ninjalabo.github.io/edit/main/blogs/Low_Rank_Approximation-Part4.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ninjalabo/ninjalabo.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Helsinki, Finland
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ninjalabo">
      <i class="bi bi-twitter" role="img" aria-label="NinjaLABO Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/ninjalabo/">
      <i class="bi bi-linkedin" role="img" aria-label="NinjaLABO LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ninjalabo/ninjalabo.github.io">
      <i class="bi bi-github" role="img" aria-label="NinjaLABO GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>